{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zOGS9To0o1Gg",
        "outputId": "22514928-af0c-4269-dec2-980dbc2ff5ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/HCK_HEC_LANG.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-625983ca3f0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load CSV files into DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/HCK_HEC_LANG.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/HCK_HEC_SKILLS.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_staffing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/HCK_HEC_STAFFING.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/HCK_HEC_LANG.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV files into DataFrames\n",
        "df_lang = pd.read_csv('/content/HCK_HEC_LANG.csv')\n",
        "df_skills = pd.read_csv('/content/HCK_HEC_SKILLS.csv')\n",
        "df_staffing = pd.read_csv('/content/HCK_HEC_STAFFING.csv')\n",
        "df_user = pd.read_csv('/content/HCK_HEC_USER.csv')\n",
        "df_xp = pd.read_csv('/content/HCK_HEC_XP.csv')\n",
        "\n",
        "# Display the first few rows of each DataFrame\n",
        "print(\"HCK_HEC_LANG:\")\n",
        "print(df_lang.head())\n",
        "\n",
        "print(\"\\nHCK_HEC_SKILLS:\")\n",
        "print(df_skills.head())\n",
        "\n",
        "print(\"\\nHCK_HEC_STAFFING:\")\n",
        "print(df_staffing.head())\n",
        "\n",
        "print(\"\\nHCK_HEC_USER:\")\n",
        "print(df_user.head())\n",
        "\n",
        "print(\"\\nHCK_HEC_XP:\")\n",
        "print(df_xp.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get column names for each DataFrame\n",
        "print(\"HCK_HEC_LANG Columns:\")\n",
        "print(df_lang.columns.tolist())\n",
        "\n",
        "print(\"\\nHCK_HEC_SKILLS Columns:\")\n",
        "print(df_skills.columns.tolist())\n",
        "\n",
        "print(\"\\nHCK_HEC_STAFFING Columns:\")\n",
        "print(df_staffing.columns.tolist())\n",
        "\n",
        "print(\"\\nHCK_HEC_USER Columns:\")\n",
        "print(df_user.columns.tolist())\n",
        "\n",
        "print(\"\\nHCK_HEC_XP Columns:\")\n",
        "print(df_xp.columns.tolist())\n"
      ],
      "metadata": {
        "id": "c08YXxEJpbb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique USER_IDs in each table\n",
        "unique_user_lang = df_lang['USER_ID'].nunique()\n",
        "unique_user_skills = df_skills['USER_ID'].nunique()\n",
        "unique_user_staffing = df_staffing['USER_ID'].nunique()\n",
        "unique_user_user = df_user['USER_ID'].nunique()\n",
        "unique_user_xp = df_xp['USER_ID'].nunique()\n",
        "\n",
        "# Calculate the total number of unique USER_IDs across all tables\n",
        "total_unique_users = df_user['USER_ID'].nunique()  # Assuming df_user has all unique user ids\n",
        "\n",
        "# Create a dictionary to store the results\n",
        "results = {\n",
        "    \"HCK_HEC_LANG\": {\"Unique Users\": unique_user_lang, \"Percentage\": (unique_user_lang / total_unique_users) * 100},\n",
        "    \"HCK_HEC_SKILLS\": {\"Unique Users\": unique_user_skills, \"Percentage\": (unique_user_skills / total_unique_users) * 100},\n",
        "    \"HCK_HEC_STAFFING\": {\"Unique Users\": unique_user_staffing, \"Percentage\": (unique_user_staffing / total_unique_users) * 100},\n",
        "    \"HCK_HEC_USER\": {\"Unique Users\": unique_user_user, \"Percentage\": (unique_user_user / total_unique_users) * 100},\n",
        "    \"HCK_HEC_XP\": {\"Unique Users\": unique_user_xp, \"Percentage\": (unique_user_xp / total_unique_users) * 100},\n",
        "}\n",
        "\n",
        "# Print the results\n",
        "for table_name, data in results.items():\n",
        "    print(f\"{table_name}:\")\n",
        "    print(f\"  Unique Users: {data['Unique Users']}\")\n",
        "    print(f\"  Percentage of Total: {data['Percentage']:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "o1Cao9Q-pwLj",
        "outputId": "e2b0ff12-d64f-49c8-f98a-7d5e934d1fd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_lang' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d154f8b6c7a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the number of unique USER_IDs in each table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munique_user_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_lang\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munique_user_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_skills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munique_user_staffing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_staffing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munique_user_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_lang' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that in some tables the USER_ID is not always unique, suggesting each user can have one or more row entries.\n"
      ],
      "metadata": {
        "id": "RIsQ_wBeqE3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the display options to show all columns\n",
        "pd.set_option('display.max_rows', None)\n"
      ],
      "metadata": {
        "id": "ha0GveyXrLch"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_skills['DOMAIN_DSC'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "NPQAuTuhqOnx",
        "outputId": "b17659fa-c52f-405d-d717-8e9a65d1db72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_skills' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-de3e3b9e3b7f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_skills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DOMAIN_DSC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_skills' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_skills['SKILLS_DSC'].value_counts()"
      ],
      "metadata": {
        "id": "Wvc75XiJq_0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_xp['MISSION_DSC'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "hCmilvqdvpqs",
        "outputId": "11cfecbe-3146-4664-d363-b0f85f5f57cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_xp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8953c09fbde4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_xp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MISSION_DSC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_xp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your DataFrames (df_lang, df_skills, etc.) loaded as before\n",
        "\n",
        "def null_counts(df, table_name):\n",
        "  \"\"\"Calculates null counts for each column in a DataFrame.\"\"\"\n",
        "  null_counts = df.isnull().sum()\n",
        "  print(f\"\\nNull counts for {table_name}:\")\n",
        "  print(null_counts)\n",
        "\n",
        "# Get null counts for each table\n",
        "null_counts(df_lang, \"HCK_HEC_LANG\")\n",
        "null_counts(df_skills, \"HCK_HEC_SKILLS\")\n",
        "null_counts(df_staffing, \"HCK_HEC_STAFFING\")\n",
        "null_counts(df_user, \"HCK_HEC_USER\")\n",
        "null_counts(df_xp, \"HCK_HEC_XP\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "t7H8G7xexcLO",
        "outputId": "cdfee31f-1ffa-45fe-8caf-3e52a833aaac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_lang' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-be6717557162>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get null counts for each table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnull_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HCK_HEC_LANG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mnull_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_skills\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HCK_HEC_SKILLS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnull_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_staffing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HCK_HEC_STAFFING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_lang' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find non-unique rows in df_staffing based on USER_ID\n",
        "duplicate_user_ids = df_staffing[df_staffing.duplicated(subset=['USER_ID'], keep=False)]['USER_ID'].unique()\n",
        "\n",
        "# Filter the DataFrame to include only rows with duplicate USER_IDs\n",
        "df_duplicates = df_staffing[df_staffing['USER_ID'].isin(duplicate_user_ids)]\n",
        "\n",
        "# Check if df_duplicates is empty before proceeding\n",
        "if df_duplicates.empty:\n",
        "    print(\"No duplicate USER_IDs found in df_staffing.\")\n",
        "    df_inconsistent_months = []  # Initialize as an empty list to avoid errors later\n",
        "else:\n",
        "    # Group by USER_ID and check if all values for month columns are the same\n",
        "    def has_varying_months(group):\n",
        "        month_cols = [f'MONTH_{i}' for i in range(1, 13)]\n",
        "        return not group[month_cols].nunique().eq(1).all()\n",
        "\n",
        "    # Apply the function to each group and filter for cases where months are not the same\n",
        "    df_inconsistent_months = df_duplicates.groupby('USER_ID').apply(has_varying_months).loc[lambda x: x].index.tolist()\n",
        "\n",
        "# Filter the DataFrame to show only rows with inconsistent months for the duplicate USER_IDs\n",
        "df_inconsistent_staffing = df_staffing[df_staffing['USER_ID'].isin(df_inconsistent_months)]\n",
        "\n",
        "print(\"Number of rows with non unique USER_IDs:\", df_duplicates.shape[0])\n",
        "print(\"Number of rows with non unique USER_IDs and inconsistent month columns:\", df_inconsistent_staffing.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1luScR_wynxj",
        "outputId": "2667e54c-5d47-479e-a107-bc5555b2f521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_staffing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b1e0881b0016>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Find non-unique rows in df_staffing based on USER_ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mduplicate_user_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_staffing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_staffing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Filter the DataFrame to include only rows with duplicate USER_IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_duplicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_staffing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_staffing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduplicate_user_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_staffing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define columns that need to be merged as comma-separated values\n",
        "merge_columns = {\n",
        "    \"HCK_HEC_LANG\": [\"LANGUAGE_SKILL_DSC\", \"LANGUAGE_SKILL_LVL\"],\n",
        "    \"HCK_HEC_SKILLS\": [\"SKILLS_DSC\", \"DOMAIN_DSC\", \"LEVEL_VAL\"],\n",
        "    \"HCK_HEC_XP\": [\"MISSION_DSC\"]\n",
        "}\n",
        "\n",
        "# Aggregate function for merging rows with comma separation\n",
        "def merge_values(series):\n",
        "    return ', '.join(series.dropna().astype(str).unique())\n",
        "\n",
        "# Apply aggregation to each dataset\n",
        "df_lang = df_lang.groupby(\"USER_ID\").agg({col: merge_values for col in merge_columns[\"HCK_HEC_LANG\"]}).reset_index()\n",
        "df_skills = df_skills.groupby(\"USER_ID\").agg({col: merge_values for col in merge_columns[\"HCK_HEC_SKILLS\"]}).reset_index()\n",
        "df_xp = df_xp.groupby(\"USER_ID\").agg({col: merge_values for col in merge_columns[\"HCK_HEC_XP\"]}).reset_index()\n",
        "\n",
        "# Merge all tables on USER_ID\n",
        "df_merged = df_user.copy()  # Start with user table (full reference)\n",
        "df_merged = df_merged.merge(df_lang, on=\"USER_ID\", how=\"left\")\n",
        "df_merged = df_merged.merge(df_skills, on=\"USER_ID\", how=\"left\")\n",
        "df_merged = df_merged.merge(df_staffing, on=\"USER_ID\", how=\"left\")\n",
        "df_merged = df_merged.merge(df_xp, on=\"USER_ID\", how=\"left\")"
      ],
      "metadata": {
        "id": "NB3AumZExKan",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "14582f09-58ba-4bd9-9497-a202644d069d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_lang' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-63cd1ffb41da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Apply aggregation to each dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"USER_ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerge_values\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerge_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HCK_HEC_LANG\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_skills\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"USER_ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerge_values\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerge_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HCK_HEC_SKILLS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdf_xp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_xp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"USER_ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerge_values\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerge_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HCK_HEC_XP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_lang' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "UmCpVLNRxNtr",
        "outputId": "d9aeda14-4381-4023-86dc-a57470a6c1b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-947d9424058f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate null counts and percentages for each column in df_merged\n",
        "null_counts = df_merged.isnull().sum()\n",
        "null_percentages = (null_counts / len(df_merged)) * 100\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "null_analysis = pd.DataFrame({\n",
        "    'Null Counts': null_counts,\n",
        "    'Null Percentages': null_percentages\n",
        "})\n",
        "\n",
        "# Display the null analysis\n",
        "null_analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0oq0BqOXxRZA",
        "outputId": "9d6c0d37-3e51-429c-b6dd-883d3d95f7aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e898aff79093>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate null counts and percentages for each column in df_merged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnull_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnull_percentages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnull_counts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a DataFrame to display the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: df merged give me one row with missing values\n",
        "\n",
        "# Find a row with missing values\n",
        "row_with_missing_values = df_merged[df_merged.isnull().any(axis=1)].head(1)\n",
        "\n",
        "row_with_missing_values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9uvebVbwyIoh",
        "outputId": "3f04e954-5a4b-4d2f-f180-1be77f3a38c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-937be20767cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Find a row with missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrow_with_missing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrow_with_missing_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check if userid 2817191 is in df skills\n",
        "\n",
        "if 2817191 in df_skills['USER_ID'].values:\n",
        "  print(\"User ID 2817191 is present in df_skills.\")\n",
        "else:\n",
        "  print(\"User ID 2817191 is not present in df_skills.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3Eeio0C-yQtp",
        "outputId": "fb7c1951-51bf-45bf-b125-68d3b46dcb87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_skills' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c30d25049aab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: check if userid 2817191 is in df skills\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;36m2817191\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_skills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User ID 2817191 is present in df_skills.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_skills' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROWS Are NAN because some user_ids don't have matching rows in other tables like skills"
      ],
      "metadata": {
        "id": "XqPNIUzI1G8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_numeric_column(df, col_name, prefix):\n",
        "    # Split string values into lists of integers\n",
        "    # Filter out empty strings before conversion\n",
        "    df[col_name] = df[col_name].apply(lambda x: [int(float(i)) for i in x.split(\", \") if i] if pd.notna(x) else [])\n",
        "\n",
        "    # Expand into separate columns\n",
        "    max_length = df[col_name].apply(len).max()  # Find max number of values\n",
        "    for i in range(max_length):\n",
        "        df[f\"{prefix}_{i+1}\"] = df[col_name].apply(lambda x: x[i] if i < len(x) else None)\n",
        "\n",
        "    # Drop original column after expansion\n",
        "    df.drop(columns=[col_name], inplace=True)\n",
        "\n",
        "# Apply function to split `LANGUAGE_SKILL_LVL` and `LEVEL_VAL`\n",
        "split_numeric_column(df_merged, \"LANGUAGE_SKILL_LVL\", \"LANGUAGE_SKILL_LVL\")\n",
        "split_numeric_column(df_merged, \"LEVEL_VAL\", \"LEVEL_VAL\")"
      ],
      "metadata": {
        "id": "YS4lWhlT1FTd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "ca2dfd61-95f2-4a7e-d11e-6e2e738065b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-098a41744d59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Apply function to split `LANGUAGE_SKILL_LVL` and `LEVEL_VAL`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msplit_numeric_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LANGUAGE_SKILL_LVL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LANGUAGE_SKILL_LVL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0msplit_numeric_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LEVEL_VAL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LEVEL_VAL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "YYvAdADp1eMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_language_skill(df):\n",
        "    english_skills, french_skills = [], []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Check if LANGUAGE_SKILL_DSC is a string and not null before splitting\n",
        "        if isinstance(row[\"LANGUAGE_SKILL_DSC\"], str) and not pd.isnull(row[\"LANGUAGE_SKILL_DSC\"]):\n",
        "            languages = row[\"LANGUAGE_SKILL_DSC\"].split(\", \")  # Split into list\n",
        "        else:\n",
        "            languages = []  # Assign an empty list if not a string or null\n",
        "\n",
        "        # Access the split columns directly using the prefix\n",
        "        # Assuming skill levels are stored in columns like \"LANGUAGE_SKILL_LVL_1\", \"LANGUAGE_SKILL_LVL_2\", etc.\n",
        "        # Get the number of LANGUAGE_SKILL_LVL_ columns\n",
        "        num_skill_cols = len([col for col in df.columns if col.startswith(\"LANGUAGE_SKILL_LVL_\")])\n",
        "\n",
        "        skill_levels = [row[f\"LANGUAGE_SKILL_LVL_{i+1}\"] for i in range(num_skill_cols) if pd.notna(row[f\"LANGUAGE_SKILL_LVL_{i+1}\"])]\n",
        "        skill_levels = [int(float(x)) for x in skill_levels]\n",
        "\n",
        "        # Initialize default values\n",
        "        english_level = None\n",
        "        french_level = None\n",
        "\n",
        "        # Assign skill levels based on language order\n",
        "        for i, lang in enumerate(languages):\n",
        "            if lang == \"English\":\n",
        "                english_level = skill_levels[i] if i < len(skill_levels) else None  # Handle potential IndexError\n",
        "            elif lang == \"French\":\n",
        "                french_level = skill_levels[i] if i < len(skill_levels) else None  # Handle potential IndexError\n",
        "\n",
        "        english_skills.append(english_level)\n",
        "        french_skills.append(french_level)\n",
        "\n",
        "    df[\"english_skill_lvl\"] = english_skills\n",
        "    df[\"french_skill_lvl\"] = french_skills\n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply function to match language skills\n",
        "df_merged = match_language_skill(df_merged)\n",
        "\n",
        "# Drop the original columns if no longer needed\n",
        "df_merged.drop(columns=[\"LANGUAGE_SKILL_DSC\"] + [col for col in df_merged.columns if col.startswith(\"LANGUAGE_SKILL_LVL_\")], inplace=True)  # drop the split columns\n"
      ],
      "metadata": {
        "id": "Xgo92_2n2N6O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6ab9f330-41ec-4846-bcb0-3ba1a26889e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e627c3764b48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Apply function to match language skills\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_language_skill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Drop the original columns if no longer needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "YwGuPFyj2tu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: df merged to csv\n",
        "\n",
        "# Assuming df_merged is your final DataFrame\n",
        "df_merged.to_csv('merged_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ttifM4MX29yc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "2c1466f1-5fdc-4d65-c177-997988cb22de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d42f6df0a042>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming df_merged is your final DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'merged_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: skills_dsc has comma seperated value process them all like lower trim etc and then find how many unique ones are there and list them\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_skills is your DataFrame\n",
        "def process_skills(df_skills):\n",
        "  \"\"\"Processes skills data, removes duplicates, and returns unique skills.\"\"\"\n",
        "\n",
        "  unique_skills = set()\n",
        "  if 'SKILLS_DSC' in df_skills.columns:\n",
        "    for skills in df_skills['SKILLS_DSC'].dropna().astype(str):\n",
        "      for skill in skills.split(', '):\n",
        "          processed_skill = skill.strip().lower()\n",
        "          unique_skills.add(processed_skill)\n",
        "\n",
        "  return list(unique_skills)\n",
        "\n",
        "unique_skill_list = process_skills(df_skills)\n",
        "print(\"Number of Unique Skills:\", len(unique_skill_list))\n",
        "print(\"Unique Skills:\")\n",
        "for skill in unique_skill_list:\n",
        "    print(skill) # Added indentation and print statement"
      ],
      "metadata": {
        "id": "G3Vypte-668W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate MISSION_DSC column into english"
      ],
      "metadata": {
        "id": "RpNob7ZzaGAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install deep-translator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEm2EKaccDxG",
        "outputId": "6d7479c1-4722-4099-ba90-e7c4aa9e8d07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df_merged = pd.read_csv('merged_data.csv')\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "\n",
        "# Function to translate text safely\n",
        "def safe_translate(text):\n",
        "    try:\n",
        "        if pd.notnull(text) and text.strip():  # Ensure text is not empty\n",
        "            return translator.translate(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for '{text}': {e}\")\n",
        "        return text  # Return original text if translation fails\n",
        "    return text\n",
        "\n",
        "# Apply translation\n",
        "df_merged['mission_dsc_eng'] = df_merged['MISSION_DSC'].astype(str).apply(safe_translate)\n",
        "\n",
        "# Save the updated dataset\n",
        "df_merged.to_csv('merged_data_translated.csv', index=False)\n",
        "\n",
        "print(\"Translation completed and saved as 'merged_data_translated.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpInf64icDr3",
        "outputId": "f79d77eb-85b4-47c8-9f2f-d3c1f456bfd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation error for 'Définir et déployer la vision, les objectifs et les plans d’action relatifs à l’Analytique en cohérence avec la stratégie globale de [CLIENT]., Collaborer étroitement avec le [CLIENT] pour stimuler la croissance des revenus et avec le [CLIENT] pour optimiser les opérations., Intervenir en tant que Customer Success Manager sur des comptes stratégiques pour assurer la bonne tenue de nos interventions, maintenir une excellente relation avec le client et générer des opportunités., S’assurer du bon taux d’activité des consultants de la BU et de sa rentabilité, Anticiper les risques et mettre en place des actions préventives et évolutives afin de les gérer, Représenter l’Entreprise sur son périmètre d’activité, faire connaître sa BU et son offre sur le marché., Assumer un rôle d'architecte, superviser les autres architectes et les faire profiter de son expérience pour leur permettre de monter en compétence., En lien avec le Bureau de projet, assurer la bonne tenue des projets analytiques, qu’ils soient terminés dans les délais, respectent le budget et soient de la plus haute qualité., Composer, affecter et coordonner, en collaboration avec les ventes, les équipes opérationnelles dans le cadre des activités pré-sales : démos, appels de découverte, présentations, propositions; il est le garant de l’implication de ses équipes et du respect du process de vente par celles-ci, Supervise l'ensemble des livraisons analytiques de son périmètre, des aspects de Pré-sales, et des activités architecturales., Membre de la direction de la pratique d'ingénierie des données, Impliqué dans le processus de recrutement afin de constituer une équipe performante et talentueuse à Montréal et Toronto., Gestion des affectations de nos talents pour les nouveaux projets [CLIENT] en alignant les compétences adéquates avec les initiatives pertinentes pour maximiser la performance de l'équipe et assurer le succès des projets., Organiser des réunions d'équipe pour promouvoir la collaboration et la communication entre les membres de l'équipe., Cultiver des partenariats technologiques et élaborant une feuille de route pluriannuelle pour la pratique., Diriger les efforts pour établir et mettre en œuvre des pratiques de pointe des méthodologies innovantes, des cadres techniques et des accélérateurs dans l'industrie., Apporter un soutien ponctuel aux diverses équipes de projet en tant qu'expert en la matière, contribuant à leur succès et développement., * Piloter l'exploration, la conception et le déploiement de la mise en œuvre de la plateforme de données Azure et Databricks et la création de rapports MVP Power BI, incluant l'architecture cible, les pipelines d'ingestion de données et les configurations des services de données Azure et de la plateforme Databricks., Apporter soutien, conseils et partager les meilleures pratiques Azure avec les clients., Animer des sessions de transfert de connaissances, des démonstrations de solutions et rédiger une documentation détaillée des solutions., Entretenir une communication régulière et soutenue avec les clients lors des réunions de suivi, fournissant des mises à jour, des éclairages et des recommandations expertes pour stimuler la collaboration et encourager le succès., Organiser et orchestrer les sprint planning pour les membres de l'équipe assurant une répartition optimale des tâches et un bon déroulement des projets., Piloter l'exploration, la conception et le déploiement de la mise en œuvre de la plateforme de données Azure et la création de rapports MVP Power BI, incluant l'architecture cible, les pipelines d'ingestion de données et les configurations des services de données Azure., Élaborer une stratégie de plateforme de données Azure à l'échelle de l'entreprise pour répondre aux besoins prioritaires de la [CLIENT]., Documenter une architecture cible de référence alignés avec les principes clés et la vision [CLIENT]., Participer activement à la stratégie de Gestion du Changement et aux initiatives de gouvernance des rapports., Maintenir une communication soutenue et régulière avec les clients lors des réunions de suivi, apportant des mises à jour, des conseils avisés et des recommandations d'experts pour stimuler la collaboration et garantir le succès des projets., Piloter une équipe transversale de 12 membres pour unifier diverses sources de données au sein d'une architecture globale, permettant l'élaboration d'offres précieuses pour les clients biopharmaceutiques., Instaurer des méthodologies rigoureuses pour l'entretien et l'appui d'écosystèmes complexes dotés d'applications à fort impact., Planifier et orchestrer les emplois du temps des membres de l'équipe garantissant une attribution optimale des tâches et un bon déroulement des projets., Concevoir une solution intégrant Snowflake, Matillion, Qlik Replicate sur Azure et Power BI., Entretenir une communication soutenue et régulière avec les clients lors des réunions de suivi, apportant des mises à jour, des éclairages et des recommandations d'experts pour stimuler la collaboration et assurer le succès des projets., En tant que fervent défenseur des solutions innovantes et des analyses basées sur les données, j'ai joué un rôle crucial en tant que Responsable de Centre d'Excellence en Données et Analytique Avancée, contribuant au succès organisationnel grâce à des stratégies et technologies de pointe., Diriger le recrutement, la formation et le développement professionnel des membres de l'équipe, en leur fournissant un encadrement pratique et un soutien technique pour les aider à atteindre leurs objectifs., Surveiller et gérer plusieurs projets, fournissant des mises à jour régulières et des rapports de progression lors des réunions de comité de suivi., Élaborer et mettre en œuvre une feuille de route pluriannuelle pour les données et l'analyse, alignée sur les objectifs de l'[CLIENT] et comprenant l'architecture, la stratégie cloud, la gouvernance des données, l'analyse avancée avec l'apprentissage automatique, la littératie des données et la formation des équipes d'affaires., Collaborer avec diverses équipes, y compris l'architecture d'entreprise l'infrastructure, DevOps, la cybersécurité, la gestion de projet et les lignes d'affaires, pour créer un portefeuille de projets complet et présenter des stratégies, projets et démonstrations innovants., Initier un programme novateur visant à attirer et à former des talents émergents juniors, en leur offrant des opportunités variées pour travailler dans différentes lignes d'activité et développer leur expertise en matière de données et d'analyses avancées., Responsable de la conception technique détaillée, de la mise en œuvre et de l'évolution de la plateforme de gestion de données et intelligence d'affaire., Participer aux divers projets dans les phases d'architecture de solution, de données et d'intégration., Encadrer les équipes de développements en recommandant les orientations et les décisions techniques., Accompagner les différents intervenants lors du cycle de vie du développement des projets et des évolutions., Conceptualiser les solutions en considérant les principes directeurs et les ormes d'architecture corporative de la [CLIENT]., Etablir des stratégies et s'assurer de leur mise en place pour mieux gérer l'évolution de nos applications, de nos infrastructures, et assurer la pérennité de nos solutions existantes., Traiter et estimer les nouvelles demandes de sujets de données et d'intégration systèmes des différents projets d'affaires., Assurer la vigie des systèmes d'information et des technologies.': Définir et déployer la vision, les objectifs et les plans d’action relatifs à l’Analytique en cohérence avec la stratégie globale de [CLIENT]., Collaborer étroitement avec le [CLIENT] pour stimuler la croissance des revenus et avec le [CLIENT] pour optimiser les opérations., Intervenir en tant que Customer Success Manager sur des comptes stratégiques pour assurer la bonne tenue de nos interventions, maintenir une excellente relation avec le client et générer des opportunités., S’assurer du bon taux d’activité des consultants de la BU et de sa rentabilité, Anticiper les risques et mettre en place des actions préventives et évolutives afin de les gérer, Représenter l’Entreprise sur son périmètre d’activité, faire connaître sa BU et son offre sur le marché., Assumer un rôle d'architecte, superviser les autres architectes et les faire profiter de son expérience pour leur permettre de monter en compétence., En lien avec le Bureau de projet, assurer la bonne tenue des projets analytiques, qu’ils soient terminés dans les délais, respectent le budget et soient de la plus haute qualité., Composer, affecter et coordonner, en collaboration avec les ventes, les équipes opérationnelles dans le cadre des activités pré-sales : démos, appels de découverte, présentations, propositions; il est le garant de l’implication de ses équipes et du respect du process de vente par celles-ci, Supervise l'ensemble des livraisons analytiques de son périmètre, des aspects de Pré-sales, et des activités architecturales., Membre de la direction de la pratique d'ingénierie des données, Impliqué dans le processus de recrutement afin de constituer une équipe performante et talentueuse à Montréal et Toronto., Gestion des affectations de nos talents pour les nouveaux projets [CLIENT] en alignant les compétences adéquates avec les initiatives pertinentes pour maximiser la performance de l'équipe et assurer le succès des projets., Organiser des réunions d'équipe pour promouvoir la collaboration et la communication entre les membres de l'équipe., Cultiver des partenariats technologiques et élaborant une feuille de route pluriannuelle pour la pratique., Diriger les efforts pour établir et mettre en œuvre des pratiques de pointe des méthodologies innovantes, des cadres techniques et des accélérateurs dans l'industrie., Apporter un soutien ponctuel aux diverses équipes de projet en tant qu'expert en la matière, contribuant à leur succès et développement., * Piloter l'exploration, la conception et le déploiement de la mise en œuvre de la plateforme de données Azure et Databricks et la création de rapports MVP Power BI, incluant l'architecture cible, les pipelines d'ingestion de données et les configurations des services de données Azure et de la plateforme Databricks., Apporter soutien, conseils et partager les meilleures pratiques Azure avec les clients., Animer des sessions de transfert de connaissances, des démonstrations de solutions et rédiger une documentation détaillée des solutions., Entretenir une communication régulière et soutenue avec les clients lors des réunions de suivi, fournissant des mises à jour, des éclairages et des recommandations expertes pour stimuler la collaboration et encourager le succès., Organiser et orchestrer les sprint planning pour les membres de l'équipe assurant une répartition optimale des tâches et un bon déroulement des projets., Piloter l'exploration, la conception et le déploiement de la mise en œuvre de la plateforme de données Azure et la création de rapports MVP Power BI, incluant l'architecture cible, les pipelines d'ingestion de données et les configurations des services de données Azure., Élaborer une stratégie de plateforme de données Azure à l'échelle de l'entreprise pour répondre aux besoins prioritaires de la [CLIENT]., Documenter une architecture cible de référence alignés avec les principes clés et la vision [CLIENT]., Participer activement à la stratégie de Gestion du Changement et aux initiatives de gouvernance des rapports., Maintenir une communication soutenue et régulière avec les clients lors des réunions de suivi, apportant des mises à jour, des conseils avisés et des recommandations d'experts pour stimuler la collaboration et garantir le succès des projets., Piloter une équipe transversale de 12 membres pour unifier diverses sources de données au sein d'une architecture globale, permettant l'élaboration d'offres précieuses pour les clients biopharmaceutiques., Instaurer des méthodologies rigoureuses pour l'entretien et l'appui d'écosystèmes complexes dotés d'applications à fort impact., Planifier et orchestrer les emplois du temps des membres de l'équipe garantissant une attribution optimale des tâches et un bon déroulement des projets., Concevoir une solution intégrant Snowflake, Matillion, Qlik Replicate sur Azure et Power BI., Entretenir une communication soutenue et régulière avec les clients lors des réunions de suivi, apportant des mises à jour, des éclairages et des recommandations d'experts pour stimuler la collaboration et assurer le succès des projets., En tant que fervent défenseur des solutions innovantes et des analyses basées sur les données, j'ai joué un rôle crucial en tant que Responsable de Centre d'Excellence en Données et Analytique Avancée, contribuant au succès organisationnel grâce à des stratégies et technologies de pointe., Diriger le recrutement, la formation et le développement professionnel des membres de l'équipe, en leur fournissant un encadrement pratique et un soutien technique pour les aider à atteindre leurs objectifs., Surveiller et gérer plusieurs projets, fournissant des mises à jour régulières et des rapports de progression lors des réunions de comité de suivi., Élaborer et mettre en œuvre une feuille de route pluriannuelle pour les données et l'analyse, alignée sur les objectifs de l'[CLIENT] et comprenant l'architecture, la stratégie cloud, la gouvernance des données, l'analyse avancée avec l'apprentissage automatique, la littératie des données et la formation des équipes d'affaires., Collaborer avec diverses équipes, y compris l'architecture d'entreprise l'infrastructure, DevOps, la cybersécurité, la gestion de projet et les lignes d'affaires, pour créer un portefeuille de projets complet et présenter des stratégies, projets et démonstrations innovants., Initier un programme novateur visant à attirer et à former des talents émergents juniors, en leur offrant des opportunités variées pour travailler dans différentes lignes d'activité et développer leur expertise en matière de données et d'analyses avancées., Responsable de la conception technique détaillée, de la mise en œuvre et de l'évolution de la plateforme de gestion de données et intelligence d'affaire., Participer aux divers projets dans les phases d'architecture de solution, de données et d'intégration., Encadrer les équipes de développements en recommandant les orientations et les décisions techniques., Accompagner les différents intervenants lors du cycle de vie du développement des projets et des évolutions., Conceptualiser les solutions en considérant les principes directeurs et les ormes d'architecture corporative de la [CLIENT]., Etablir des stratégies et s'assurer de leur mise en place pour mieux gérer l'évolution de nos applications, de nos infrastructures, et assurer la pérennité de nos solutions existantes., Traiter et estimer les nouvelles demandes de sujets de données et d'intégration systèmes des différents projets d'affaires., Assurer la vigie des systèmes d'information et des technologies. --> Text length need to be between 0 and 5000 characters\n",
            "Translation error for '*    Dedicated PCO to a team delivering a 5-year software development project for the revamp of the application used by pharmacies to fill and bill prescriptions across all [CLIENT] and [CLIENT] pharmacies., Managed project financials for a team of 40 people., Setup project documentation in SharePoint, Teams, Confluence., Prepared and tracked the project plan in Microsoft Project., Participated in the organization of the project governance., Worked with software development teams using Agile methodologies and tools., *    Setup of Data Extraction team within [CLIENT] and outside vendors for 5 Divestiture Programs., Responsible for implementing tracking tools that enabled daily reporting on project activities and record keeping (SharePoint timesheet, activity-issue tracker, data folders)., Managed a complex monthly financial chargeback across multiple internal teams (monthly 250K)., Responsible for reviewing SOWs, preparing POs and invoice tracking with outside vendors., Oversee 2 portfolios: up to 80 projects at one time representing a $20M annual Portfolio budget., Responsible for processing all Purchase Orders for the projects as well as invoice approvals., Reviewed monthly cost reports and accruals with PMs; responsible for coaching and ensuring accurate forecasting and reporting done by the [CLIENT] team., Implemented improvements and consolidated various tracking tools used by the PMO and PCO teams., Mentor to junior PCOs as well as training new team of interns each semester., *    Audit and QA review all project management related documents (Charter, Financials, Project Management, Plan) submitted for the project gating process and manage the administration of the funding request., Coordinate the operationalization of the newly implemented project life cycle, which includes the update of templates, process improvements and documentation, and updates to the PMO SharePoint site., Responsible for promoting the project life cycle process through written communication to the IT Project community and the organization of group learning sessions, PM community meetings., Manage the administration of the Smartsheet project tracker tool: support to the PM community, develop eports and dashboards, improvements to existing processes and templates., Review and provide sign-off on project closures from a governance standpoint; measure adherence to project life cycle process, quality of deliverables, use of the available tools and templates., General go-to for project governance questions, issues, project tasks and administration (SharePoint eporting, change requests, project lifecycle, gating process, Smartsheet tracker tool)., *    Managed the project budgets for the Global IT Portfolio ($13M); setup budget tracking, acted as lead contact with finance, conducted monthly reviews with project managers, responsible for accrual process., Lead and organized the Project Review Boards and in turn prepared the Executive Dashboard (included project status, KPIs, Risks and Issues) for the CIO and regional IT Directors., Coordinated with the Portfolio team, the yearly Portfolio planning exercise which included the Global projects and the consolidation of regional project portfolios; all within a decentralized environment., Setup the SharePoint site for Global Portfolio which included creation of project templates (RAID logs), Project Status Reports, Dashboard, Charter, Project Closure) as well as repository for project financials contract management, team operations., Lead preparation of project plan for the Infrastructure Outsourcing transition team., Organized PMO forums with the objective of knowledge sharing of project processes across worldwide egional PMOs; part of a continuous improvement initiative., Coordinated transition activities for in-flight projects and integrate PMO processes with outsourcing partner., *    Program Control Officer for two projects sponsored by Finance function: Financial Planning & Analysis, $6M) and Enhanced Finance Controls and Analysis ($5M)., Managed project budgets with multiple sources of data, prepared IT/Business View, tracked multi-year project against the portfolio budget. Prepared monthly cost reports for PMO and finance teams., Lead the Daily Morning Meetings with Business and IT teams., Setup governance structure with program manager (business and IT participation)., Prepared Steering Committees presentations with PM and took meeting minutes during the meeting., Coordinated procurement activities for contracts with external suppliers., Setup project documentation and performed weekly tracking on risks, actions, issues, decisions., Prepared weekly status reports for key project stakeholders and PMO teams., Project Management Analyst, Revamped the main PMO reporting tool used across all IT Delivery teams; created the template, executed ain the trainer sessions and provide support post-implementation., Reporting tool encompassed weekly status on a project with embedded risk, action, issue, decision logs to facilitate automation and ease of reporting. Report aligned with the CIO's Project Portfolio tracking., *    Prepared and controlled project budgets for software development studio working in an Agile environment., Organized and tracked resource allocation for 60 resources delivering on-going projects., *    Lead and delivered projects successfully within a Project Management Office for a major client ([CLIENT])., Managed an average of 20 projects representing budget over $1M while respecting quality and maintaining focus on meeting [CLIENT] requirements and expectations., Controlled project plans that accounted for project risk, scope, schedule, and cost., Coordinated weekly team meetings with external [CLIENT]; Specialized in international projects and crossfunctional teams., Built proposals and pricing with technical teams that brought value to the [CLIENT] requirements., *    Managed and controlled the contractual and financial aspect of 15 on-going projects for [CLIENT] client., Ensured project health by using project management methodology and following business control processes., Created and monitored project work schedules and budgets., Focal point for 30 on-going proposals and responsible for reporting results to upper management., Trained new resources and undertook a team lead approach within Project Control team.': *    Dedicated PCO to a team delivering a 5-year software development project for the revamp of the application used by pharmacies to fill and bill prescriptions across all [CLIENT] and [CLIENT] pharmacies., Managed project financials for a team of 40 people., Setup project documentation in SharePoint, Teams, Confluence., Prepared and tracked the project plan in Microsoft Project., Participated in the organization of the project governance., Worked with software development teams using Agile methodologies and tools., *    Setup of Data Extraction team within [CLIENT] and outside vendors for 5 Divestiture Programs., Responsible for implementing tracking tools that enabled daily reporting on project activities and record keeping (SharePoint timesheet, activity-issue tracker, data folders)., Managed a complex monthly financial chargeback across multiple internal teams (monthly 250K)., Responsible for reviewing SOWs, preparing POs and invoice tracking with outside vendors., Oversee 2 portfolios: up to 80 projects at one time representing a $20M annual Portfolio budget., Responsible for processing all Purchase Orders for the projects as well as invoice approvals., Reviewed monthly cost reports and accruals with PMs; responsible for coaching and ensuring accurate forecasting and reporting done by the [CLIENT] team., Implemented improvements and consolidated various tracking tools used by the PMO and PCO teams., Mentor to junior PCOs as well as training new team of interns each semester., *    Audit and QA review all project management related documents (Charter, Financials, Project Management, Plan) submitted for the project gating process and manage the administration of the funding request., Coordinate the operationalization of the newly implemented project life cycle, which includes the update of templates, process improvements and documentation, and updates to the PMO SharePoint site., Responsible for promoting the project life cycle process through written communication to the IT Project community and the organization of group learning sessions, PM community meetings., Manage the administration of the Smartsheet project tracker tool: support to the PM community, develop eports and dashboards, improvements to existing processes and templates., Review and provide sign-off on project closures from a governance standpoint; measure adherence to project life cycle process, quality of deliverables, use of the available tools and templates., General go-to for project governance questions, issues, project tasks and administration (SharePoint eporting, change requests, project lifecycle, gating process, Smartsheet tracker tool)., *    Managed the project budgets for the Global IT Portfolio ($13M); setup budget tracking, acted as lead contact with finance, conducted monthly reviews with project managers, responsible for accrual process., Lead and organized the Project Review Boards and in turn prepared the Executive Dashboard (included project status, KPIs, Risks and Issues) for the CIO and regional IT Directors., Coordinated with the Portfolio team, the yearly Portfolio planning exercise which included the Global projects and the consolidation of regional project portfolios; all within a decentralized environment., Setup the SharePoint site for Global Portfolio which included creation of project templates (RAID logs), Project Status Reports, Dashboard, Charter, Project Closure) as well as repository for project financials contract management, team operations., Lead preparation of project plan for the Infrastructure Outsourcing transition team., Organized PMO forums with the objective of knowledge sharing of project processes across worldwide egional PMOs; part of a continuous improvement initiative., Coordinated transition activities for in-flight projects and integrate PMO processes with outsourcing partner., *    Program Control Officer for two projects sponsored by Finance function: Financial Planning & Analysis, $6M) and Enhanced Finance Controls and Analysis ($5M)., Managed project budgets with multiple sources of data, prepared IT/Business View, tracked multi-year project against the portfolio budget. Prepared monthly cost reports for PMO and finance teams., Lead the Daily Morning Meetings with Business and IT teams., Setup governance structure with program manager (business and IT participation)., Prepared Steering Committees presentations with PM and took meeting minutes during the meeting., Coordinated procurement activities for contracts with external suppliers., Setup project documentation and performed weekly tracking on risks, actions, issues, decisions., Prepared weekly status reports for key project stakeholders and PMO teams., Project Management Analyst, Revamped the main PMO reporting tool used across all IT Delivery teams; created the template, executed ain the trainer sessions and provide support post-implementation., Reporting tool encompassed weekly status on a project with embedded risk, action, issue, decision logs to facilitate automation and ease of reporting. Report aligned with the CIO's Project Portfolio tracking., *    Prepared and controlled project budgets for software development studio working in an Agile environment., Organized and tracked resource allocation for 60 resources delivering on-going projects., *    Lead and delivered projects successfully within a Project Management Office for a major client ([CLIENT])., Managed an average of 20 projects representing budget over $1M while respecting quality and maintaining focus on meeting [CLIENT] requirements and expectations., Controlled project plans that accounted for project risk, scope, schedule, and cost., Coordinated weekly team meetings with external [CLIENT]; Specialized in international projects and crossfunctional teams., Built proposals and pricing with technical teams that brought value to the [CLIENT] requirements., *    Managed and controlled the contractual and financial aspect of 15 on-going projects for [CLIENT] client., Ensured project health by using project management methodology and following business control processes., Created and monitored project work schedules and budgets., Focal point for 30 on-going proposals and responsible for reporting results to upper management., Trained new resources and undertook a team lead approach within Project Control team. --> Text length need to be between 0 and 5000 characters\n",
            "Translation error for 'Oversee the university's website redesign and admissions program in close collaboration with internal employees and consultants from various sectors., Identified, defined and established project objectives, budget needs, roles, responsibilities and roadmap., Creation of thorough project plans, migration roadmaps and strategies to ensure smooth project execution and within defined time and cost., Monitoring of project progress, risk and change management and regular communication with senior management., Oversee numerous BI IT projects in close collaboration with internal employees and consultants from various sectors., Involvement of the scrum master from project alignment to closure., Acting IT Support Team Manager, Oversee numerous IT projects in close collaboration with internal employees and consultants from various sectors., Identified, defined and established project objectives, budget needs, roles, esponsibilities and roadmap., Monitoring of project progress, isk and change management and regular communication with senior management., Oversaw numerous IT projects in close collaboration with internal employees and consultants from various sectors. Identified, defined, and established project objectives, budgetary needs, roles, responsibilities, and roadmap., Created in-depth project plans, migration roadmaps, and strategies to ensure smooth project execution and within set timeline and cost requirement., Monitored and communicated project progress on regular basis with senior management, Azure Migration (Cloud) and Server Migration (On-Prem), Evaluated, replaced, and decommissioned Microsoft BizTalk server and APEC systems., Jenkins Update, Web Stack Updates, Oversaw Salesforce optimizations and deployment in close collaboration with internal employees and consultants from various departments., Identified, defined, and established project objectives, budgetary needs, roles, responsibilities, and roadmap., Created in-depth project plans, migration roadmaps, and strategies to ensure smooth project execution, change management and within set timeline and cost requirement., Supervised and reported project progress on regular basis with senior management and stakeholders., Administered Salesforce customer relationship management (CRM) and FinancialForce (ERP Cloud) implementation for four customers., Managed projects from initial phase to delivery phase deployment in close collaboration with internal employees and consultants from various departments., Identified, defined, and established project objectives, roles, responsibilities, and roadmap., Formulated detailed project plans, migration roadmaps, and strategies to ensure smooth project execution, change management while meeting timeline and cost requirements., Monitored and communicated project progress on regular basis with executive management and stakeholders., Oversaw numerous IT projects in close collaboration with internal employees and consultants from various sectors., Monitored and communicated project progress on regular basis with senior management, while according with PMO standards., Updated project documentations, including charts, tables, plans, and diagrams for assistance in issue analysis and resolution., Organized periodic meeting with all levels of management for overseeing project changes/status, negotiations, and prioritizing work., Implemented Statistical Analysis System (SAS) solution for International Financial Reporting, Standards (IFRS), while managing $30M budget., Deployed IT Scrum and Kanban board for tracking day-to-day processes/activities identifying/mitigating technical bugs and risks, developing software, and ensuring quality assurance., Piloted project operations from inception to finalization which included plan development execution, transition stage, and resources allocation in line with established project scope and quality constraints., Projected and administered project budgets with minimal negative impact on budget., Delegated high performing quality assurance team, including lead QA and two analysts on project., Developed tactical project and program plans, while complying with business requirements., Directed team activities to ensure maximum productivity and performance in compliance with cost and time requirements., Planned, developed, and implemented projects through resources allocation and adherence with project scope, quality requirements, and timeframe., Forecasted, tacked, and managed project as well as program budgets in efficient manner., Oversaw technical delivery of projects and programs as scheduled., Maintained charts, tables, diagrams, and plans with up-to-date information for facilitating risk analysis and rectification., Deployed Oracle Enterprise Resource Planning (ERP) Cloud by managing budget of around $40M and 25k p-d., Successfully installed IT Scrum and Kanban board for keeping abreast with existing processes/operations, software development, bugs identification/mitigation, and quality assurance., Controlled project costs, timelines, and daily activities in line with established project roadmap., Scrutinized project progress to identify and report areas of improvement by delivering ecommendations., Generated and maintained various reports regarding projects, records, dashboards, meetings progress for communicating with project office, manager, and management committees., Spearheaded five envelope projects (Identity and Access Management, EPM, GEN IV, System Update) of worth Gen$35M throughout completion within deadline and budget., Ensured imely project deliverables by overseeing changes requests, project closure, and chart progress board., Coached and guided project leaders for efficiently leveraging project/practical management tools, budget management/planning, and documentation control., Stowed and automated connection between Identity and Access Management system and HR system through roles/account deployment, risk analysis/identification, orphan accounts closure, user ID consolidation, and workflow implementation., Revamped project office by developing and executing project plans and standards/procedures optimizing EPM, formulating templates, and maintaining software applications.': Oversee the university's website redesign and admissions program in close collaboration with internal employees and consultants from various sectors., Identified, defined and established project objectives, budget needs, roles, responsibilities and roadmap., Creation of thorough project plans, migration roadmaps and strategies to ensure smooth project execution and within defined time and cost., Monitoring of project progress, risk and change management and regular communication with senior management., Oversee numerous BI IT projects in close collaboration with internal employees and consultants from various sectors., Involvement of the scrum master from project alignment to closure., Acting IT Support Team Manager, Oversee numerous IT projects in close collaboration with internal employees and consultants from various sectors., Identified, defined and established project objectives, budget needs, roles, esponsibilities and roadmap., Monitoring of project progress, isk and change management and regular communication with senior management., Oversaw numerous IT projects in close collaboration with internal employees and consultants from various sectors. Identified, defined, and established project objectives, budgetary needs, roles, responsibilities, and roadmap., Created in-depth project plans, migration roadmaps, and strategies to ensure smooth project execution and within set timeline and cost requirement., Monitored and communicated project progress on regular basis with senior management, Azure Migration (Cloud) and Server Migration (On-Prem), Evaluated, replaced, and decommissioned Microsoft BizTalk server and APEC systems., Jenkins Update, Web Stack Updates, Oversaw Salesforce optimizations and deployment in close collaboration with internal employees and consultants from various departments., Identified, defined, and established project objectives, budgetary needs, roles, responsibilities, and roadmap., Created in-depth project plans, migration roadmaps, and strategies to ensure smooth project execution, change management and within set timeline and cost requirement., Supervised and reported project progress on regular basis with senior management and stakeholders., Administered Salesforce customer relationship management (CRM) and FinancialForce (ERP Cloud) implementation for four customers., Managed projects from initial phase to delivery phase deployment in close collaboration with internal employees and consultants from various departments., Identified, defined, and established project objectives, roles, responsibilities, and roadmap., Formulated detailed project plans, migration roadmaps, and strategies to ensure smooth project execution, change management while meeting timeline and cost requirements., Monitored and communicated project progress on regular basis with executive management and stakeholders., Oversaw numerous IT projects in close collaboration with internal employees and consultants from various sectors., Monitored and communicated project progress on regular basis with senior management, while according with PMO standards., Updated project documentations, including charts, tables, plans, and diagrams for assistance in issue analysis and resolution., Organized periodic meeting with all levels of management for overseeing project changes/status, negotiations, and prioritizing work., Implemented Statistical Analysis System (SAS) solution for International Financial Reporting, Standards (IFRS), while managing $30M budget., Deployed IT Scrum and Kanban board for tracking day-to-day processes/activities identifying/mitigating technical bugs and risks, developing software, and ensuring quality assurance., Piloted project operations from inception to finalization which included plan development execution, transition stage, and resources allocation in line with established project scope and quality constraints., Projected and administered project budgets with minimal negative impact on budget., Delegated high performing quality assurance team, including lead QA and two analysts on project., Developed tactical project and program plans, while complying with business requirements., Directed team activities to ensure maximum productivity and performance in compliance with cost and time requirements., Planned, developed, and implemented projects through resources allocation and adherence with project scope, quality requirements, and timeframe., Forecasted, tacked, and managed project as well as program budgets in efficient manner., Oversaw technical delivery of projects and programs as scheduled., Maintained charts, tables, diagrams, and plans with up-to-date information for facilitating risk analysis and rectification., Deployed Oracle Enterprise Resource Planning (ERP) Cloud by managing budget of around $40M and 25k p-d., Successfully installed IT Scrum and Kanban board for keeping abreast with existing processes/operations, software development, bugs identification/mitigation, and quality assurance., Controlled project costs, timelines, and daily activities in line with established project roadmap., Scrutinized project progress to identify and report areas of improvement by delivering ecommendations., Generated and maintained various reports regarding projects, records, dashboards, meetings progress for communicating with project office, manager, and management committees., Spearheaded five envelope projects (Identity and Access Management, EPM, GEN IV, System Update) of worth Gen$35M throughout completion within deadline and budget., Ensured imely project deliverables by overseeing changes requests, project closure, and chart progress board., Coached and guided project leaders for efficiently leveraging project/practical management tools, budget management/planning, and documentation control., Stowed and automated connection between Identity and Access Management system and HR system through roles/account deployment, risk analysis/identification, orphan accounts closure, user ID consolidation, and workflow implementation., Revamped project office by developing and executing project plans and standards/procedures optimizing EPM, formulating templates, and maintaining software applications. --> Text length need to be between 0 and 5000 characters\n",
            "Translation completed and saved as 'merged_data_translated.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does not work well with very long descriptions, so translation error"
      ],
      "metadata": {
        "id": "paZ0qefkcrH5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkTQxShWeLMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load both datasets\n",
        "df_original = pd.read_csv('merged_data.csv')\n",
        "df_translated = pd.read_csv('merged_data_translated.csv')\n",
        "\n",
        "# Compare shape (rows, columns)\n",
        "print(f\"Original Dataset: {df_original.shape[0]} rows, {df_original.shape[1]} columns\")\n",
        "print(f\"Translated Dataset: {df_translated.shape[0]} rows, {df_translated.shape[1]} columns\")\n",
        "\n",
        "# Check if any rows were lost or columns were added/removed\n",
        "if df_original.shape == df_translated.shape:\n",
        "    print(\"✅ Both datasets have the same number of rows and columns.\")\n",
        "else:\n",
        "    print(\"⚠️ There is a mismatch in rows or columns!\")\n",
        "\n",
        "# Check if an extra column 'mission_dsc_eng' was correctly added\n",
        "if 'mission_dsc_eng' in df_translated.columns:\n",
        "    print(\"✅ 'mission_dsc_eng' column is present in the translated dataset.\")\n",
        "else:\n",
        "    print(\"❌ 'mission_dsc_eng' column is missing!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11OW2CacU0q",
        "outputId": "d8968eb1-e715-4762-dddd-dcb80d3b33bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset: 119 rows, 30 columns\n",
            "Translated Dataset: 119 rows, 31 columns\n",
            "⚠️ There is a mismatch in rows or columns!\n",
            "✅ 'mission_dsc_eng' column is present in the translated dataset.\n"
          ]
        }
      ]
    }
  ]
}