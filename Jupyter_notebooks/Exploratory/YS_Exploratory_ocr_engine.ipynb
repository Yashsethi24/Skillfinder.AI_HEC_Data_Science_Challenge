{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 95606,
          "databundleVersionId": 11453440,
          "sourceType": "competition"
        },
        {
          "sourceId": 11157090,
          "sourceType": "datasetVersion",
          "datasetId": 6961322
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reading files from OCR"
      ],
      "metadata": {
        "id": "LIWB_ZAWw21r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt install libtesseract-dev\n",
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr-fra\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install transformers torch spacy pandas\n",
        "!pip install textblob_fr"
      ],
      "metadata": {
        "id": "jQJ6ekHfvwwj",
        "outputId": "f5d0630d-1685-43a3-a375-75465ff77b6a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T04:32:59.652400Z",
          "iopub.execute_input": "2025-03-26T04:32:59.652802Z",
          "iopub.status.idle": "2025-03-26T04:33:32.151435Z",
          "shell.execute_reply.started": "2025-03-26T04:32:59.652738Z",
          "shell.execute_reply": "2025-03-26T04:33:32.150200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,334 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.3 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 1s (2,889 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 126256 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-fra\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 527 kB of archives.\n",
            "After this operation, 1,145 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fra all 1:4.00~git30-7274cfa-1.1 [527 kB]\n",
            "Fetched 527 kB in 1s (612 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-fra.\n",
            "(Reading database ... 126389 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-fra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=b18e014bd8f003d7e3b95e14da059a09b6c3f686014744a008a718ada84e62d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.68.2 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.18 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.7.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# pytesseract.pytesseract.tesseract_cmd = r'/kaggle/working/tesseract' # Set the path to your tesseract executable"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T04:34:15.191247Z",
          "iopub.execute_input": "2025-03-26T04:34:15.191795Z",
          "iopub.status.idle": "2025-03-26T04:34:15.195492Z",
          "shell.execute_reply.started": "2025-03-26T04:34:15.191764Z",
          "shell.execute_reply": "2025-03-26T04:34:15.194636Z"
        },
        "id": "B3Xp58FdGCKo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "# from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from googletrans import Translator\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from textblob_fr import PatternTagger, PatternAnalyzer\n",
        "\n",
        "def translate_french_to_english(text):\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(text, src='fr', dest='en')\n",
        "    return translation.text\n",
        "\n",
        "def ocr_pdf(pdf_path, language='eng'):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file using OCR.\n",
        "\n",
        "    Args:\n",
        "    pdf_path: Path to the PDF file.\n",
        "    language: Language of the text ('eng' for English, 'fra' for French).\n",
        "\n",
        "    Returns:\n",
        "    Extracted text as a string.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = ''\n",
        "    for page in doc:\n",
        "        pix = page.get_pixmap()\n",
        "        # Create a PIL Image object from the pixmap bytes\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        text += pytesseract.image_to_string(img, lang=language)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans job description text before feeding it into a model.\"\"\"\n",
        "    # Remove unwanted newline characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Replace OCR-specific artifacts\n",
        "    text = re.sub(r\"\\+ _\", \"•\", text)  # Standardize bullet points\n",
        "    text = re.sub(r\"{|}\", \"\", text)  # Remove curly braces\n",
        "    text = re.sub(r\"(?<!\\d),(?!\\d)\", \"\", text)  # Remove misplaced commas\n",
        "\n",
        "    return text\n",
        "\n",
        "def correct_ocr_errors_fr(text):\n",
        "    \"\"\"Automatically correct OCR errors using TextBlob-FR.\"\"\"\n",
        "\n",
        "    # Remove unwanted characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove or standardize bullet points\n",
        "    text = re.sub(r\"\\+\\.\", \"\", text)  # Remove '+.' (OCR bullet point misinterpretation)\n",
        "    text = re.sub(r\"\\+\", \"\", text)    # Remove standalone '+'\n",
        "    # Apply French grammar and spelling correction\n",
        "    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
        "    corrected_text = str(blob.correct())\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "def correct_ocr_errors_en(text):\n",
        "    \"\"\"Automatically corrects OCR errors using TextBlob for English text.\"\"\"\n",
        "\n",
        "    # Remove unwanted characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Apply English grammar and spelling correction\n",
        "    blob = TextBlob(text)\n",
        "    corrected_text = str(blob.correct())\n",
        "\n",
        "    return corrected_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T06:13:37.347992Z",
          "iopub.execute_input": "2025-03-26T06:13:37.348357Z",
          "iopub.status.idle": "2025-03-26T06:13:37.359506Z",
          "shell.execute_reply.started": "2025-03-26T06:13:37.348329Z",
          "shell.execute_reply": "2025-03-26T06:13:37.358117Z"
        },
        "id": "DUUASLUXGCKp"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import pytesseract\n",
        "from googletrans import Translator\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from textblob_fr import PatternTagger, PatternAnalyzer\n",
        "import time\n",
        "\n",
        "def translate_french_to_english(text):\n",
        "    translator = Translator()\n",
        "    # Add error handling and retry mechanism\n",
        "    for _ in range(3):  # Retry up to 3 times\n",
        "        try:\n",
        "            translation = translator.translate(text, src='fr', dest='en')\n",
        "            return translation.text  # Return if successful\n",
        "        except TypeError as e:\n",
        "            print(f\"Translation error: {e}. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "    return None  # Return None if all retries fail\n",
        "\n",
        "def ocr_pdf(pdf_path, language='eng'):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file using OCR.\n",
        "\n",
        "    Args:\n",
        "    pdf_path: Path to the PDF file.\n",
        "    language: Language of the text ('eng' for English, 'fra' for French).\n",
        "\n",
        "    Returns:\n",
        "    Extracted text as a string.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = ''\n",
        "    for page in doc:\n",
        "        pix = page.get_pixmap()\n",
        "        # Create a PIL Image object from the pixmap bytes\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        text += pytesseract.image_to_string(img, lang=language)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans job description text before feeding it into a model.\"\"\"\n",
        "    # Remove unwanted newline characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Replace OCR-specific artifacts\n",
        "    text = re.sub(r\"\\+ _\", \"•\", text)  # Standardize bullet points\n",
        "    text = re.sub(r\"{|}\", \"\", text)  # Remove curly braces\n",
        "    text = re.sub(r\"(?<!\\d),(?!\\d)\", \"\", text)  # Remove misplaced commas\n",
        "\n",
        "    return text\n",
        "\n",
        "def correct_ocr_errors_fr(text):\n",
        "    \"\"\"Automatically correct OCR errors using TextBlob-FR.\"\"\"\n",
        "\n",
        "    # Remove unwanted characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove or standardize bullet points\n",
        "    text = re.sub(r\"\\+\\.\", \"\", text)  # Remove '+.' (OCR bullet point misinterpretation)\n",
        "    text = re.sub(r\"\\+\", \"\", text)    # Remove standalone '+'\n",
        "    # Apply French grammar and spelling correction\n",
        "    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
        "    corrected_text = str(blob.correct())\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "def correct_ocr_errors_en(text):\n",
        "    \"\"\"Automatically corrects OCR errors using TextBlob for English text.\"\"\"\n",
        "\n",
        "    # Remove unwanted characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Apply English grammar and spelling correction\n",
        "    blob = TextBlob(text)\n",
        "    corrected_text = str(blob.correct())\n",
        "\n",
        "    return corrected_text"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3rWGN853JUSS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing PDF files\n",
        "pdf_directory = \"/content/\"  # Adjust this path\n",
        "\n",
        "# List of PDF files\n",
        "pdf_files = [\"Scrum.pdf\", \"Data Engineer.pdf\", \"Data Analyst.pdf\"]\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Process each PDF file\n",
        "for pdf in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_directory, pdf)\n",
        "\n",
        "    # Extract text using OCR\n",
        "    extracted_text = correct_ocr_errors_fr(clean_text(ocr_pdf(pdf_path, language='fra')))\n",
        "\n",
        "    # Translate to English\n",
        "    translated_text = correct_ocr_errors_en(translate_french_to_english(extracted_text))\n",
        "\n",
        "    # Append data to the list\n",
        "    data.append({\n",
        "        \"demand_id\": os.path.splitext(pdf)[0],\n",
        "        \"extracted_text\": extracted_text,\n",
        "        \"translated_text\": translated_text\n",
        "    })\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aZJPwgboILow",
        "outputId": "199eb271-66ce-4aab-a859-6581c7e6f0e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Scrum.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "hTKloC7aKsbB",
        "outputId": "040c548c-d576-4cf0-a2cd-fb083eabbbf8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       demand_id                                     extracted_text  \\\n",
              "0          Scrum  Serum Master- Contrat de 12 moist Dieu : Montr...   \n",
              "1  Data Engineer  Most: Data Engineer  Type de contrat: Contrat ...   \n",
              "2   Data Analyst  Profit Data Analyst - Marketing Analytics. A p...   \n",
              "\n",
              "                                     translated_text  \n",
              "0  Serum Master- Contract of 12 Lists God: Montre...  \n",
              "1  Most: Data Engineer Type of contract: Contract...  \n",
              "2  Profit Data Analyst - Marketing Analytics.Abou...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15032b69-c8b6-4b45-8bd1-a911acc3b777\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>demand_id</th>\n",
              "      <th>extracted_text</th>\n",
              "      <th>translated_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scrum</td>\n",
              "      <td>Serum Master- Contrat de 12 moist Dieu : Montr...</td>\n",
              "      <td>Serum Master- Contract of 12 Lists God: Montre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Engineer</td>\n",
              "      <td>Most: Data Engineer  Type de contrat: Contrat ...</td>\n",
              "      <td>Most: Data Engineer Type of contract: Contract...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Profit Data Analyst - Marketing Analytics. A p...</td>\n",
              "      <td>Profit Data Analyst - Marketing Analytics.Abou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15032b69-c8b6-4b45-8bd1-a911acc3b777')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15032b69-c8b6-4b45-8bd1-a911acc3b777 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15032b69-c8b6-4b45-8bd1-a911acc3b777');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f76d3411-e39c-4eef-b9e2-4127b4dd4255\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f76d3411-e39c-4eef-b9e2-4127b4dd4255')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f76d3411-e39c-4eef-b9e2-4127b4dd4255 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9329bd84-ba3b-4647-88c1-1873e97c0ee9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9329bd84-ba3b-4647-88c1-1873e97c0ee9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"demand_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Scrum\",\n          \"Data Engineer\",\n          \"Data Analyst\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extracted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Serum Master- Contrat de 12 moist Dieu : Montreal Late de d\\u00e9marrage : Was que possible Sure : 12 moist avec possibility de renouvellement Dans He care d'un mandate d'importance strat\\u00e9gique nous recherchons un(e) Serum Master experiment(e) pour guide une quite Agile multidisciplnaire au sein d'un environment stimulant et en line transformation num\\u00e9rique. Tom de l'enterprise : Alimora Troupe Slogan : Cuttiver l'avenue burri l'excellence Presentation de l'enterprise Fond en 1998, Alimora Troupe est un after major de l'agroalimentaire unable en Am\\u00e9rique du Word. Sp\\u00e9cialis\\u00e9 dans la production la transformation et a distribution de products alimentaires de quality He group s'engage \\u00e0 affair une lamentation spine et accessible tout en respecting l'environment et les communaut\\u00e9s Scales. Avec plus de 1 500 employs r\\u00e9partis sur 7 sites de production et une presence dans plus de 12 pays Alimora se distinguee par On innovation constant son excellence \\u2018op\\u00e9rationnelle et sa capacity \\u00e0 anticiper les tendencies de consolation. Domaines d'expertise : \\u2022 Transformation de products frail (fruits l\\u00e9gumes proteins v\\u00e9g\\u00e9tales)  Production big et circuits courts \\\" Recherche & D\\u00e9veloppement en nutrition unable  Logistique int\\u00e9gr\\u00e9e et chain du from  D\\u00e9veloppement de products \\u00e0 marque prince (MDD) Able etresponsabilit\\u00e9s Instant que Serum Master vous jousrez un rule all dans le pillage de l'agility op\\u00e9rationnelle d'une quite en favorisant l'attainted des objections d'affaires tout en renfor\\u00e7ant la collaboration la transparent et l'am\\u00e9lioration continue. Os responsabilit\\u00e9s incident notamment Letter en place et maintenir un care Agile efficacy produce \\u00e0 A performance collective. Facliter l'resemble des ceremonies Serum (Daily Serum Print Planning Print Review Introspective Refinement Assured une question practice des obstacles akin de garantir He on d\\u00e9roulement des livrables. Accompagner l'quite dans le d\\u00e9veloppement de son autonomy et de sa capacity d'auto-organisation. AAgiren want que coach Agile on giant l'quite etes parties prenantes dans A comprehension et l'application des values principes et pratiques agile. Promouvoir une culture de collaboration et d'am\\u00e9lioration continue. Wider le Product Owner \\u00e0 grew efficacement le backing product \\u00e0 d\\u00e9finir Yes priority et \\u00e0 maximiser A valour live. Securer et analyse A performance de l'quite via des indicates close (velocity burndown charts lead time etc) en utilisant des duties adapted. Prot\\u00e9ger l'quite des interference external et veiled \\u00e0 maintenir un climate de travail herein. Environnement technologique et duties utilizes Duties de question Agile : era Confluence Communication et collaboration : Microsoft Beams Black Fire Tableaux de word et m\\u00e9triques : era Dashboard Over of Expel advance M\\u00e9thodologies : Serum Kanban SAFe (about Mean Ranges : Francais (essential Anglaise (fonctionnel) Profit recherch\\u00e9  Minimum 3 and d' fort enter: experience en want que Serum Master dans un context Agile \\u00e0  Police comprehension des cares m\\u00e9thodologiques Miles (Serum Kanban Mean) et capacity \\u00e0 adapted les pratiques au context de l'quite. \\u2022 Excellentes competence interpersonnelles : leadership diplomatic capacity d'influence.  Fortespri d'analyse et capacity \\u00e0 traduire les probl\\u00e9matiques en actions concrete  Experience dans un context bride ou multi-squires est un about He mandate vous permettra de contributed achievement \\u00e0 A russie d'une quite Agile \\u201cengage out en \\u00e9voluant dans un environment produce \\u00e0 la transformation num\\u00e9rique et \\u00e0 l'innovation.\",\n          \"Most: Data Engineer  Type de contrat: Contrat \\\" Late de debut: Was que possible  Sure du mandate: 12mois  Late limited de depot: 6 november \\u00e0 oh Propos de enterprise RetailNova out un Leader du setter du commerce de detail avec un rameau de distribution omnicanal present \\u00e0 traders tout He territory. L'enterprise combine innovation technologique et excellence op\\u00e9rationnelle pour optimism ses processes anticiper Yes lesions des clients et maximiser la performance. Dans He care de sa strategic de transformation num\\u00e9rique RetailNova invested massivement dans A modernisation de ses. infrastructure de donned pour acc\\u00e9l\\u00e9rer a rise de decision fond sur des analysis Context du mandate : RetailNova met en sure une nouvelle platform d'enterprise base sur des technologies cloud akin de centralized s\\u00e9curiser et valoriser ses donned op\\u00e9rationnelles transactionnelles et clients. He project strat\\u00e9gique vise \\u00e0 moderniser l'\\u00e9cosyst\\u00e8me analytique \\u00e0 automatiser Yes klux de donned \\u00e0 am\\u00e9liorer la gouvernance et \\u00e0 enforce A quality et la tra\\u00e7abilit\\u00e9 des donned \\u00e0 l'\\u00e9chelle de l'enterprise. He Data Engineer sera responsible de la conception du d\\u00e9veloppement de l'optimisation t du d\\u00e9ploiement des pipelines de donned n\\u00e9cessaires \\u00e0 l'lamentation du data lake d'enterprise. Linterviendra \\u00e9galement dans A rise en place de nouvelles architecture de traitement en temps red en assurance la robustesse A scalabiit\\u00e9 tell performance des solutions d\\u00e9ploy\\u00e9es. Responsabilit\\u00e9s d\\u00e9taill\\u00e9es :  Concevoir et developer des pipelines optimism \\u00e0 l'aide d'Azure Data Factory pour int\\u00e9grer des donned covenant de sources muttiptes (ERP CRM POS e-commerce etc. ingestion de donned routes et D\\u00e9velopper des transformation complete en utilisant Apache Park (Scala/Python) dans Azure Databricks avec une force orientation performance et optimisation des coats. Impl\\u00e9menter des klux de donned streaming et batch via Databricks Streaming et Apache Vaska en assurance une fable patience et une haute disponibilit\\u00e9. Participer \\u00e0 la definition et \\u00e0 A rise en sure de nodules de donned adapted aux lesions analytiques (data parts data warehouse). Letter en sure des processes de variation de A quality des donned (data quality checks data profiting monitoring automatic) Automatiser les d\\u00e9ploiements et Yes tests des pipelines via Azure DevOps (of/of versioning rollback Documenter es solutions d\\u00e9velopp\\u00e9es salon Yes holmes internet et assured A tra\\u00e7abilit\\u00e9 tell r\\u00e9plicabilit\\u00e9 des traitements. Travailler en collaboration avec Yes squires Data Science Ll et T Infrastructure pour garantir l'alignement des livrables avec Yes objections strat\\u00e9giques. Participer \\u00e0 l'\\u00e9laboration de holmes et de standards d'ing\\u00e9nierie des donned \\u00e0 l'\\u00e9chelle de l'organisation. Comp\\u00e9tences technique requires : 8 and d'experience en d\\u00e9veloppement d'applications de traitement de donned distributed avec Apache Park (obligatory) Police maitresse de Python et Scala pour le d\\u00e9veloppement de traitements de donned volumineuses. Experience significative avec Azure Data Factory Azure Data Take Storage (Men) et Azure Databricks. Connaissance approfondie de Vaska (installation configuration question des topics S\\u00e9curit\\u00e9 monitoring). Maitrise des principes d'architecture data : partitionnement indication optimisation des formats (Parquet Felt Take) question des m\\u00e9tadonn\\u00e9es. Experience avec Yes duties de question des klux of/of dans Azure DevOps pipelines artefacts question de configuration)  Connaissance des principes de gouvernance des donned de catalogue (ex. Azure Purview) et des bones pratiques de security (RBAC encryption making).  Capacity \\u00e0 concevoir des nodules de donned analytiques (Kimball Data Fault) t \\u00e0 traveller avec des entrep\\u00f4ts de donned (ex. Synapse Analytics) Stout :  Experience dans des environnements retail ou grande distribution [question des donned transactionnelles question de A Supply chain etc  Experience avec des duties de monitoring des pipelines ex : Azure Monitor Dog Analytics Datadog).  Connaissances de base en Data Science/Machine Learning pour interagir avec Yes squires concerned. Comp\\u00e9tences interpersonnelles :  More capacity \\u00e0 r\\u00e9soudre des problems complete de manure autonomy.  Rigueur et attention aux details particuli\\u00e8rement dans Yes aspects lips \\u00e0 A quality la liability des donned.  Comp\\u00e9tences en communication technique capacity \\u00e0 document et presented des solutions technique \\u00e0 divers niveaux d'interlocuteurs.  Spirit d'quite overture au partake de connaissances et participation active \\u00e0 l'am\\u00e9lioration continue. Ranges :  Francais et anglaise: ma\\u00eetrise professionnelle complete want \\u00e0 l'oral qu'\\u00e0 l'merit\",\n          \"Profit Data Analyst - Marketing Analytics. A propos de Veltrida Vettrixia Technologies And. estate enterprise specialise dans Yes solutions de donned etlinteligence marketing. Was \\u00e0 Montreal Veltrixia accompany depuis plus de 15 and des enterprises word-am\\u00e9ricaines dans l'optimisation de pleurs performances grace \\u00e0 des technologies analytiques advances. L'enterprise rise sur l'innovation la collaboration et l'excellence pour propulser ses clients very He future num\\u00e9rique. Context Dans He care du d\\u00e9veloppement de notre quite analytique marketing nous recherchons une analyst de donned pour traveller sur des projects lips \\u00e0 la collected l'analyse tell visualisation de donned marketing. Vous were responsible de la maintenance et de L'evolution de yeux de donned covenant de sources varies akin de soutenir Yes squires dans peur rise de decision base sur Yes donned. Details du post  Sure : 10 moist (renouvelable)  Type: Temps plain \\\" Rode de travail : Bride (2 hours par remained au bureau \\u00e0 Montreal) Responsabilit\\u00e9s principles \\\" Rarer et am\\u00e9liorerLes yeux de donned marketing. \\\" D\\u00e9velopper des tableaux de word et reports dans Power of \\\" Optimiserles performances des datasets Power of. \\u2022 Int\\u00e9grer les donned issues de diverse platforms (Google Analytics CRM duties marketing)  R\\u00e9diger du code SQL DAX et Power Query performance. Gollaborer \\u00e9troitement avec Yes anatystes pour comprendre Years remains. Order des requires dans Databricks et SSMS. Assured a quality et a liability des donned. Soutenir les call\\u00e8gues via des review de made et du support technique. Concevoir des nodules de donned adapted aux objections matters. Profit recherch\\u00e9 Comp\\u00e9tences technique Minimum 8 and d'experience en analyse de donned. Police maitresse de SQL (of SQL Server Power I Databricks. Exp\\u00e9rionce avec DAX Power Query ETL. Connaissance des donned marketing un about. Capacity \\u00e0 mod\\u00e9liser Yes donned et \\u00e0 optimism les processes. Comp\\u00e9tences personnelles Furieuse autonomy rigoureux se. Bonne question du temps et spirit d'quite. Excellent capacity d'analyse et de resolution de problems. Ranges Maitrise de l'anglaise et du francais.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"translated_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Serum Master- Contract of 12 Lists God: Montreal Late Starting: WAS that possible sure: 12 MONTH with possibility of renewal in he care of a mandate of strategic importance He are looking for a Serum Master Experiment (E) for Guide an Agile Multidpilar Quite within a stimulating Environment and in Line Digital Transformation.Tom de l'Enterprise: Alimora Slogan troop: Cultivating Burri Avenue Excellence Presentation of the Enterprise Background in 1998, Alimora Troupe is an after major of the Word's united food industry.Specializing in production The processing and distribution of quality food products of Group undertakes to affect a spine and accessible lamentation while respecting the environment and the Scales communities.With more than 1,500 Employs spread over 7 production sites and a presence in more than 12 Alimora countries is distinguished by constant innovation His excellence \\u2018operational and its capacity to anticipate consolation tendencies.Remains of expertise: \\u2022 Transformation of Trail Products (Fruits Vegetable Vegetables) Fig production and short circuits \\\"Research & Development in Unable Nutrition Integrated Logistics and Chain of the Development of Prince Grand Prince Products (MDD) Able and Instant Authorities that Serum Master you will house an all in the looting of the operational agilityReinforce the collaboration of transparent and continuous improvement.and its self-organization capacity.Promote a culture of collaboration and continuous improvement.Wider He Product Owner in Grew effectively backing product to define Yes Priority and to minimize a Valour Give.Secure and analysis of the quite performance via closed indicates (Velocity Burndown Charts Head Time, etc.) using adapted dies.Protect the Quite from external interference and veiled to maintain a Herein work climate.Technological environment and dies utilizes dies of agile question: era confluence communication and collaboration: Microsoft beams black fire paintings of word and merits: era dashboard over of expel advance methodologies: serum kansas safe (About mean Ranges: French (English essential (functional) profit sought minimum 3 andthat Serum Master in an agile police with police understanding of Miles (Serum Kanban Mean) and Capacity to adapt the practices of the Quite Context.Multi-Squire is an about he mandate will allow you to contribute to a Russia of an agile quite \\u201cengaged out by revolving in an environmental product for digital transformation and innovation.\",\n          \"Most: Data Engineer Type of contract: Contract \\\"Late Debut: Was as possible sure of the mandate: 12 months Late Limited of Depot: November 6 with Oh words of Enterprise Retailnova But A leader of the Detail Trade Letter with an Omnichannel distribution branch present in Traders all of Territory. Enterprise combined technological innovation and operational excellence for optimism its processesAnticipate YES Lesions des Customers and minimize the performance. In the Are of its Retailnova digital transformation strategic invested passively.Its transaction and customer operational donations.He Data Engineer will be responsible for the design of the development of the employment of the Donned pipelines necessary for the lamentation of the Data Take of Enterprise.In addition to the new RED treatment architecture in insurance in insurance in place in the Robustness to Calamity Well Performance of the solutions deployed.Detailed responsibilities: Design and develop optimism pipelines using Azure Data Factory to integrate Muttiptes Covenant (ERP CRM POS e-commerce etc. Ingestion of Fond Routes and develop complete transformation by using Apache Dark (Scala/Python) in Azure Databricks with a performance orientation force.Implement Klux of Donned Streaming and Watch via Databricks Streaming and Apache Vaska in insurance patience and high availability.Data Growing Monitoring Automatic) Automate employments and yes tests of pipelines via Azure Devops (of/Of versioning rollback Document of Solutions developed living room Yes Holmes Internet and Insured A TRACEBILITY TELL REPLICABILITY of TREATMENTS.Work in collaboration with Yes Squires Data Science of and T infrastructure to guarantee the ligament of deliverables with strategic objections.Participate in the development of Holmes and Engineering Standards of Data on the scale of the organization.Technical skill require: 8 and experience in development of Boned Distributed processing applications with Apache Dark (OblITory) Pontresse de Python and Scala for the development of voluminous data treatments.Significant experience with Azure Data Factory Azure Data Take Storage (MEN) and Azure Databricks.In -depth knowledge of Vaska (Installation Configuration Question of topics Safety Monitoring).Mastery of the principles of architecture data: petitioning indication Optimization of formats (Felt Take parquet) Question of metadata.Experience with Yes Duties of the Klux of/Of in Azure Devops Pipelines Artefacts Configuration Question) Knowledge of the principles of governance of the catalogue given (eg Azure Purview) and the practices of Security (Back Encryption Taking).Capacity to design analytical donations nodules (Kimball Data Fault) t in Traveler with Donned warehouses (e.g. Synapse Analytics) Stout: Experience in retail or large distribution environment [question of transaction data question of A Supply Chain etc experience with dies of monitoring of pipelines ex:Analytics Datadog).Basic knowledge in Data Science/Machine Learning to interact with Yes Squire concerned.Interpersonal skill: More Capacity to solve Complete Manure Problems.Rigor and attention to details, particularly in yes aspects lips to a quality the liability of the data.Kills in technical communication capacity with document and presented technical solutions at various levels of interlocutor.Spirit of Quite Overture in the Sharing of Knowledge and Active Participation in continuous improvement.Ranges: French and English: Complete Want Professional Masters morally that at Merit\",\n          \"Profit Data Analyst - Marketing Analytics.About Veltrida Vettrixia Technologies and.Enterprise Specialized in Yes Solutions of Boned Etlinteligence Marketing.Was to Montreal Veltrixia accompanying for more than 15 and Word-American enterprises in optimizing crying thanks to Advances analytical technologies.Enterprise RISE on innovation collaboration and excellence to proper its very of Future Digital customers.Context in he care of the development of our Marketing Analytical Quite He are looking for an analyst of Boned for Traveler on projects lips to the collection Well Visualization of Boned Marketing.You are responsible for maintenance and evolution of Boned Covenant of Skin Sources Varies to support Yes Squires in fear of Decision Based on Yes Boned.Details of the sure post: 10 MONT (renewal) Type: Plain time \\\"Work Rode: Ride (2 Hours per Remained at the Bureau in Montreal) Principle responsibilities\\\" Are and Improve the Boned Marketing eyes.\\\"Develop Word and Reports Tables in Power of\\\" Optimize Power of Datasets Performances.\\u2022 Integrate Bones from various platforms (Google Analytics CRM Duties Marketing) Write SQL Tax code and Power Query Performance.Closely to be clerk with Yes Anatysts to understand Years Remains.Order requires in Databricks and SSMS.Insured a Quality and Liabibility des Fond.Support the Call\\u00e8gues via Made reviews and technical support.Design Nodules from Wonted Adapted to Matters objections.Profit sought Minimum Technical Kills 8 and Experience in Boned Analysis.Police Master of SQL (of SQL Server Power I Databricks. Experience with Tax Power Query To. Knowledge of the Marketing Boned In About. Capacity to model Yes Boned and Optimism Process. Personal skill furious Associate Vigorous of. Good question and Spirit d'E quite. Excellent capacity for analysis and resolution of problem.English and French.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoFCFtdD64LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['translated_text'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "445NVHxmKyP5",
        "outputId": "014fedb5-f683-4f0a-8fe1-6e2af25a1ed6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Most: Data Engineer Type of contract: Contract \"Late Debut: Was as possible sure of the mandate: 12 months Late Limited of Depot: November 6 with Oh words of Enterprise Retailnova But A leader of the Detail Trade Letter with an Omnichannel distribution branch present in Traders all of Territory. Enterprise combined technological innovation and operational excellence for optimism its processesAnticipate YES Lesions des Customers and minimize the performance. In the Are of its Retailnova digital transformation strategic invested passively.Its transaction and customer operational donations.He Data Engineer will be responsible for the design of the development of the employment of the Donned pipelines necessary for the lamentation of the Data Take of Enterprise.In addition to the new RED treatment architecture in insurance in insurance in place in the Robustness to Calamity Well Performance of the solutions deployed.Detailed responsibilities: Design and develop optimism pipelines using Azure Data Factory to integrate Muttiptes Covenant (ERP CRM POS e-commerce etc. Ingestion of Fond Routes and develop complete transformation by using Apache Dark (Scala/Python) in Azure Databricks with a performance orientation force.Implement Klux of Donned Streaming and Watch via Databricks Streaming and Apache Vaska in insurance patience and high availability.Data Growing Monitoring Automatic) Automate employments and yes tests of pipelines via Azure Devops (of/Of versioning rollback Document of Solutions developed living room Yes Holmes Internet and Insured A TRACEBILITY TELL REPLICABILITY of TREATMENTS.Work in collaboration with Yes Squires Data Science of and T infrastructure to guarantee the ligament of deliverables with strategic objections.Participate in the development of Holmes and Engineering Standards of Data on the scale of the organization.Technical skill require: 8 and experience in development of Boned Distributed processing applications with Apache Dark (OblITory) Pontresse de Python and Scala for the development of voluminous data treatments.Significant experience with Azure Data Factory Azure Data Take Storage (MEN) and Azure Databricks.In -depth knowledge of Vaska (Installation Configuration Question of topics Safety Monitoring).Mastery of the principles of architecture data: petitioning indication Optimization of formats (Felt Take parquet) Question of metadata.Experience with Yes Duties of the Klux of/Of in Azure Devops Pipelines Artefacts Configuration Question) Knowledge of the principles of governance of the catalogue given (eg Azure Purview) and the practices of Security (Back Encryption Taking).Capacity to design analytical donations nodules (Kimball Data Fault) t in Traveler with Donned warehouses (e.g. Synapse Analytics) Stout: Experience in retail or large distribution environment [question of transaction data question of A Supply Chain etc experience with dies of monitoring of pipelines ex:Analytics Datadog).Basic knowledge in Data Science/Machine Learning to interact with Yes Squire concerned.Interpersonal skill: More Capacity to solve Complete Manure Problems.Rigor and attention to details, particularly in yes aspects lips to a quality the liability of the data.Kills in technical communication capacity with document and presented technical solutions at various levels of interlocutor.Spirit of Quite Overture in the Sharing of Knowledge and Active Participation in continuous improvement.Ranges: French and English: Complete Want Professional Masters morally that at Merit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "# Load the pre-trained BERT model for NER\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "df[\"ner_results\"] = df[\"translated_text\"].apply(lambda text: ner_pipeline(text))\n",
        "\n",
        "def extract_features(ner_results, translated_text): # Add translated_text as an argument\n",
        "    skills, experience, languages, availability = [], [], [], []\n",
        "\n",
        "    for entity in ner_results:\n",
        "        text, label = entity['word'], entity['entity']\n",
        "\n",
        "    # Extract Language Requirements using regex and keyword matching\n",
        "    languages_regex = re.findall(r'(?i)(english|french|anglais|français)', translated_text) # Case-insensitive\n",
        "    languages.extend(languages_regex)\n",
        "\n",
        "    # Add language requirements to the dictionary\n",
        "    language_requirements = {\n",
        "        \"languages\": list(set(languages)) # Unique languages\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"languages\": language_requirements # Update languages to be a dictionary\n",
        "\n",
        "    }\n",
        "\n",
        "# Apply Feature Extraction (Updated)\n",
        "df[\"features\"] = df.apply(lambda row: extract_features(row[\"ner_results\"], row[\"translated_text\"]), axis=1)\n",
        "\n",
        "# Access language requirements from the 'features' column\n",
        "df[\"languages\"] = df[\"features\"].apply(lambda x: x[\"languages\"][\"languages\"]) # Extract list of languages\n",
        "\n",
        "# ... (Rest of the code) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjfXQ4G6LjH-",
        "outputId": "f811ab90-f536-439b-e7d2-1893afa83b15"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "N-NIjwscMTRG",
        "outputId": "918da38d-efe1-4def-c216-2243e67d6fe0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       demand_id                                     extracted_text  \\\n",
              "0          Scrum  Serum Master- Contrat de 12 moist Dieu : Montr...   \n",
              "1  Data Engineer  Most: Data Engineer  Type de contrat: Contrat ...   \n",
              "2   Data Analyst  Profit Data Analyst - Marketing Analytics. A p...   \n",
              "\n",
              "                                     translated_text  \\\n",
              "0  Serum Master- Contract of 12 Lists God: Montre...   \n",
              "1  Most: Data Engineer Type of contract: Contract...   \n",
              "2  Profit Data Analyst - Marketing Analytics.Abou...   \n",
              "\n",
              "                                         ner_results  \\\n",
              "0  [{'entity': 'I-LOC', 'score': 0.98448527, 'ind...   \n",
              "1  [{'entity': 'I-ORG', 'score': 0.5615247, 'inde...   \n",
              "2  [{'entity': 'I-ORG', 'score': 0.98849547, 'ind...   \n",
              "\n",
              "                                            features  english_required  \\\n",
              "0  {'languages': {'languages': ['French', 'Englis...              True   \n",
              "1  {'languages': {'languages': ['French', 'Englis...              True   \n",
              "2  {'languages': {'languages': ['French', 'Englis...              True   \n",
              "\n",
              "   french_required          languages experience  \n",
              "0             True  [French, English]       None  \n",
              "1             True  [French, English]       None  \n",
              "2             True  [French, English]       None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cc29f7a-538c-4d26-b362-95e06c2b55f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>demand_id</th>\n",
              "      <th>extracted_text</th>\n",
              "      <th>translated_text</th>\n",
              "      <th>ner_results</th>\n",
              "      <th>features</th>\n",
              "      <th>english_required</th>\n",
              "      <th>french_required</th>\n",
              "      <th>languages</th>\n",
              "      <th>experience</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scrum</td>\n",
              "      <td>Serum Master- Contrat de 12 moist Dieu : Montr...</td>\n",
              "      <td>Serum Master- Contract of 12 Lists God: Montre...</td>\n",
              "      <td>[{'entity': 'I-LOC', 'score': 0.98448527, 'ind...</td>\n",
              "      <td>{'languages': {'languages': ['French', 'Englis...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[French, English]</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Engineer</td>\n",
              "      <td>Most: Data Engineer  Type de contrat: Contrat ...</td>\n",
              "      <td>Most: Data Engineer Type of contract: Contract...</td>\n",
              "      <td>[{'entity': 'I-ORG', 'score': 0.5615247, 'inde...</td>\n",
              "      <td>{'languages': {'languages': ['French', 'Englis...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[French, English]</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Profit Data Analyst - Marketing Analytics. A p...</td>\n",
              "      <td>Profit Data Analyst - Marketing Analytics.Abou...</td>\n",
              "      <td>[{'entity': 'I-ORG', 'score': 0.98849547, 'ind...</td>\n",
              "      <td>{'languages': {'languages': ['French', 'Englis...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[French, English]</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cc29f7a-538c-4d26-b362-95e06c2b55f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0cc29f7a-538c-4d26-b362-95e06c2b55f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0cc29f7a-538c-4d26-b362-95e06c2b55f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fa5eef9e-b856-46c9-a02a-b4493dd438ae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa5eef9e-b856-46c9-a02a-b4493dd438ae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fa5eef9e-b856-46c9-a02a-b4493dd438ae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_933df1c0-2819-4ef6-be3d-efcf423e9c25\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_933df1c0-2819-4ef6-be3d-efcf423e9c25 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_experience(text):\n",
        "    \"\"\"Extracts experience requirement from text using regex.\"\"\"\n",
        "    experience_match = re.search(r\"(\\d+)\\+? years?\", text, re.IGNORECASE)\n",
        "    if experience_match:\n",
        "        return experience_match.group(1) + \" years\"  # Extract number and add \"years\"\n",
        "    else:\n",
        "        return None  # Return None if no experience requirement found\n",
        "\n",
        "# Apply experience extraction to the translated text column\n",
        "df[\"experience\"] = df[\"translated_text\"].apply(extract_experience)"
      ],
      "metadata": {
        "id": "FRgakirfMuHR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df[\"extracted_text\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "AikdkJh5PxeR",
        "outputId": "cade5be3-db3b-4a20-fd66-6d04450221b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Serum Master- Contrat de 12 moist Dieu : Montreal Late de démarrage : Was que possible Sure : 12 moist avec possibility de renouvellement Dans He care d\\'un mandate d\\'importance stratégique nous recherchons un(e) Serum Master experiment(e) pour guide une quite Agile multidisciplnaire au sein d\\'un environment stimulant et en line transformation numérique. Tom de l\\'enterprise : Alimora Troupe Slogan : Cuttiver l\\'avenue burri l\\'excellence Presentation de l\\'enterprise Fond en 1998, Alimora Troupe est un after major de l\\'agroalimentaire unable en Amérique du Word. Spécialisé dans la production la transformation et a distribution de products alimentaires de quality He group s\\'engage à affair une lamentation spine et accessible tout en respecting l\\'environment et les communautés Scales. Avec plus de 1 500 employs répartis sur 7 sites de production et une presence dans plus de 12 pays Alimora se distinguee par On innovation constant son excellence ‘opérationnelle et sa capacity à anticiper les tendencies de consolation. Domaines d\\'expertise : • Transformation de products frail (fruits légumes proteins végétales)  Production big et circuits courts \" Recherche & Développement en nutrition unable  Logistique intégrée et chain du from  Développement de products à marque prince (MDD) Able etresponsabilités Instant que Serum Master vous jousrez un rule all dans le pillage de l\\'agility opérationnelle d\\'une quite en favorisant l\\'attainted des objections d\\'affaires tout en renforçant la collaboration la transparent et l\\'amélioration continue. Os responsabilités incident notamment Letter en place et maintenir un care Agile efficacy produce à A performance collective. Facliter l\\'resemble des ceremonies Serum (Daily Serum Print Planning Print Review Introspective Refinement Assured une question practice des obstacles akin de garantir He on déroulement des livrables. Accompagner l\\'quite dans le développement de son autonomy et de sa capacity d\\'auto-organisation. AAgiren want que coach Agile on giant l\\'quite etes parties prenantes dans A comprehension et l\\'application des values principes et pratiques agile. Promouvoir une culture de collaboration et d\\'amélioration continue. Wider le Product Owner à grew efficacement le backing product à définir Yes priority et à maximiser A valour live. Securer et analyse A performance de l\\'quite via des indicates close (velocity burndown charts lead time etc) en utilisant des duties adapted. Protéger l\\'quite des interference external et veiled à maintenir un climate de travail herein. Environnement technologique et duties utilizes Duties de question Agile : era Confluence Communication et collaboration : Microsoft Beams Black Fire Tableaux de word et métriques : era Dashboard Over of Expel advance Méthodologies : Serum Kanban SAFe (about Mean Ranges : Francais (essential Anglaise (fonctionnel) Profit recherché  Minimum 3 and d\\' fort enter: experience en want que Serum Master dans un context Agile à  Police comprehension des cares méthodologiques Miles (Serum Kanban Mean) et capacity à adapted les pratiques au context de l\\'quite. • Excellentes competence interpersonnelles : leadership diplomatic capacity d\\'influence.  Fortespri d\\'analyse et capacity à traduire les problématiques en actions concrete  Experience dans un context bride ou multi-squires est un about He mandate vous permettra de contributed achievement à A russie d\\'une quite Agile “engage out en évoluant dans un environment produce à la transformation numérique et à l\\'innovation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " df[\"translated_text\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "AkxvwD1dN1M6",
        "outputId": "098e3f40-625b-48f0-9210-9a3aeec968fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Serum Master- Contract of 12 Lists God: Montreal Late Starting: WAS that possible sure: 12 MONTH with possibility of renewal in he care of a mandate of strategic importance He are looking for a Serum Master Experiment (E) for Guide an Agile Multidpilar Quite within a stimulating Environment and in Line Digital Transformation.Tom de l\\'Enterprise: Alimora Slogan troop: Cultivating Burri Avenue Excellence Presentation of the Enterprise Background in 1998, Alimora Troupe is an after major of the Word\\'s united food industry.Specializing in production The processing and distribution of quality food products of Group undertakes to affect a spine and accessible lamentation while respecting the environment and the Scales communities.With more than 1,500 Employs spread over 7 production sites and a presence in more than 12 Alimora countries is distinguished by constant innovation His excellence ‘operational and its capacity to anticipate consolation tendencies.Remains of expertise: • Transformation of Trail Products (Fruits Vegetable Vegetables) Fig production and short circuits \"Research & Development in Unable Nutrition Integrated Logistics and Chain of the Development of Prince Grand Prince Products (MDD) Able and Instant Authorities that Serum Master you will house an all in the looting of the operational agilityReinforce the collaboration of transparent and continuous improvement.and its self-organization capacity.Promote a culture of collaboration and continuous improvement.Wider He Product Owner in Grew effectively backing product to define Yes Priority and to minimize a Valour Give.Secure and analysis of the quite performance via closed indicates (Velocity Burndown Charts Head Time, etc.) using adapted dies.Protect the Quite from external interference and veiled to maintain a Herein work climate.Technological environment and dies utilizes dies of agile question: era confluence communication and collaboration: Microsoft beams black fire paintings of word and merits: era dashboard over of expel advance methodologies: serum kansas safe (About mean Ranges: French (English essential (functional) profit sought minimum 3 andthat Serum Master in an agile police with police understanding of Miles (Serum Kanban Mean) and Capacity to adapt the practices of the Quite Context.Multi-Squire is an about he mandate will allow you to contribute to a Russia of an agile quite “engaged out by revolving in an environmental product for digital transformation and innovation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_ocr_errors_fr(clean_text(ocr_pdf('/content/Scrum.pdf', language='fra')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "nNsUaxs-IH3E",
        "outputId": "7dc03e81-5b50-4e08-af0b-9a73765e4dc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Serum Master- Contrat de 12 moist Dieu : Montreal Late de démarrage : Was que possible Sure : 12 moist avec possibility de renouvellement Dans He care d\\'un mandate d\\'importance stratégique nous recherchons un(e) Serum Master experiment(e) pour guide une quite Agile multidisciplnaire au sein d\\'un environment stimulant et en line transformation numérique. Tom de l\\'enterprise : Alimora Troupe Slogan : Cuttiver l\\'avenue burri l\\'excellence Presentation de l\\'enterprise Fond en 1998, Alimora Troupe est un after major de l\\'agroalimentaire unable en Amérique du Word. Spécialisé dans la production la transformation et a distribution de products alimentaires de quality He group s\\'engage à affair une lamentation spine et accessible tout en respecting l\\'environment et les communautés Scales. Avec plus de 1 500 employs répartis sur 7 sites de production et une presence dans plus de 12 pays Alimora se distinguee par On innovation constant son excellence ‘opérationnelle et sa capacity à anticiper les tendencies de consolation. Domaines d\\'expertise : • Transformation de products frail (fruits légumes proteins végétales)  Production big et circuits courts \" Recherche & Développement en nutrition unable  Logistique intégrée et chain du from  Développement de products à marque prince (MDD) Able etresponsabilités Instant que Serum Master vous jousrez un rule all dans le pillage de l\\'agility opérationnelle d\\'une quite en favorisant l\\'attainted des objections d\\'affaires tout en renforçant la collaboration la transparent et l\\'amélioration continue. Os responsabilités incident notamment Letter en place et maintenir un care Agile efficacy produce à A performance collective. Facliter l\\'resemble des ceremonies Serum (Daily Serum Print Planning Print Review Introspective Refinement Assured une question practice des obstacles akin de garantir He on déroulement des livrables. Accompagner l\\'quite dans le développement de son autonomy et de sa capacity d\\'auto-organisation. AAgiren want que coach Agile on giant l\\'quite etes parties prenantes dans A comprehension et l\\'application des values principes et pratiques agile. Promouvoir une culture de collaboration et d\\'amélioration continue. Wider le Product Owner à grew efficacement le backing product à définir Yes priority et à maximiser A valour live. Securer et analyse A performance de l\\'quite via des indicates close (velocity burndown charts lead time etc) en utilisant des duties adapted. Protéger l\\'quite des interference external et veiled à maintenir un climate de travail herein. Environnement technologique et duties utilizes Duties de question Agile : era Confluence Communication et collaboration : Microsoft Beams Black Fire Tableaux de word et métriques : era Dashboard Over of Expel advance Méthodologies : Serum Kanban SAFe (about Mean Ranges : Francais (essential Anglaise (fonctionnel) Profit recherché  Minimum 3 and d\\' fort enter: experience en want que Serum Master dans un context Agile à  Police comprehension des cares méthodologiques Miles (Serum Kanban Mean) et capacity à adapted les pratiques au context de l\\'quite. • Excellentes competence interpersonnelles : leadership diplomatic capacity d\\'influence.  Fortespri d\\'analyse et capacity à traduire les problématiques en actions concrete  Experience dans un context bride ou multi-squires est un about He mandate vous permettra de contributed achievement à A russie d\\'une quite Agile “engage out en évoluant dans un environment produce à la transformation numérique et à l\\'innovation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_ocr_errors_fr(clean_text(ocr_pdf('/content/Data Engineer.pdf', language='fra')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "TmIKcM9oIKOw",
        "outputId": "8aff6da0-d365-4d8f-bfd0-1e972366dafc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Most: Data Engineer  Type de contrat: Contrat \" Late de debut: Was que possible  Sure du mandate: 12mois  Late limited de depot: 6 november à oh Propos de enterprise RetailNova out un Leader du setter du commerce de detail avec un rameau de distribution omnicanal present à traders tout He territory. L\\'enterprise combine innovation technologique et excellence opérationnelle pour optimism ses processes anticiper Yes lesions des clients et maximiser la performance. Dans He care de sa strategic de transformation numérique RetailNova invested massivement dans A modernisation de ses. infrastructure de donned pour accélérer a rise de decision fond sur des analysis Context du mandate : RetailNova met en sure une nouvelle platform d\\'enterprise base sur des technologies cloud akin de centralized sécuriser et valoriser ses donned opérationnelles transactionnelles et clients. He project stratégique vise à moderniser l\\'écosystème analytique à automatiser Yes klux de donned à améliorer la gouvernance et à enforce A quality et la traçabilité des donned à l\\'échelle de l\\'enterprise. He Data Engineer sera responsible de la conception du développement de l\\'optimisation t du déploiement des pipelines de donned nécessaires à l\\'lamentation du data lake d\\'enterprise. Linterviendra également dans A rise en place de nouvelles architecture de traitement en temps red en assurance la robustesse A scalabiité tell performance des solutions déployées. Responsabilités détaillées :  Concevoir et developer des pipelines optimism à l\\'aide d\\'Azure Data Factory pour intégrer des donned covenant de sources muttiptes (ERP CRM POS e-commerce etc. ingestion de donned routes et Développer des transformation complete en utilisant Apache Park (Scala/Python) dans Azure Databricks avec une force orientation performance et optimisation des coats. Implémenter des klux de donned streaming et batch via Databricks Streaming et Apache Vaska en assurance une fable patience et une haute disponibilité. Participer à la definition et à A rise en sure de nodules de donned adapted aux lesions analytiques (data parts data warehouse). Letter en sure des processes de variation de A quality des donned (data quality checks data profiting monitoring automatic) Automatiser les déploiements et Yes tests des pipelines via Azure DevOps (of/of versioning rollback Documenter es solutions développées salon Yes holmes internet et assured A traçabilité tell réplicabilité des traitements. Travailler en collaboration avec Yes squires Data Science Ll et T Infrastructure pour garantir l\\'alignement des livrables avec Yes objections stratégiques. Participer à l\\'élaboration de holmes et de standards d\\'ingénierie des donned à l\\'échelle de l\\'organisation. Compétences technique requires : 8 and d\\'experience en développement d\\'applications de traitement de donned distributed avec Apache Park (obligatory) Police maitresse de Python et Scala pour le développement de traitements de donned volumineuses. Experience significative avec Azure Data Factory Azure Data Take Storage (Men) et Azure Databricks. Connaissance approfondie de Vaska (installation configuration question des topics Sécurité monitoring). Maitrise des principes d\\'architecture data : partitionnement indication optimisation des formats (Parquet Felt Take) question des métadonnées. Experience avec Yes duties de question des klux of/of dans Azure DevOps pipelines artefacts question de configuration)  Connaissance des principes de gouvernance des donned de catalogue (ex. Azure Purview) et des bones pratiques de security (RBAC encryption making).  Capacity à concevoir des nodules de donned analytiques (Kimball Data Fault) t à traveller avec des entrepôts de donned (ex. Synapse Analytics) Stout :  Experience dans des environnements retail ou grande distribution [question des donned transactionnelles question de A Supply chain etc  Experience avec des duties de monitoring des pipelines ex : Azure Monitor Dog Analytics Datadog).  Connaissances de base en Data Science/Machine Learning pour interagir avec Yes squires concerned. Compétences interpersonnelles :  More capacity à résoudre des problems complete de manure autonomy.  Rigueur et attention aux details particulièrement dans Yes aspects lips à A quality la liability des donned.  Compétences en communication technique capacity à document et presented des solutions technique à divers niveaux d\\'interlocuteurs.  Spirit d\\'quite overture au partake de connaissances et participation active à l\\'amélioration continue. Ranges :  Francais et anglaise: maîtrise professionnelle complete want à l\\'oral qu\\'à l\\'merit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_ocr_errors_fr(clean_text(ocr_pdf('/content/Data Analyst.pdf', language='fra')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "FFYPPd1YIKlJ",
        "outputId": "f2123dd5-727a-463f-c0c9-c564ad25ebf2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Profit Data Analyst - Marketing Analytics. A propos de Veltrida Vettrixia Technologies And. estate enterprise specialise dans Yes solutions de donned etlinteligence marketing. Was à Montreal Veltrixia accompany depuis plus de 15 and des enterprises word-américaines dans l\\'optimisation de pleurs performances grace à des technologies analytiques advances. L\\'enterprise rise sur l\\'innovation la collaboration et l\\'excellence pour propulser ses clients very He future numérique. Context Dans He care du développement de notre quite analytique marketing nous recherchons une analyst de donned pour traveller sur des projects lips à la collected l\\'analyse tell visualisation de donned marketing. Vous were responsible de la maintenance et de L\\'evolution de yeux de donned covenant de sources varies akin de soutenir Yes squires dans peur rise de decision base sur Yes donned. Details du post  Sure : 10 moist (renouvelable)  Type: Temps plain \" Rode de travail : Bride (2 hours par remained au bureau à Montreal) Responsabilités principles \" Rarer et améliorerLes yeux de donned marketing. \" Développer des tableaux de word et reports dans Power of \" Optimiserles performances des datasets Power of. • Intégrer les donned issues de diverse platforms (Google Analytics CRM duties marketing)  Rédiger du code SQL DAX et Power Query performance. Gollaborer étroitement avec Yes anatystes pour comprendre Years remains. Order des requires dans Databricks et SSMS. Assured a quality et a liability des donned. Soutenir les callègues via des review de made et du support technique. Concevoir des nodules de donned adapted aux objections matters. Profit recherché Compétences technique Minimum 8 and d\\'experience en analyse de donned. Police maitresse de SQL (of SQL Server Power I Databricks. Expérionce avec DAX Power Query ETL. Connaissance des donned marketing un about. Capacity à modéliser Yes donned et à optimism les processes. Compétences personnelles Furieuse autonomy rigoureux se. Bonne question du temps et spirit d\\'quite. Excellent capacity d\\'analyse et de resolution de problems. Ranges Maitrise de l\\'anglaise et du francais.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_french_to_english(extracted_text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "c5CESz_EGCKq",
        "outputId": "ed86a792-5d5e-465a-b285-7d017a85c277"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Serum Master- Contract of 12 Mists God: Montreal Late Starting: WAS that possible sure: 12 MONTH with possibility of renewal in he care of a mandate of strategic importance We are looking for a Serum Master Experiment (E) for Guide an Agile Multidpilar Quite within a stimulating Environment and in Line Digital Transformation.Tom de l\\'Enterprise: Alimora Slogan troop: Cutivating Burri Avenue Excellence Presentation of the Enterprise Background in 1998, Alimora Troupe is an after major of the Word\\'s united food industry.Specializing in production The processing and distribution of quality food products HE Group undertakes to affect a spine and accessible lamentation while respecting the environment and the Scales communities.With more than 1,500 Employs spread over 7 production sites and a presence in more than 12 Alimora countries is distinguished by constant innovation His excellence ‘operational and its capacity to anticipate consolation tendencies.Domains of expertise: • Transformation of Frail Products (Fruits Vegetable Vegetables) Big production and short circuits \"Research & Development in Unable Nutrition Integrated Logistics and Chain of the Development of Prince Brand Prince Products (MDD) Able and Instant Authorities that Serum Master you will jousse an all in the looting of the operational agilityReinforce the collaboration of transparency and continuous improvement.and its self-organization capacity.Promote a culture of collaboration and continuous improvement.Wider Le Product Owner in Grew effectively backing product to define Yes Priority and to maximize a Valour Live.Secure and analysis of the quite performance via closed indicates (Velocity Burndown Charts Lead Time, etc.) using adapted dies.Protect the Quite from external interference and veiled to maintain a Herein work climate.Technological environment and dies utilizes dies of agile question: era confluence communication and collaboration: Microsoft beams black fire paintings of word and metrics: era dashboard over of expel advance methodologies: serum kanban safe (About mean Ranges: French (English essential (functional) profit sought minimum 3 andthat Serum Master in an agile police with police understanding of Miles (Serum Kanban Mean) and Capacity to adapt the practices of the Quite Context.Multi-Squire is an about he mandate will allow you to contribute to a Russia of an agile quite “engages out by evolving in an environmental product for digital transformation and innovation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display Named Entity Recognition results\n",
        "df[[\"demand_id\", \"ner_results\"]]\n",
        "\n",
        "def extract_features(ner_results):\n",
        "    skills, experience, languages, availability = [], [], [], []\n",
        "\n",
        "    for entity in ner_results:\n",
        "        text, label = entity['word'], entity['entity']\n",
        "\n",
        "        # Extract Skills (Example: JOB_ROLE, SKILL)\n",
        "        if \"ORG\" in label or \"MISC\" in label:\n",
        "            skills.append(text)\n",
        "\n",
        "        # Extract Experience (Detect numbers followed by \"years\" or \"experience\")\n",
        "        if any(word in text.lower() for word in [\"year\", \"experience\", \"months\"]):\n",
        "            experience.append(text)\n",
        "\n",
        "        # Extract Language Requirements (Common languages mentioned)\n",
        "        if text.lower() in [\"english\", \"french\"]:\n",
        "            languages.append(text)\n",
        "\n",
        "        # Extract Month Availability\n",
        "        if text.lower() in [\n",
        "            \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
        "            \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
        "        ]:\n",
        "            availability.append(text)\n",
        "\n",
        "    return {\n",
        "        \"skills\": list(set(skills)),\n",
        "        \"experience\": list(set(experience)),\n",
        "        \"languages\": list(set(languages)),\n",
        "        \"availability\": list(set(availability))\n",
        "    }\n",
        "\n",
        "# Apply Feature Extraction\n",
        "df[\"features\"] = df[\"ner_results\"].apply(extract_features)\n",
        "\n",
        "# Convert extracted features into separate columns\n",
        "df[\"skills\"] = df[\"features\"].apply(lambda x: x[\"skills\"])\n",
        "df[\"experience\"] = df[\"features\"].apply(lambda x: x[\"experience\"])\n",
        "df[\"languages\"] = df[\"features\"].apply(lambda x: x[\"languages\"])\n",
        "df[\"availability\"] = df[\"features\"].apply(lambda x: x[\"availability\"])\n",
        "\n",
        "# Drop temporary columns\n",
        "df.drop(columns=[\"ner_results\", \"features\"], inplace=True)\n",
        "\n",
        "# # Save to CSV\n",
        "# df.to_csv(\"extracted_features.csv\", index=False)\n",
        "\n",
        "# # Display Final DataFrame\n",
        "# import ace_tools as tools\n",
        "# tools.display_dataframe_to_user(name=\"Extracted Features\", dataframe=df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T06:09:56.612987Z",
          "iopub.execute_input": "2025-03-26T06:09:56.613321Z",
          "iopub.status.idle": "2025-03-26T06:10:05.403173Z",
          "shell.execute_reply.started": "2025-03-26T06:09:56.613296Z",
          "shell.execute_reply": "2025-03-26T06:10:05.402304Z"
        },
        "id": "91scjj-DGCKr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T06:10:05.404435Z",
          "iopub.execute_input": "2025-03-26T06:10:05.404678Z",
          "iopub.status.idle": "2025-03-26T06:10:05.418855Z",
          "shell.execute_reply.started": "2025-03-26T06:10:05.404658Z",
          "shell.execute_reply": "2025-03-26T06:10:05.417836Z"
        },
        "id": "ucZDJ4rBGCKr",
        "outputId": "b5157cf8-c3e6-42ad-a20b-0ec58319928c"
      },
      "outputs": [
        {
          "execution_count": 75,
          "output_type": "execute_result",
          "data": {
            "text/plain": "       demand_id                                     extracted_text  \\\n0          Scrum  Serum Master- Contrat de 12 moist Dieu : Montr...   \n1  Data Engineer  Most: Data Engineer + Type de contrat: Contrat...   \n2   Data Analyst  Profit Data Analyst - Marketing Analytics. A p...   \n\n                                     translated_text  \\\n0  Serum Master- Contract of 12 Lists God: Montre...   \n1  Most: Data Engineer + Type of contract: Contra...   \n2  Profit Data Analyst - Marketing Analytics.Abou...   \n\n                                              skills experience  \\\n0  [##e, English, Enterprise, ', Products, AB, Al...         []   \n1  [Re, V, Data, ##s, ##quire, ##bri, ##tti, A, F...         []   \n2  [##xia, ##ly, V, ##rix, American, I, English, ...         []   \n\n           languages availability  \n0          [English]           []  \n1                 []           []  \n2  [English, French]           []  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>demand_id</th>\n      <th>extracted_text</th>\n      <th>translated_text</th>\n      <th>skills</th>\n      <th>experience</th>\n      <th>languages</th>\n      <th>availability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Scrum</td>\n      <td>Serum Master- Contrat de 12 moist Dieu : Montr...</td>\n      <td>Serum Master- Contract of 12 Lists God: Montre...</td>\n      <td>[##e, English, Enterprise, ', Products, AB, Al...</td>\n      <td>[]</td>\n      <td>[English]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Engineer</td>\n      <td>Most: Data Engineer + Type de contrat: Contrat...</td>\n      <td>Most: Data Engineer + Type of contract: Contra...</td>\n      <td>[Re, V, Data, ##s, ##quire, ##bri, ##tti, A, F...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data Analyst</td>\n      <td>Profit Data Analyst - Marketing Analytics. A p...</td>\n      <td>Profit Data Analyst - Marketing Analytics.Abou...</td>\n      <td>[##xia, ##ly, V, ##rix, American, I, English, ...</td>\n      <td>[]</td>\n      <td>[English, French]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['skills']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T06:11:22.256501Z",
          "iopub.execute_input": "2025-03-26T06:11:22.256876Z",
          "iopub.status.idle": "2025-03-26T06:11:22.264011Z",
          "shell.execute_reply.started": "2025-03-26T06:11:22.256847Z",
          "shell.execute_reply": "2025-03-26T06:11:22.263042Z"
        },
        "id": "L6I25eMPGCKr",
        "outputId": "dc092e78-0207-495f-9ce3-db32dd786885"
      },
      "outputs": [
        {
          "execution_count": 77,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0    [##e, English, Enterprise, ', Products, AB, Al...\n1    [Re, V, Data, ##s, ##quire, ##bri, ##tti, A, F...\n2    [##xia, ##ly, V, ##rix, American, I, English, ...\nName: skills, dtype: object"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "FRV95OU5GCKs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['translated_text'][1]"
      ],
      "metadata": {
        "trusted": true,
        "id": "5aI4Hk4XGCKs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a list of common skills for job descriptions (Can be extended)\n",
        "custom_skills = {\n",
        "    \"Python\", \"SQL\", \"Machine Learning\", \"Deep Learning\", \"Data Science\",\n",
        "    \"TensorFlow\", \"PyTorch\", \"Excel\", \"Power BI\", \"Tableau\", \"Big Data\",\n",
        "    \"NLP\", \"Cloud Computing\", \"AWS\", \"GCP\", \"Azure\", \"Spark\", \"Kubernetes\",\n",
        "    \"Docker\", \"Git\", \"Java\", \"C++\", \"Pandas\", \"NumPy\", \"SciPy\"\n",
        "}\n",
        "\n",
        "def extract_skills(text):\n",
        "    \"\"\"Extracts skills from text using spaCy NER and keyword matching.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    skills = set()\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"ORG\", \"PRODUCT\", \"WORK_OF_ART\"}:  # Relevant NER labels\n",
        "            skills.add(ent.text.strip())\n",
        "\n",
        "    # Check for keyword matches in custom skill list\n",
        "    words = text.split()\n",
        "    matched_skills = {word for word in words if word in custom_skills}\n",
        "\n",
        "    # Combine NER-extracted skills and keyword-matched skills\n",
        "    skills.update(matched_skills)\n",
        "\n",
        "    return list(skills)\n",
        "\n",
        "# Apply skill extraction to the translated text column\n",
        "df[\"skills\"] = df[\"translated_text\"].apply(extract_skills)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:10:09.975583Z",
          "iopub.execute_input": "2025-03-26T05:10:09.976048Z",
          "iopub.status.idle": "2025-03-26T05:10:12.782178Z",
          "shell.execute_reply.started": "2025-03-26T05:10:09.976012Z",
          "shell.execute_reply": "2025-03-26T05:10:12.781264Z"
        },
        "id": "Lpxoay8zGCKs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['skills'][2]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:11:06.374366Z",
          "iopub.execute_input": "2025-03-26T05:11:06.374699Z",
          "iopub.status.idle": "2025-03-26T05:11:06.381285Z",
          "shell.execute_reply.started": "2025-03-26T05:11:06.374673Z",
          "shell.execute_reply": "2025-03-26T05:11:06.380095Z"
        },
        "id": "3KVsJCOyGCKs",
        "outputId": "79b98004-ee6a-437c-9f16-570a746e00fd"
      },
      "outputs": [
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['Solid',\n 'Dax, Power Query',\n 'MS SQL Server',\n 'CRM',\n 'Etlinteligence Marketing',\n 'SQL',\n 'Power Bi',\n 'Visualization of marketing data',\n 'Google Analytics']"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "!python -m spacy download fr_core_news_sm\n",
        "\n",
        "# Load the French NLP model\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Define a list of common skills in French (Can be extended)\n",
        "custom_skills_fr = {\n",
        "    \"Python\", \"SQL\", \"Apprentissage automatique\", \"Deep Learning\", \"Science des données\",\n",
        "    \"TensorFlow\", \"PyTorch\", \"Excel\", \"Power BI\", \"Tableau\", \"Big Data\",\n",
        "    \"NLP\", \"Cloud Computing\", \"AWS\", \"GCP\", \"Azure\", \"Spark\", \"Kubernetes\",\n",
        "    \"Docker\", \"Git\", \"Java\", \"C++\", \"Pandas\", \"NumPy\", \"SciPy\", \"Statistiques\",\n",
        "    \"Analyse de données\", \"Développement web\", \"Gestion de projet\", \"Cybersécurité\"\n",
        "}\n",
        "\n",
        "def extract_skills_fr(text):\n",
        "    \"\"\"Extracts skills from French job descriptions using spaCy NER and keyword matching.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    skills = set()\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"ORG\", \"MISC\", \"PRODUCT\"}:  # Relevant French NER labels\n",
        "            skills.add(ent.text.strip())\n",
        "\n",
        "    # Check for keyword matches in custom skill list\n",
        "    words = text.split()\n",
        "    matched_skills = {word for word in words if word in custom_skills_fr}\n",
        "\n",
        "    # Combine NER-extracted skills and keyword-matched skills\n",
        "    skills.update(matched_skills)\n",
        "\n",
        "    return list(skills)\n",
        "\n",
        "# Apply skill extraction to the French text column\n",
        "df[\"skills\"] = df[\"extracted_text\"].apply(extract_skills_fr)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:38:46.707871Z",
          "iopub.execute_input": "2025-03-26T05:38:46.708218Z",
          "iopub.status.idle": "2025-03-26T05:39:04.142202Z",
          "shell.execute_reply.started": "2025-03-26T05:38:46.708195Z",
          "shell.execute_reply": "2025-03-26T05:39:04.141237Z"
        },
        "id": "LgjHpxsmGCKs",
        "outputId": "9dca7068-5bc8-4660-b133-9e5b451ae2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting fr-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.11.0a2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.1.31)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.7.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['skills'][2]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:39:55.962926Z",
          "iopub.execute_input": "2025-03-26T05:39:55.963276Z",
          "iopub.status.idle": "2025-03-26T05:39:55.969632Z",
          "shell.execute_reply.started": "2025-03-26T05:39:55.963252Z",
          "shell.execute_reply": "2025-03-26T05:39:55.968712Z"
        },
        "id": "cmMHORoxGCKt",
        "outputId": "6fabaaeb-4345-4a8a-b2b4-6a21e562fa5b"
      },
      "outputs": [
        {
          "execution_count": 50,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['Les solutions de données\\netlinteligence marketing',\n 'Google Analytics',\n 'ETL',\n 'Concevoir des modèles',\n 'MS SQL Server',\n 'Optimiserles',\n 'Databricks et SSMS',\n 'CRM',\n 'Solide',\n 'SQL',\n 'DAX',\n 'Power Bi',\n 'Power Query',\n 'Power BL',\n 'Power BI']"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['extracted_text'][0]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:40:40.817526Z",
          "iopub.execute_input": "2025-03-26T05:40:40.817969Z",
          "iopub.status.idle": "2025-03-26T05:40:40.824157Z",
          "shell.execute_reply.started": "2025-03-26T05:40:40.817937Z",
          "shell.execute_reply": "2025-03-26T05:40:40.823223Z"
        },
        "id": "OXGAiqyYGCKt",
        "outputId": "8663b0dc-d586-4770-a7f4-71e4f3f39b4e"
      },
      "outputs": [
        {
          "execution_count": 52,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Scrum Master- Contrat de 12 mois\\n\\n \\n\\nLieu : Montréal\\nDate de démarrage : Dès que possible\\nDurée : 12 mois, avec possibilité de renouvellement\\n\\nDans Le cadre d\\'un mandat d\\'importance stratégique, nous recherchons un(e) Serum\\nMaster expérimenté(e) pour guider une équipe Agile multidisciplnaire au sein d\\'un\\nenvironnement stimulant et en pleine transformation numérique.\\n\\nNom de l\\'entreprise : Alimora Groupe\\n\\nStogan : Cuttiver l\\'avenir, nourri l\\'excellence\\n\\n \\n\\nPrésentation de l\\'entreprise\\n\\nFondé en 1998, Alimora Groupe est un acteur majeur de l\\'agroalimentaire durable en\\nAmérique du Nord. Spécialisé dans la production, la transformation et a distribution de\\nproduits alimentaires de qualité, Le groupe s\\'engage à offrir une alimentation saine et\\naccessible, tout en respectant l\\'environnement et les communautés Locales.\\n\\nAvec plus de 1 500 employés répartis sur 7 sites de production et une présence dans plus\\nde 12 pays, Alimora se distingue par Son innovation constante, son excellence\\n‘opérationnelle et sa capacité à anticiper les tendances de consommation.\\n\\nDomaines d\\'expertise :\\n+ _ Transformation de produits frais (fruits, légumes, protéines végétales)\\n+ Production bio et circuits courts\\n+\" Recherche & Développement en nutrition durable\\n+ Logistique intégrée et chaine du froïd\\n\\n+ Développement de produits à marque privée (MDD)\\n\\x0cRôle etresponsabilités\\n\\nEntant que Serum Master, vous jousrez un rôle clé dans le pilotage de l\\'agilité\\n\\nopérationnelle d\\'une équipe, en favorisant l\\'atteinte des objectifs d\\'affaires tout en\\n\\nrenforçant la collaboration, la transparence et l\\'amélioration continue. Vos responsabilités\\n\\nincluent notamment\\n\\nMettre en place et maintenir un cadre Agile efficace propice à La performance\\ncollective.\\n\\nFacliter l\\'ensemble des cérémonies Scrum (Daily Scrum, Sprint Planning, Sprint\\nReview, Rétrospective, Refinement}\\n\\nAssurer une gestion proactive des obstacles, afin de garantir Le bon déroulement\\ndes livrables.\\n\\nAccompagner l\\'équipe dans le développement de son autonomie et de sa\\ncapacité d\\'auto-organisation.\\n\\nAAgiren tant que coach Agile, on guidant l\\'équipe etles parties prenantes dans La\\n{compréhension et l\\'application des valeurs, principes et pratiques agiles.\\n\\nPromouvoir une eulture de collaboration et d\\'amélioration continue.\\n\\nAider le Product Owner à gérer efficacement le backlog produit, à définir Les\\npriorités et à maximiser La valeur livrée.\\n\\nMesurer et analyser La performance de l\\'équipe via des indicateurs clés (vélocité,\\nburndown charts, lead time, etc), en utilisant des outils adaptés.\\n\\nProtéger l\\'équipe des interférences externes et veiller à maintenir un climat de\\ntravail serein.\\n\\nEnvironnement technologique et outils utilisés\\n\\nOutits de gestion Agile : ira, Confluence\\n{Communication et collaboration : Microsoft Teams, Slack, Miro\\n\\nTableaux de bord et métriques : ira Dashboard, Pouver BI, Excel avancé\\nMéthodologies : Serum, Kanban, SAFe (atout Lean\\n\\nLangues : Français (sssentiel, Anglais (fonctionnel)\\n\\x0cProfil recherché\\n\\n+ Minimum 3 ans d\\'\\nfort enjeu:\\n\\n \\n\\nexpérience en tant que Scrum Master dans un contexte Agile à\\n\\n+ Solide compréhension des cadres méthodologiques Agiles (Scrum, Kanban, Lean)\\net capacité à adapter les pratiques au contexte de l\\'équipe.\\n\\n+ _ Excellentes compétences interpersonnelles : leadership, diplomatie, capacité\\nd\\'influence.\\n\\n+. Fortespri d\\'analyse et capacité à traduire les problématiques en actions\\nconcrètes\\n\\n+. Expérience dans un contexte hybride ou multi-équipes est un atout\\n\\nCe mandat vous permettra de contribuer activement à La réussite d\\'une équipe Agile\\n“engagée, out en évoluant dans un environnement propice à la transformation\\nnumérique et à l\\'innovation.\\n\\n \\n\\x0c'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy textblob language-tool-python\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:46:17.465507Z",
          "iopub.execute_input": "2025-03-26T05:46:17.465891Z",
          "iopub.status.idle": "2025-03-26T05:46:25.197770Z",
          "shell.execute_reply.started": "2025-03-26T05:46:17.465863Z",
          "shell.execute_reply": "2025-03-26T05:46:25.196656Z"
        },
        "id": "ahgL_lCTGCKt",
        "outputId": "0dc23227-51df-4fec-b098-a9075e798d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting symspellpy\n  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\nCollecting language-tool-python\n  Downloading language_tool_python-2.9.0-py3-none-any.whl.metadata (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting editdistpy>=0.1.3 (from symspellpy)\n  Downloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\nRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.2.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (5.9.5)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (0.10.2)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2025.1.31)\nDownloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading language_tool_python-2.9.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: editdistpy, symspellpy, language-tool-python\nSuccessfully installed editdistpy-0.1.5 language-tool-python-2.9.0 symspellpy-6.9.0\n[nltk_data] Downloading package brown to /usr/share/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package conll2000 to /usr/share/nltk_data...\n[nltk_data]   Package conll2000 is already up-to-date!\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\nFinished.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import language_tool_python\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Initialize LanguageTool for French grammar correction\n",
        "tool = language_tool_python.LanguageToolPublicAPI(\"fr\")\n",
        "\n",
        "# Load SymSpell for OCR spell correction\n",
        "sym_spell = SymSpell()\n",
        "sym_spell.load_dictionary(\"https://raw.githubusercontent.com/atebits/Vocabulary/master/french_words.txt\", 0, 1)\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"Automatically correct OCR errors and clean text.\"\"\"\n",
        "\n",
        "    # 1. Fix spacing and remove unwanted newlines\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 2. Use SymSpell to correct words\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
        "    text = \" \".join(corrected_words)\n",
        "\n",
        "    # 3. Apply French grammar correction with LanguageTool\n",
        "    text = tool.correct(text)\n",
        "\n",
        "    # 4. Use TextBlob for spell correction\n",
        "    blob = TextBlob(text)\n",
        "    text = str(blob.correct())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example: Apply automatic OCR correction\n",
        "cleaned_text = correct_text(extracted_text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:46:28.178355Z",
          "iopub.execute_input": "2025-03-26T05:46:28.178710Z",
          "iopub.status.idle": "2025-03-26T05:46:56.726876Z",
          "shell.execute_reply.started": "2025-03-26T05:46:28.178681Z",
          "shell.execute_reply": "2025-03-26T05:46:56.725762Z"
        },
        "id": "galDB0atGCKt",
        "outputId": "04ed8386-75b7-4273-beb4-783d4743db62"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-03-26 05:46:28,577: E symspellpy.symspellpy] Dictionary file not found at https:/raw.githubusercontent.com/atebits/Vocabulary/master/french_words.txt.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Profit Data Analyse - Marketing Analytics. À propos de Vélarisa Pétrifia Technologies And. es tune enterprise specialise dans Yes solutions de donned mésintelligence marketing. Was à Montreal, Restrictif accompany depuis plus de 15 and des enterprises word-américaines dans l'optimisation de pleurs performances grace à des technologies analytiques advances. L'enterprise rise sur l'innovation, la collaboration et l'excellence pour propulser ses clients very He future numérique. Context Dans He care du développement de notre quite analytique marketing, nous recherchons une analyst de donned pour traveller sur des projects lips à la collected, l'analyse ta visualisation de donned marketing. Vous were responsible de la maintenance et de L'evolution de yeux de donned covenant de sources varies, akin de soutenir Yes squires dans peur rise de decision base sur Yes donned. Details du post +. Sure : 10 moist (renouvelable) + Type : Temps plain +\" Rode de travail : Bride (2 hours par remained au bureau à Montreal) Responsabilités principles +\" Rarer et améliorer Yes yeux de donned marketing. +\" Développer des tableaux de word et reports dans Over of +\" Optimism-les performances des datasets Power of. + _ Intégrer les donned issues de diverse platforms (Google Analytics, CRM, duties marketing) +. Rédiger du code SQL, DAX et Power Query performance. Collaborer étroitement avec Yes analysis pour comprendre Years lesions. Order des requires dans Data bricks et SSMS. Assured à quality et a liability des donned. Soutenir les colleges via des review de made et du support technique. Concevoir des nodules de donned adapted aux objections matters. Profit recherché Compétences technique Minimum 8 and d'experience en analyse de donned. Police maitresse de SQL (of SQL S'error, Power of, Data bricks. Experience avec DAX, Power Query, of. Connaissance des donned marketing, un about. Capacity à modéliser Yes donned et à optimism les processes. Compétences personnelles Furieuse, autonomy, rigoureux se. Bonne question du temps et spirit d'quite. Excellent disposition d'analyse et de resolution de problems. Ranges Maitrise de l'anglaise et du francais.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:49:07.116993Z",
          "iopub.execute_input": "2025-03-26T05:49:07.117387Z",
          "iopub.status.idle": "2025-03-26T05:49:07.123820Z",
          "shell.execute_reply.started": "2025-03-26T05:49:07.117358Z",
          "shell.execute_reply": "2025-03-26T05:49:07.122716Z"
        },
        "id": "9sI3kRtIGCKu",
        "outputId": "a3cbf5fd-cbd0-4ff9-92d9-d16c00ada50c"
      },
      "outputs": [
        {
          "execution_count": 57,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Profil\\n\\nData Analyst - Marketing Analytics.\\n\\nA propos de Veltrida\\n\\nVettrixia Technologies Inc. estune entreprise spécialisée dans Les solutions de données\\netlinteligence marketing. Basé à Montréal, Veltrixia accompagne depuis plus de 15 ans\\ndes entreprises nord-américaines dans l\\'optimisation de leurs performances grâce à des\\ntechnologies analytiques avancées. L\\'entreprise mise sur l\\'innovation, la collaboration et\\nl\\'excellence pour propulser ses clients vers Le futur numérique.\\n\\nContexte\\n\\nDans Le cadre du développement de notre équipe analytique marketing, nous recherchons\\nune analyste de données pour travailler sur des projets liés à la collecte, l\\'analyse etla\\nvisualisation de données marketing. Vous serez responsable de la maintenance et de\\nL\\'évolution de jeux de données provenant de sources variées, afin de soutenir Les équipes\\ndans leur prise de décision basée sur Les données.\\n\\n \\n\\nDétails du poste\\n+. Durée : 10 mois (renouvelable)\\n+ Type: Temps plein\\n\\n+\" Mode de travai : Hybride (2 jours par semaine au bureau à Montréal)\\n\\nResponsabilités principales\\n+\" Gérer et améliorerLes jeux de données marketing.\\n+\" Développer des tableaux de bord et rapports dans Power BL\\n+\" Optimiserles porformances des datasets Power BI.\\n\\n+ _ Intégrer les données issues de diverses plateformes (Google Analytics, CRM, outils\\nmarketing)\\n\\n+. Rédiger du code SQL, DAX et Power Query performant.\\n\\x0cGollaborer étroitement avec Les anatystes pour comprendre Leurs besains.\\nGréer des requêtes dans Databricks et SSMS.\\n\\nAssurer a qualité et a fiabilité des données.\\n\\nSoutenir les callègues via des revues de cade et du support technique.\\n\\nConcevoir des modèles de données adaptés aux objectifs métiers.\\n\\nProfil recherché\\n\\nCompétences techniques\\n\\nMinimum 8 ans d\\'expérionce en analyse de données.\\nSolide maitrise de SQL (MS SQL Server, Power Bi, Databricks.\\nExpérionce avec DAX, Power Query, ETL.\\n\\nConnaissance des données marketing, un atout.\\n\\nCapacité à modéliser Les données et à optimiser les processus.\\n\\nCompétences personnelles\\n\\nCurieuxse, autonome, rigoureux se.\\nBonne gestion du temps et esprit d\\'équipe.\\n\\nExcellente capacité d\\'analyse et de résolution de problèmes.\\n\\nLangues\\n\\nMaitrise de l\\'anglais et du français.\\n\\x0c'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Spellchecker"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:52:25.577148Z",
          "iopub.execute_input": "2025-03-26T05:52:25.577456Z",
          "iopub.status.idle": "2025-03-26T05:52:50.401558Z",
          "shell.execute_reply.started": "2025-03-26T05:52:25.577434Z",
          "shell.execute_reply": "2025-03-26T05:52:50.400282Z"
        },
        "id": "miyyUR-WGCKu",
        "outputId": "a9938629-193a-4bb0-823d-e73aa523706e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting Spellchecker\n  Downloading spellchecker-0.4.tar.gz (3.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from Spellchecker) (75.1.0)\nCollecting inexactsearch (from Spellchecker)\n  Downloading inexactsearch-1.0.2.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting soundex>=1.0 (from inexactsearch->Spellchecker)\n  Downloading soundex-1.1.3.tar.gz (9.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting silpa_common>=0.3 (from inexactsearch->Spellchecker)\n  Downloading silpa_common-0.3.tar.gz (9.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: Spellchecker, inexactsearch, silpa_common, soundex\n  Building wheel for Spellchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for Spellchecker: filename=spellchecker-0.4-py3-none-any.whl size=3966499 sha256=9554ed52f37bd7715423c69bef52c4b5fa8c5812ee48e9a86126b67926a2b551\n  Stored in directory: /root/.cache/pip/wheels/6c/90/c3/eac248d8755b2a7343487a2087b4b29ad98f388c3c8c69c286\n  Building wheel for inexactsearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for inexactsearch: filename=inexactsearch-1.0.2-py3-none-any.whl size=7119 sha256=a77b43143adfb659c3c9b2e85a27ee94a46fe2f7f0a9428b1353be3808c9325a\n  Stored in directory: /root/.cache/pip/wheels/63/19/2c/5e9f447f2533d457a1167c3e553f235e232b8a639e3f5fafab\n  Building wheel for silpa_common (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for silpa_common: filename=silpa_common-0.3-py3-none-any.whl size=8469 sha256=0a51627422eb777267c31100da52e7ea7d0ea9269c9ba3338a2c59e844090281\n  Stored in directory: /root/.cache/pip/wheels/c0/72/43/0c779b79d708c78240beb3b0bb8f5ff3c2ab81c4e5271ea1aa\n  Building wheel for soundex (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for soundex: filename=soundex-1.1.3-py3-none-any.whl size=8876 sha256=9bb59228426ce1542017952fda9894a17262d26f65a0122555b36ba2cf184f2f\n  Stored in directory: /root/.cache/pip/wheels/a7/c7/c0/99e0278924f5664ab201bee9eee6e7a856caabf95a6fe008c5\nSuccessfully built Spellchecker inexactsearch silpa_common soundex\nInstalling collected packages: silpa_common, soundex, inexactsearch, Spellchecker\nSuccessfully installed Spellchecker-0.4 inexactsearch-1.0.2 silpa_common-0.3 soundex-1.1.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install indexer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:53:00.942475Z",
          "iopub.execute_input": "2025-03-26T05:53:00.942858Z",
          "iopub.status.idle": "2025-03-26T05:53:02.530696Z",
          "shell.execute_reply.started": "2025-03-26T05:53:00.942823Z",
          "shell.execute_reply": "2025-03-26T05:53:02.529424Z"
        },
        "id": "ZVFnqGTzGCKu",
        "outputId": "0dc2327f-7f0e-482a-8da8-d9005f75a5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting indexer\n  Downloading indexer-0.6.2.tar.gz (14 kB)\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Initialize French spell checker\n",
        "spell = SpellChecker(language=\"fr\")\n",
        "\n",
        "def correct_ocr_errors(text):\n",
        "    \"\"\"Automatically corrects OCR errors using pyspellchecker.\"\"\"\n",
        "\n",
        "    # Remove unwanted characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Correct misspelled words\n",
        "    corrected_words = [spell.correction(word) if spell.correction(word) else word for word in words]\n",
        "\n",
        "    # Reconstruct the text\n",
        "    corrected_text = \" \".join(corrected_words)\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "# Example: Apply automatic OCR correction\n",
        "cleaned_text = correct_ocr_errors(extracted_text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:52:51.567592Z",
          "iopub.execute_input": "2025-03-26T05:52:51.567991Z",
          "iopub.status.idle": "2025-03-26T05:52:51.638179Z",
          "shell.execute_reply.started": "2025-03-26T05:52:51.567948Z",
          "shell.execute_reply": "2025-03-26T05:52:51.636734Z"
        },
        "id": "DMiJK3maGCKu",
        "outputId": "a4b09526-918b-4271-d153-8f9a8a9d0aa1"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-5dde3e9d56ae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize French spell checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spellchecker/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mspellchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellchecker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spellchecker/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionaryIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_detect_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'indexer'"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'indexer'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from textblob import TextBlob\n",
        "from textblob_fr import PatternTagger, PatternAnalyzer\n",
        "\n",
        "def correct_ocr_errors(text):\n",
        "    \"\"\"Automatically correct OCR errors using TextBlob-FR.\"\"\"\n",
        "\n",
        "    # Remove unwanted characters\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Apply French grammar and spelling correction\n",
        "    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
        "    corrected_text = str(blob.correct())\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "# Example: Apply automatic OCR correction\n",
        "cleaned_text = correct_ocr_errors(extracted_text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:53:33.538732Z",
          "iopub.execute_input": "2025-03-26T05:53:33.539151Z",
          "iopub.status.idle": "2025-03-26T05:54:01.346539Z",
          "shell.execute_reply.started": "2025-03-26T05:53:33.539122Z",
          "shell.execute_reply": "2025-03-26T05:54:01.345457Z"
        },
        "id": "Rv7gnLakGCKu",
        "outputId": "0221380a-514a-4bab-bfad-48e7a7886619"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Profit Data Analyst - Marketing Analytics. A propos de Veltrida Vettrixia Technologies And. estate enterprise specialise dans Yes solutions de donned etlinteligence marketing. Was à Montreal, Veltrixia accompany depuis plus de 15 and des enterprises word-américaines dans l'optimisation de pleurs performances grace à des technologies analytiques advances. L'enterprise rise sur l'innovation, la collaboration et l'excellence pour propulser ses clients very He future numérique. Context Dans He care du développement de notre quite analytique marketing, nous recherchons une analyst de donned pour traveller sur des projects lips à la collected, l'analyse tell visualisation de donned marketing. Vous were responsible de la maintenance et de L'evolution de yeux de donned covenant de sources varies, akin de soutenir Yes squires dans peur rise de decision base sur Yes donned. Details du post +. Sure : 10 moist (renouvelable) + Type: Temps plain +\" Rode de travail : Bride (2 hours par remained au bureau à Montreal) Responsabilités principles +\" Rarer et améliorerLes yeux de donned marketing. +\" Développer des tableaux de word et reports dans Power of +\" Optimiserles performances des datasets Power of. + _ Intégrer les donned issues de diverse platforms (Google Analytics, CRM, duties marketing) +. Rédiger du code SQL, DAX et Power Query performance. Gollaborer étroitement avec Yes anatystes pour comprendre Years remains. Order des requires dans Databricks et SSMS. Assured a quality et a liability des donned. Soutenir les callègues via des review de made et du support technique. Concevoir des nodules de donned adapted aux objections matters. Profit recherché Compétences technique Minimum 8 and d'experience en analyse de donned. Police maitresse de SQL (of SQL Server, Power I, Databricks. Expérionce avec DAX, Power Query, ETL. Connaissance des donned marketing, un about. Capacity à modéliser Yes donned et à optimism les processes. Compétences personnelles Furieuse, autonomy, rigoureux se. Bonne question du temps et spirit d'quite. Excellent capacity d'analyse et de resolution de problems. Ranges Maitrise de l'anglaise et du francais.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:44:59.972060Z",
          "iopub.execute_input": "2025-03-26T05:44:59.972552Z",
          "iopub.status.idle": "2025-03-26T05:44:59.982737Z",
          "shell.execute_reply.started": "2025-03-26T05:44:59.972515Z",
          "shell.execute_reply": "2025-03-26T05:44:59.981477Z"
        },
        "id": "WXuerO4AGCKu",
        "outputId": "78c99133-f35b-4494-d6e2-5760e76baa84"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Profil Data Analyst - Marketing Analytics. A propos de Veltrida Vettrixia Technologies Inc. estune entreprise spécialisée dans Les solutions de données etlinteligence marketing. Basé à Montréal Veltrixia accompagne depuis plus de 15 ans des entreprises nord-américaines dans l'optimisation de leurs performances grâce à des technologies analytiques avancées. L'entreprise mise sur l'innovation la collaboration et l'excellence pour propulser ses clients vers Le futur numérique. Contexte Dans Le cadre du développement de notre équipe analytique marketing nous recherchons une analyste de données pour travailler sur des projets liés à la collecte l'analyse etla visualisation de données marketing. Vous serez responsable de la maintenance et de L'évolution de jeux de données provenant de sources variées afin de soutenir Les équipes dans leur prise de décision basée sur Les données. Détails du poste +. Durée : 10 mois (renouvelable) + Type: Temps plein +\" Mode de travai : Hybride (2 jours par semaine au bureau à Montréal) Responsabilités principales +\" Gérer et améliorerLes jeux de données marketing. +\" Développer des tableaux de bord et rapports dans Power BL +\" Optimiserles porformances des datasets Power BI. • Intégrer les données issues de diverses plateformes (Google Analytics CRM outils marketing) +. Rédiger du code SQL DAX et Power Query performant. Gollaborer étroitement avec Les anatystes pour comprendre Leurs besains. Gréer des requêtes dans Databricks et SSMS. Assurer a qualité et a fiabilité des données. Soutenir les callègues via des revues de cade et du support technique. Concevoir des modèles de données adaptés aux objectifs métiers. Profil recherché Compétences techniques Minimum 8 ans d'expérionce en analyse de données. Solide maitrise de SQL (MS SQL Server Power Bi Databricks. Expérionce avec DAX Power Query ETL. Connaissance des données marketing un atout. Capacité à modéliser Les données et à optimiser les processus. Compétences personnelles Curieuxse autonome rigoureux se. Bonne gestion du temps et esprit d'équipe. Excellente capacité d'analyse et de résolution de problèmes. Langues Maitrise de l'anglais et du français.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ZKXath4vGCKu"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}