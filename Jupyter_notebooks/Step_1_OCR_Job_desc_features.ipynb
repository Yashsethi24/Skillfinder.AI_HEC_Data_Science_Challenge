{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M61DaJh5BNb9",
        "outputId": "ef774673-ebee-47c4-aa61-eea581443c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m969.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.7)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->easyocr)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->easyocr)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->easyocr)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->easyocr)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->easyocr)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->easyocr)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.3.13)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, easyocr\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyclipper-1.3.0.post6 python-bidi-0.6.6\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=bdadbcd08e66608a62b511c2d01cbe249b1a26fc23884bb8833d591a6f8628ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install pdf2image\n",
        "!pip install easyocr\n",
        "!pip install deep_translator\n",
        "!pip install langdetect\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from pdf2image import convert_from_path\n",
        "import easyocr\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from deep_translator import GoogleTranslator\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os"
      ],
      "metadata": {
        "id": "5DNbUwuxBcNo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt', quiet=True)\n",
        "reader = easyocr.Reader(['en'], gpu=False)\n",
        "translator = GoogleTranslator(source='auto', target='en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lSVKiRbBe-g",
        "outputId": "7c88f90c-5d20-4123-b8fe-1758413eba50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_pdf_file(file_path):\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = \"\\n\".join([page.extract_text() or '' for page in pdf.pages])\n",
        "        if text.strip():\n",
        "            return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"pdfplumber failed: {e}\")\n",
        "\n",
        "    # Fallback to OCR\n",
        "    try:\n",
        "        images = convert_from_path(file_path)\n",
        "        text = \"\"\n",
        "        for image in images:\n",
        "            result = reader.readtext(np.array(image), detail=0, paragraph=True)\n",
        "            text += \" \".join(result) + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"OCR failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Reuse the same preprocess function\n",
        "def preprocess_text(text):\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    text = text.strip().lower()\n",
        "    return text\n",
        "\n",
        "def force_split_text(text, max_len):\n",
        "    return [text[i:i+max_len] for i in range(0, len(text), max_len)]\n",
        "\n",
        "def safe_translate(text, max_chunk_size=4500):\n",
        "    try:\n",
        "        # Ensure input is string\n",
        "        if not isinstance(text, str):\n",
        "            text = '' if pd.isna(text) else str(text)\n",
        "\n",
        "        text = text.strip()\n",
        "        if not text:\n",
        "            return ''\n",
        "\n",
        "        if len(text) <= max_chunk_size:\n",
        "            return translator.translate(text)\n",
        "\n",
        "        # Tokenize into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "        chunks = []\n",
        "        current_chunk = ''\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            if len(sentence) > max_chunk_size:\n",
        "                sub_chunks = force_split_text(sentence, max_chunk_size)\n",
        "                chunks.extend(sub_chunks)\n",
        "                continue\n",
        "\n",
        "            if len(current_chunk) + len(sentence) + 1 > max_chunk_size:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk += ' ' + sentence\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        # Translate chunks safely\n",
        "        translated_chunks = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if len(chunk) > max_chunk_size:\n",
        "                print(f\"[SKIPPED] Chunk {i} exceeds {max_chunk_size} characters: {len(chunk)} chars\")\n",
        "                sub_chunks = force_split_text(chunk, max_chunk_size)\n",
        "                for sub_chunk in sub_chunks:\n",
        "                    translated_chunks.append(translator.translate(sub_chunk))\n",
        "            else:\n",
        "                translated_chunks.append(translator.translate(chunk))\n",
        "\n",
        "        return ' '.join(translated_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        snippet = str(text)[:100].replace('\\n', ' ')\n",
        "        print(f\"Translation error for: {snippet}... — {e}\")\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "hZSx7Hq8Bg_P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing PDF files\n",
        "pdf_directory = \"/content/\"  # Adjust this path\n",
        "\n",
        "# List of PDF files\n",
        "pdf_files = [\"Scrum.pdf\", \"Data Engineer.pdf\", \"Data Analyst.pdf\"]\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Process each PDF file\n",
        "for pdf in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_directory, pdf)\n",
        "\n",
        "    # Extract text using OCR\n",
        "    extracted_text = preprocess_text(extract_text_from_pdf_file(pdf_path))\n",
        "\n",
        "    # Translate to English\n",
        "    translated_text = safe_translate(extracted_text)\n",
        "\n",
        "    # Append data to the list\n",
        "    data.append({\n",
        "        \"demand_id\": os.path.splitext(pdf)[0],\n",
        "        \"extracted_text\": extracted_text,\n",
        "        \"translated_text\": translated_text\n",
        "    })\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg6Yt016BjTl",
        "outputId": "4246266a-a7ee-4094-8390-fd214d4bc7ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation error for: poste : data engineer • type de contrat : contrat • date de début : dès que possible • durée du m... — \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "YIdTQAVpCS8e",
        "outputId": "247b4e75-327d-407b-c1b6-448380658a2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       demand_id                                     extracted_text  \\\n",
              "0          Scrum  scrum master – contrat de 12 mois lieu : montr...   \n",
              "1  Data Engineer  poste : data engineer • type de contrat : cont...   \n",
              "2   Data Analyst  profil data analyst – marketing analytics à p...   \n",
              "\n",
              "                                     translated_text  \n",
              "0  Scrum Master - 12 -month contract Place: Montr...  \n",
              "1  poste : data engineer • type de contrat : cont...  \n",
              "2  Profile Data Analyst - Marketing Analytics abo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcb37b0f-c0d2-445d-bd89-77d076911a7b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>demand_id</th>\n",
              "      <th>extracted_text</th>\n",
              "      <th>translated_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scrum</td>\n",
              "      <td>scrum master – contrat de 12 mois lieu : montr...</td>\n",
              "      <td>Scrum Master - 12 -month contract Place: Montr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Engineer</td>\n",
              "      <td>poste : data engineer • type de contrat : cont...</td>\n",
              "      <td>poste : data engineer • type de contrat : cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>profil data analyst – marketing analytics à p...</td>\n",
              "      <td>Profile Data Analyst - Marketing Analytics abo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcb37b0f-c0d2-445d-bd89-77d076911a7b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dcb37b0f-c0d2-445d-bd89-77d076911a7b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dcb37b0f-c0d2-445d-bd89-77d076911a7b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ffdf4c3a-29bb-46e1-8f30-faceab1a4894\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ffdf4c3a-29bb-46e1-8f30-faceab1a4894')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ffdf4c3a-29bb-46e1-8f30-faceab1a4894 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b08583c8-1ccd-4098-ac25-14320078286d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b08583c8-1ccd-4098-ac25-14320078286d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"demand_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Scrum\",\n          \"Data Engineer\",\n          \"Data Analyst\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extracted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"scrum master \\u2013 contrat de 12 mois lieu : montre\\u0301al date de de\\u0301marrage : de\\u0300s que possible dure\\u0301e : 12 mois, avec possibilite\\u0301 de renouvellement dans le cadre d\\u2019un mandat d\\u2019importance strate\\u0301gique, nous recherchons un(e) scrum master expe\\u0301rimente\\u0301(e) pour guider une e\\u0301quipe agile multidisciplinaire au sein d\\u2019un environnement stimulant et en pleine transformation nume\\u0301rique. nom de l\\u2019entreprise : alimora groupe slogan : cultiver l\\u2019avenir, nourrir l\\u2019excellence pre\\u0301sentation de l\\u2019entreprise : fonde\\u0301 en 1998, alimora groupe est un acteur majeur de l\\u2019agroalimentaire durable en ame\\u0301rique du nord. spe\\u0301cialise\\u0301 dans la production, la transformation et la distribution de produits alimentaires de qualite\\u0301, le groupe s'engage a\\u0300 offrir une alimentation saine et accessible, tout en respectant l\\u2019environnement et les communaute\\u0301s locales. avec plus de 1 500 employe\\u0301s re\\u0301partis sur 7 sites de production et une pre\\u0301sence dans plus de 12 pays, alimora se distingue par son innovation constante, son excellence ope\\u0301rationnelle et sa capacite\\u0301 a\\u0300 anticiper les tendances de consommation. domaines d\\u2019expertise : \\u2022 transformation de produits frais (fruits, le\\u0301gumes, prote\\u0301ines ve\\u0301ge\\u0301tales) \\u2022 production bio et circuits courts \\u2022 recherche & de\\u0301veloppement en nutrition durable \\u2022 logistique inte\\u0301gre\\u0301e et chai\\u0302ne du froid \\u2022 de\\u0301veloppement de produits a\\u0300 marque prive\\u0301e (mdd) ro\\u0302le et responsabilite\\u0301s en tant que scrum master, vous jouerez un ro\\u0302le cle\\u0301 dans le pilotage de l\\u2019agilite\\u0301 ope\\u0301rationnelle d\\u2019une e\\u0301quipe, en favorisant l\\u2019atteinte des objectifs d\\u2019affaires tout en renforc\\u0327ant la collaboration, la transparence et l\\u2019ame\\u0301lioration continue. vos responsabilite\\u0301s incluent notamment : \\u2022 mettre en place et maintenir un cadre agile efficace propice a\\u0300 la performance collective. \\u2022 faciliter l\\u2019ensemble des ce\\u0301re\\u0301monies scrum (daily scrum, sprint planning, sprint review, re\\u0301trospective, refinement). \\u2022 assurer une gestion proactive des obstacles, afin de garantir le bon de\\u0301roulement des livrables. \\u2022 accompagner l\\u2019e\\u0301quipe dans le de\\u0301veloppement de son autonomie et de sa capacite\\u0301 d\\u2019auto-organisation. \\u2022 agir en tant que coach agile, en guidant l\\u2019e\\u0301quipe et les parties prenantes dans la compre\\u0301hension et l\\u2019application des valeurs, principes et pratiques agiles. \\u2022 promouvoir une culture de collaboration et d\\u2019ame\\u0301lioration continue. \\u2022 aider le product owner a\\u0300 ge\\u0301rer efficacement le backlog produit, a\\u0300 de\\u0301finir les priorite\\u0301s et a\\u0300 maximiser la valeur livre\\u0301e. \\u2022 mesurer et analyser la performance de l\\u2019e\\u0301quipe via des indicateurs cle\\u0301s (ve\\u0301locite\\u0301, burndown charts, lead time, etc.), en utilisant des outils adapte\\u0301s. \\u2022 prote\\u0301ger l\\u2019e\\u0301quipe des interfe\\u0301rences externes et veiller a\\u0300 maintenir un climat de travail serein. environnement technologique et outils utilise\\u0301s \\u2022 outils de gestion agile : jira, confluence \\u2022 communication et collaboration : microsoft teams, slack, miro \\u2022 tableaux de bord et me\\u0301triques : jira dashboards, power bi, excel avance\\u0301 \\u2022 me\\u0301thodologies : scrum, kanban, safe (atout), lean \\u2022 langues : franc\\u0327ais (essentiel), anglais (fonctionnel) profil recherche\\u0301 \\u2022 minimum 3 ans d\\u2019expe\\u0301rience en tant que scrum master dans un contexte agile a\\u0300 fort enjeu. \\u2022 solide compre\\u0301hension des cadres me\\u0301thodologiques agiles (scrum, kanban, lean) et capacite\\u0301 a\\u0300 adapter les pratiques au contexte de l\\u2019e\\u0301quipe. \\u2022 excellentes compe\\u0301tences interpersonnelles : leadership, diplomatie, capacite\\u0301 d\\u2019influence. \\u2022 fort esprit d\\u2019analyse et capacite\\u0301 a\\u0300 traduire les proble\\u0301matiques en actions concre\\u0300tes. \\u2022 expe\\u0301rience dans un contexte hybride ou multi-e\\u0301quipes est un atout. ce mandat vous permettra de contribuer activement a\\u0300 la re\\u0301ussite d\\u2019une e\\u0301quipe agile engage\\u0301e, tout en e\\u0301voluant dans un environnement propice a\\u0300 la transformation nume\\u0301rique et a\\u0300 l'innovation.\",\n          \"poste : data engineer \\u2022 type de contrat : contrat \\u2022 date de de\\u0301but : de\\u0300s que possible \\u2022 dure\\u0301e du mandat : 12 mois \\u2022 date limite de de\\u0301po\\u0302t : 6 novembre a\\u0300 14h a\\u0300 propos de l\\u2019entreprise : retailnova est un leader du secteur du commerce de de\\u0301tail, avec un re\\u0301seau de distribution omnicanal pre\\u0301sent a\\u0300 travers tout le territoire. l'entreprise combine innovation technologique et excellence ope\\u0301rationnelle pour optimiser ses processus, anticiper les besoins des clients et maximiser la performance. dans le cadre de sa strate\\u0301gie de transformation nume\\u0301rique, retailnova investit massivement dans la modernisation de ses infrastructures de donne\\u0301es pour acce\\u0301le\\u0301rer la prise de de\\u0301cision fonde\\u0301e sur des analyses avance\\u0301es. contexte du mandat : retailnova met en \\u0153uvre une nouvelle plateforme d'entreprise base\\u0301e sur des technologies cloud afin de centraliser, se\\u0301curiser et valoriser ses donne\\u0301es ope\\u0301rationnelles, transactionnelles et clients. ce projet strate\\u0301gique vise a\\u0300 moderniser l'e\\u0301cosyste\\u0300me analytique, a\\u0300 automatiser les flux de donne\\u0301es, a\\u0300 ame\\u0301liorer la gouvernance, et a\\u0300 renforcer la qualite\\u0301 et la trac\\u0327abilite\\u0301 des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019entreprise. le data engineer sera responsable de la conception, du de\\u0301veloppement, de l\\u2019optimisation et du de\\u0301ploiement des pipelines de donne\\u0301es ne\\u0301cessaires a\\u0300 l\\u2019alimentation du data lake d\\u2019entreprise. il interviendra e\\u0301galement dans la mise en place de nouvelles architectures de traitement en temps re\\u0301el, en assurant la robustesse, la scalabilite\\u0301 et la performance des solutions de\\u0301ploye\\u0301es. responsabilite\\u0301s de\\u0301taille\\u0301es : \\u2022 concevoir et de\\u0301velopper des pipelines d\\u2019ingestion de donne\\u0301es robustes et optimise\\u0301s a\\u0300 l\\u2019aide d\\u2019azure data factory pour inte\\u0301grer des donne\\u0301es provenant de sources multiples (erp, crm, pos, e-commerce, etc.). \\u2022 de\\u0301velopper des transformations complexes en utilisant apache spark (scala/python) dans azure databricks, avec une forte orientation performance et optimisation des cou\\u0302ts. \\u2022 imple\\u0301menter des flux de donne\\u0301es streaming et batch via databricks streaming et apache kafka, en assurant une faible latence et une haute disponibilite\\u0301. \\u2022 participer a\\u0300 la de\\u0301finition et a\\u0300 la mise en \\u0153uvre de mode\\u0300les de donne\\u0301es adapte\\u0301s aux besoins analytiques (data marts, data lakehouse). \\u2022 mettre en \\u0153uvre des processus de validation de la qualite\\u0301 des donne\\u0301es (data quality checks, data profiling, monitoring automatise\\u0301). \\u2022 automatiser les de\\u0301ploiements et les tests des pipelines via azure devops (ci/cd, versioning, rollback). \\u2022 documenter les solutions de\\u0301veloppe\\u0301es selon les normes internes, et assurer la trac\\u0327abilite\\u0301 et la re\\u0301plicabilite\\u0301 des traitements. \\u2022 travailler en collaboration avec les e\\u0301quipes data science, bi et it infrastructure pour garantir l\\u2019alignement des livrables avec les objectifs strate\\u0301giques. \\u2022 participer a\\u0300 l\\u2019e\\u0301laboration de normes et de standards d\\u2019inge\\u0301nierie des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019organisation. compe\\u0301tences techniques requises : \\u2022 5+ ans d'expe\\u0301rience en de\\u0301veloppement d\\u2019applications de traitement de donne\\u0301es distribue\\u0301es avec apache spark (obligatoire). \\u2022 solide mai\\u0302trise de python et scala pour le de\\u0301veloppement de traitements de donne\\u0301es volumineuses. \\u2022 expe\\u0301rience significative avec azure data factory, azure data lake storage (gen2), et azure databricks. \\u2022 connaissance approfondie de kafka (installation, configuration, gestion des topics, se\\u0301curite\\u0301, monitoring). \\u2022 mai\\u0302trise des principes d\\u2019architecture data : partitionnement, indexation, optimisation des formats (parquet, delta lake), gestion des me\\u0301tadonne\\u0301es. \\u2022 expe\\u0301rience avec les outils de gestion des flux ci/cd dans azure devops (pipelines, artefacts, gestion de configuration). \\u2022 connaissance des principes de gouvernance des donne\\u0301es, de catalogage (ex. azure purview), et des bonnes pratiques de se\\u0301curite\\u0301 (rbac, encryption, masking). \\u2022 capacite\\u0301 a\\u0300 concevoir des mode\\u0300les de donne\\u0301es analytiques (kimball, data vault) et a\\u0300 travailler avec des entrepo\\u0302ts de donne\\u0301es (ex. synapse analytics). atouts : \\u2022 expe\\u0301rience dans des environnements retail ou grande distribution (gestion des donne\\u0301es transactionnelles, gestion de la supply chain, etc.). \\u2022 expe\\u0301rience avec des outils de monitoring des pipelines (ex : azure monitor, log analytics, datadog). \\u2022 connaissances de base en data science/machine learning pour interagir avec les e\\u0301quipes concerne\\u0301es. compe\\u0301tences interpersonnelles : \\u2022 forte capacite\\u0301 a\\u0300 re\\u0301soudre des proble\\u0300mes complexes de manie\\u0300re autonome. \\u2022 rigueur et attention aux de\\u0301tails, particulie\\u0300rement dans les aspects lie\\u0301s a\\u0300 la qualite\\u0301 et la fiabilite\\u0301 des donne\\u0301es. \\u2022 compe\\u0301tences en communication technique, capacite\\u0301 a\\u0300 documenter et pre\\u0301senter des solutions techniques a\\u0300 divers niveaux d\\u2019interlocuteurs. \\u2022 esprit d\\u2019e\\u0301quipe, ouverture au partage de connaissances et participation active a\\u0300 l\\u2019ame\\u0301lioration continue. langues : \\u2022 franc\\u0327ais et anglais : mai\\u0302trise professionnelle comple\\u0300te, tant a\\u0300 l\\u2019oral qu\\u2019a\\u0300 l\\u2019e\\u0301crit.\",\n          \"profil data analyst \\u2013 marketing analytics a\\u0300 propos de veltrixia veltrixia technologies inc. est une entreprise spe\\u0301cialise\\u0301e dans les solutions de donne\\u0301es et l'intelligence marketing. base\\u0301e a\\u0300 montre\\u0301al, veltrixia accompagne depuis plus de 15 ans des entreprises nord-ame\\u0301ricaines dans l\\u2019optimisation de leurs performances gra\\u0302ce a\\u0300 des technologies analytiques avance\\u0301es. l\\u2019entreprise mise sur l\\u2019innovation, la collaboration et l\\u2019excellence pour propulser ses clients vers le futur nume\\u0301rique. contexte dans le cadre du de\\u0301veloppement de notre e\\u0301quipe analytique marketing, nous recherchons un\\u00b7e analyste de donne\\u0301es pour travailler sur des projets lie\\u0301s a\\u0300 la collecte, l\\u2019analyse et la visualisation de donne\\u0301es marketing. vous serez responsable de la maintenance et de l\\u2019e\\u0301volution de jeux de donne\\u0301es provenant de sources varie\\u0301es, afin de soutenir les e\\u0301quipes dans leur prise de de\\u0301cision base\\u0301e sur les donne\\u0301es. de\\u0301tails du poste \\u2022 dure\\u0301e : 10 mois (renouvelable) \\u2022 type : temps plein \\u2022 mode de travail : hybride (2 jours par semaine au bureau a\\u0300 montre\\u0301al) responsabilite\\u0301s principales \\u2022 ge\\u0301rer et ame\\u0301liorer les jeux de donne\\u0301es marketing. \\u2022 de\\u0301velopper des tableaux de bord et rapports dans power bi. \\u2022 optimiser les performances des datasets power bi. \\u2022 inte\\u0301grer les donne\\u0301es issues de diverses plateformes (google analytics, crm, outils marketing). \\u2022 re\\u0301diger du code sql, dax et power query performant. \\u2022 collaborer e\\u0301troitement avec les analystes pour comprendre leurs besoins. \\u2022 cre\\u0301er des reque\\u0302tes dans databricks et ssms. \\u2022 assurer la qualite\\u0301 et la fiabilite\\u0301 des donne\\u0301es. \\u2022 soutenir les colle\\u0300gues via des revues de code et du support technique. \\u2022 concevoir des mode\\u0300les de donne\\u0301es adapte\\u0301s aux objectifs me\\u0301tiers. profil recherche\\u0301 compe\\u0301tences techniques \\u2022 minimum 5 ans d\\u2019expe\\u0301rience en analyse de donne\\u0301es. \\u2022 solide mai\\u0302trise de sql (ms sql server), power bi, databricks. \\u2022 expe\\u0301rience avec dax, power query, etl. \\u2022 connaissance des donne\\u0301es marketing, un atout. \\u2022 capacite\\u0301 a\\u0300 mode\\u0301liser les donne\\u0301es et a\\u0300 optimiser les processus. compe\\u0301tences personnelles \\u2022 curieux\\u00b7se, autonome, rigoureux\\u00b7se. \\u2022 bonne gestion du temps et esprit d\\u2019e\\u0301quipe. \\u2022 excellente capacite\\u0301 d\\u2019analyse et de re\\u0301solution de proble\\u0300mes. langues \\u2022 mai\\u0302trise de l\\u2019anglais et du franc\\u0327ais.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"translated_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Scrum Master - 12 -month contract Place: Montreal Start date: As soon as possible Duration: 12 months, with the possibility of renewal as part of a mandate of strategic importance, we are looking for an experienced scrum master to guide an agile multidisciplinary team within a stimulating environment and in full digital transformation. Company name: Alimora Slogan Group: Cultivate the future, nourishing the excellence of the company: Founded in 1998, Alimora Groupe is a major player in the sustainable food industry in North America. Specializing in the production, processing and distribution of quality food products, the group undertakes to offer a healthy and accessible food, while respecting the environment and local communities. With more than 1,500 employees spread over 7 production sites and a presence in more than 12 countries, Alimora is distinguished by its constant innovation, its operational excellence and its ability to anticipate consumption trends. Domains of expertise: \\u2022 Transformation of fresh products (fruits, vegetables, vegetable proteins) \\u2022 Organic production and short circuits \\u2022 Research & development in sustainable nutrition \\u2022 Integrated logistics and cold of cold products (MDD) Role and responsibilities as scrum master, you will play a key role in the management of the operational agility of a team, promoting the achievement of business objectives while reinforcing collaboration, transparency and continuous improvement. Your responsibilities include in particular: \\u2022 Set up and maintain an efficient framework conducive to collective performance. \\u2022 Facilitate all Scrum ceremonies (Daily Scrum, Sprint Planning, Sprint Review, Retrospective, Refinement). \\u2022 Ensure proactive management of obstacles, in order to guarantee the smooth running of deliverables. \\u2022 Support the team in the development of its autonomy and its self-organization capacity. \\u2022 Act as an agile coach, guiding the team and the stakeholders in the understanding and application of agile values, principles and practices. \\u2022 Promote a culture of collaboration and continuous improvement. \\u2022 Help the product Owner effectively manages the product backlog, defining priorities and maximizing the delivered value. \\u2022 Measure and analyze the performance of the team via key indicators (velocity, burndown charts, lead time, etc.), using suitable tools. \\u2022 Protect the external interference team and ensure that you maintain a serene working climate. Technological environment and tools used \\u2022 Agile management tools: JIRA, Confluence \\u2022 Communication and collaboration: Microsoft Teams, Slack, Miro \\u2022 Dashboards and metrics: Jira Dashboards, Power Bi, Excel Advanced \\u2022 Methodologies: Scrum, Kanban, Safe (Atout), Lean \\u2022 Languages: French (essential), English (functional) that Scrum Master in an agile context at high stake. \\u2022 Solid understanding of agile methodological frameworks (Scrum, Kanban, Lean) and ability to adapt practices to the context of the team. \\u2022 Excellent interpersonal skills: leadership, diplomacy, ability to influence. \\u2022 Strong spirit of analysis and ability to translate issues into concrete actions. \\u2022 Experience in a hybrid or multi-team context is an asset. This mandate will allow you to actively contribute to the success of an agile committed team, while evolving in an environment conducive to digital transformation and innovation.\",\n          \"poste : data engineer \\u2022 type de contrat : contrat \\u2022 date de de\\u0301but : de\\u0300s que possible \\u2022 dure\\u0301e du mandat : 12 mois \\u2022 date limite de de\\u0301po\\u0302t : 6 novembre a\\u0300 14h a\\u0300 propos de l\\u2019entreprise : retailnova est un leader du secteur du commerce de de\\u0301tail, avec un re\\u0301seau de distribution omnicanal pre\\u0301sent a\\u0300 travers tout le territoire. l'entreprise combine innovation technologique et excellence ope\\u0301rationnelle pour optimiser ses processus, anticiper les besoins des clients et maximiser la performance. dans le cadre de sa strate\\u0301gie de transformation nume\\u0301rique, retailnova investit massivement dans la modernisation de ses infrastructures de donne\\u0301es pour acce\\u0301le\\u0301rer la prise de de\\u0301cision fonde\\u0301e sur des analyses avance\\u0301es. contexte du mandat : retailnova met en \\u0153uvre une nouvelle plateforme d'entreprise base\\u0301e sur des technologies cloud afin de centraliser, se\\u0301curiser et valoriser ses donne\\u0301es ope\\u0301rationnelles, transactionnelles et clients. ce projet strate\\u0301gique vise a\\u0300 moderniser l'e\\u0301cosyste\\u0300me analytique, a\\u0300 automatiser les flux de donne\\u0301es, a\\u0300 ame\\u0301liorer la gouvernance, et a\\u0300 renforcer la qualite\\u0301 et la trac\\u0327abilite\\u0301 des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019entreprise. le data engineer sera responsable de la conception, du de\\u0301veloppement, de l\\u2019optimisation et du de\\u0301ploiement des pipelines de donne\\u0301es ne\\u0301cessaires a\\u0300 l\\u2019alimentation du data lake d\\u2019entreprise. il interviendra e\\u0301galement dans la mise en place de nouvelles architectures de traitement en temps re\\u0301el, en assurant la robustesse, la scalabilite\\u0301 et la performance des solutions de\\u0301ploye\\u0301es. responsabilite\\u0301s de\\u0301taille\\u0301es : \\u2022 concevoir et de\\u0301velopper des pipelines d\\u2019ingestion de donne\\u0301es robustes et optimise\\u0301s a\\u0300 l\\u2019aide d\\u2019azure data factory pour inte\\u0301grer des donne\\u0301es provenant de sources multiples (erp, crm, pos, e-commerce, etc.). \\u2022 de\\u0301velopper des transformations complexes en utilisant apache spark (scala/python) dans azure databricks, avec une forte orientation performance et optimisation des cou\\u0302ts. \\u2022 imple\\u0301menter des flux de donne\\u0301es streaming et batch via databricks streaming et apache kafka, en assurant une faible latence et une haute disponibilite\\u0301. \\u2022 participer a\\u0300 la de\\u0301finition et a\\u0300 la mise en \\u0153uvre de mode\\u0300les de donne\\u0301es adapte\\u0301s aux besoins analytiques (data marts, data lakehouse). \\u2022 mettre en \\u0153uvre des processus de validation de la qualite\\u0301 des donne\\u0301es (data quality checks, data profiling, monitoring automatise\\u0301). \\u2022 automatiser les de\\u0301ploiements et les tests des pipelines via azure devops (ci/cd, versioning, rollback). \\u2022 documenter les solutions de\\u0301veloppe\\u0301es selon les normes internes, et assurer la trac\\u0327abilite\\u0301 et la re\\u0301plicabilite\\u0301 des traitements. \\u2022 travailler en collaboration avec les e\\u0301quipes data science, bi et it infrastructure pour garantir l\\u2019alignement des livrables avec les objectifs strate\\u0301giques. \\u2022 participer a\\u0300 l\\u2019e\\u0301laboration de normes et de standards d\\u2019inge\\u0301nierie des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019organisation. compe\\u0301tences techniques requises : \\u2022 5+ ans d'expe\\u0301rience en de\\u0301veloppement d\\u2019applications de traitement de donne\\u0301es distribue\\u0301es avec apache spark (obligatoire). \\u2022 solide mai\\u0302trise de python et scala pour le de\\u0301veloppement de traitements de donne\\u0301es volumineuses. \\u2022 expe\\u0301rience significative avec azure data factory, azure data lake storage (gen2), et azure databricks. \\u2022 connaissance approfondie de kafka (installation, configuration, gestion des topics, se\\u0301curite\\u0301, monitoring). \\u2022 mai\\u0302trise des principes d\\u2019architecture data : partitionnement, indexation, optimisation des formats (parquet, delta lake), gestion des me\\u0301tadonne\\u0301es. \\u2022 expe\\u0301rience avec les outils de gestion des flux ci/cd dans azure devops (pipelines, artefacts, gestion de configuration). \\u2022 connaissance des principes de gouvernance des donne\\u0301es, de catalogage (ex. azure purview), et des bonnes pratiques de se\\u0301curite\\u0301 (rbac, encryption, masking). \\u2022 capacite\\u0301 a\\u0300 concevoir des mode\\u0300les de donne\\u0301es analytiques (kimball, data vault) et a\\u0300 travailler avec des entrepo\\u0302ts de donne\\u0301es (ex. synapse analytics). atouts : \\u2022 expe\\u0301rience dans des environnements retail ou grande distribution (gestion des donne\\u0301es transactionnelles, gestion de la supply chain, etc.). \\u2022 expe\\u0301rience avec des outils de monitoring des pipelines (ex : azure monitor, log analytics, datadog). \\u2022 connaissances de base en data science/machine learning pour interagir avec les e\\u0301quipes concerne\\u0301es. compe\\u0301tences interpersonnelles : \\u2022 forte capacite\\u0301 a\\u0300 re\\u0301soudre des proble\\u0300mes complexes de manie\\u0300re autonome. \\u2022 rigueur et attention aux de\\u0301tails, particulie\\u0300rement dans les aspects lie\\u0301s a\\u0300 la qualite\\u0301 et la fiabilite\\u0301 des donne\\u0301es. \\u2022 compe\\u0301tences en communication technique, capacite\\u0301 a\\u0300 documenter et pre\\u0301senter des solutions techniques a\\u0300 divers niveaux d\\u2019interlocuteurs. \\u2022 esprit d\\u2019e\\u0301quipe, ouverture au partage de connaissances et participation active a\\u0300 l\\u2019ame\\u0301lioration continue. langues : \\u2022 franc\\u0327ais et anglais : mai\\u0302trise professionnelle comple\\u0300te, tant a\\u0300 l\\u2019oral qu\\u2019a\\u0300 l\\u2019e\\u0301crit.\",\n          \"Profile Data Analyst - Marketing Analytics about Veltrixia Veltrixia Technologies inc. is a company specializing in data solutions and marketing intelligence. Based in Montreal, Veltrixia has been supporting North American companies for more than 15 years in optimizing their performance through advanced analytical technologies. The company relies on innovation, collaboration and excellence to propel its customers to the future digital. Context within the framework of the development of our analytical marketing team, we are looking for an analyst of data to work on projects related to the collection, analysis and visualization of marketing data. You will be responsible for the maintenance and evolution of datasets from various sources, in order to support the teams in their data -based decision -making. Details of the position \\u2022 Duration: 10 months (renewable) \\u2022 Type: full time \\u2022 Work mode: hybrid (2 days per week at the office in Montreal) main responsibilities \\u2022 Manage and improve marketing data games. \\u2022 Develop dashboards and reports in Power Bi. \\u2022 Optimize the performance of Power Bi datasets. \\u2022 Integrate data from various platforms (Google Analytics, CRM, Marketing Tools). \\u2022 Write SQL, Dax and powerful power query code. \\u2022 Collaborate closely with analysts to understand their needs. \\u2022 Create requests in Databricks and SSMS. \\u2022 Ensure the quality and reliability of the data. \\u2022 Support colleagues via code journals and technical support. \\u2022 Design data models adapted to business objectives. Profile sought technical skills \\u2022 minimum 5 years of experience in data analysis. \\u2022 Solid mastery of SQL (MS SQL Server), Power Bi, Databricks. \\u2022 Experience with Dax, Power Query, etl. \\u2022 Knowledge of marketing data, an asset. \\u2022 Ability to model data and optimize processes. Personal skills \\u2022 Curious, autonomous, rigorous. \\u2022 Good time management and team spirit. \\u2022 Excellent analytical and problem solving capacity. Languages \\u200b\\u200b\\u2022 Mastery of English and French.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = OpenAI(api_key=\"key\")\n",
        "\n",
        "\n",
        "def extract_features_openai_v1(job_description):\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert in job description analysis. Extract structured information from the following job description and categorize skills and languages using a 1–3 scale as per the given guidelines.\n",
        "\n",
        "    ### Job Description:\n",
        "    {job_description}\n",
        "\n",
        "    ### Guidelines for Scaling (1-3)\n",
        "\n",
        "    - **Required & Preferred Skills**:\n",
        "      - **3** = Critical expertise (e.g., \"Expert in Python\", \"2+ years experience in Kubernetes\")\n",
        "      - **2** = Required but not expert level (e.g., \"Required: Java, C++\")\n",
        "      - **1** = Mentioned but not explicitly required (e.g., \"Nice to have: AWS\")\n",
        "\n",
        "    - **Languages**:\n",
        "      - **3** = Critical requirement (e.g., \"Fluency in French is essential\")\n",
        "      - **2** = Important but secondary (e.g., \"Functional English required\")\n",
        "      - **1** = Nice to have (e.g., \"Basic German knowledge preferred\")\n",
        "\n",
        "    ### Output Format (Strict JSON):\n",
        "    {{\n",
        "      \"Job Title\": \"<Job Title>\",\n",
        "      \"Required Skills\": [{{\"skill\": \"<Skill>\", \"level\": <1|2|3>}}],\n",
        "      \"Preferred Skills\": [{{\"skill\": \"<Skill>\", \"level\": <1|2|3>}}],\n",
        "      \"Experience Required\": <float>,\n",
        "      \"Languages\": [{{\"language\": \"<Language>\", \"level\": <1|2|3>}}],\n",
        "      \"Responsibilities\": [\"<bullet point responsibility 1>\", \"...\"],\n",
        "      \"Location\": \"<City or leave empty>\",\n",
        "      \"Salary\": \"<Value or empty>\",\n",
        "      \"Additional Notes\": {{\n",
        "        \"Duration\": \"<value>\",\n",
        "        \"Type\": \"<value>\",\n",
        "        \"Mode of work\": \"<value>\"\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    Ensure the JSON is properly formatted, strictly follows the structure, and only includes relevant data.\n",
        "    **Translate all extracted information into English, even if the job description is written in another language.**\n",
        "        \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        return json.loads(content)\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "#         # response = openai.ChatCompletion.create(\n",
        "#         #     model=\"gpt-4\",\n",
        "#         #     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#         #     temperature=0.2,\n",
        "#         # )\n",
        "#         response = openai.ChatCompletion.create(\n",
        "#             model=\"gpt-3.5-turbo-16k\",  # <- switch here\n",
        "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#             temperature=0.2,\n",
        "#         )\n",
        "\n",
        "#         content = response.choices[0].message[\"content\"]\n",
        "#         extracted_features = json.loads(content)\n",
        "#     except Exception as e:\n",
        "#         extracted_features = {\"error\": str(e)}\n",
        "\n",
        "    return extracted_features\n",
        "\n",
        "# Apply to DataFrame\n",
        "df[\"extracted_features\"] = df[\"extracted_text\"].apply(extract_features_openai_v1)\n"
      ],
      "metadata": {
        "id": "MLNezmk1BlrA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "JtWFykXQK9mY",
        "outputId": "e58c28f4-2311-48ca-871f-859386eaa29e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       demand_id                                     extracted_text  \\\n",
              "0          Scrum  scrum master – contrat de 12 mois lieu : montr...   \n",
              "1  Data Engineer  poste : data engineer • type de contrat : cont...   \n",
              "2   Data Analyst  profil data analyst – marketing analytics à p...   \n",
              "\n",
              "                                     translated_text  \\\n",
              "0  Scrum Master - 12 -month contract Place: Montr...   \n",
              "1  poste : data engineer • type de contrat : cont...   \n",
              "2  Profile Data Analyst - Marketing Analytics abo...   \n",
              "\n",
              "                                  extracted_features  \n",
              "0  {'Job Title': 'Scrum Master', 'Required Skills...  \n",
              "1  {'Job Title': 'Data Engineer', 'Required Skill...  \n",
              "2  {'Job Title': 'Data Analyst - Marketing Analyt...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91f917d3-9177-480a-bcb3-d42958d8cc8a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>demand_id</th>\n",
              "      <th>extracted_text</th>\n",
              "      <th>translated_text</th>\n",
              "      <th>extracted_features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scrum</td>\n",
              "      <td>scrum master – contrat de 12 mois lieu : montr...</td>\n",
              "      <td>Scrum Master - 12 -month contract Place: Montr...</td>\n",
              "      <td>{'Job Title': 'Scrum Master', 'Required Skills...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Engineer</td>\n",
              "      <td>poste : data engineer • type de contrat : cont...</td>\n",
              "      <td>poste : data engineer • type de contrat : cont...</td>\n",
              "      <td>{'Job Title': 'Data Engineer', 'Required Skill...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>profil data analyst – marketing analytics à p...</td>\n",
              "      <td>Profile Data Analyst - Marketing Analytics abo...</td>\n",
              "      <td>{'Job Title': 'Data Analyst - Marketing Analyt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91f917d3-9177-480a-bcb3-d42958d8cc8a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91f917d3-9177-480a-bcb3-d42958d8cc8a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91f917d3-9177-480a-bcb3-d42958d8cc8a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bf0ea783-4f60-4387-9259-75bb122e8c25\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf0ea783-4f60-4387-9259-75bb122e8c25')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bf0ea783-4f60-4387-9259-75bb122e8c25 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_575bccfc-0690-4f04-b527-ae261f47ee42\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_575bccfc-0690-4f04-b527-ae261f47ee42 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"demand_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Scrum\",\n          \"Data Engineer\",\n          \"Data Analyst\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extracted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"scrum master \\u2013 contrat de 12 mois lieu : montre\\u0301al date de de\\u0301marrage : de\\u0300s que possible dure\\u0301e : 12 mois, avec possibilite\\u0301 de renouvellement dans le cadre d\\u2019un mandat d\\u2019importance strate\\u0301gique, nous recherchons un(e) scrum master expe\\u0301rimente\\u0301(e) pour guider une e\\u0301quipe agile multidisciplinaire au sein d\\u2019un environnement stimulant et en pleine transformation nume\\u0301rique. nom de l\\u2019entreprise : alimora groupe slogan : cultiver l\\u2019avenir, nourrir l\\u2019excellence pre\\u0301sentation de l\\u2019entreprise : fonde\\u0301 en 1998, alimora groupe est un acteur majeur de l\\u2019agroalimentaire durable en ame\\u0301rique du nord. spe\\u0301cialise\\u0301 dans la production, la transformation et la distribution de produits alimentaires de qualite\\u0301, le groupe s'engage a\\u0300 offrir une alimentation saine et accessible, tout en respectant l\\u2019environnement et les communaute\\u0301s locales. avec plus de 1 500 employe\\u0301s re\\u0301partis sur 7 sites de production et une pre\\u0301sence dans plus de 12 pays, alimora se distingue par son innovation constante, son excellence ope\\u0301rationnelle et sa capacite\\u0301 a\\u0300 anticiper les tendances de consommation. domaines d\\u2019expertise : \\u2022 transformation de produits frais (fruits, le\\u0301gumes, prote\\u0301ines ve\\u0301ge\\u0301tales) \\u2022 production bio et circuits courts \\u2022 recherche & de\\u0301veloppement en nutrition durable \\u2022 logistique inte\\u0301gre\\u0301e et chai\\u0302ne du froid \\u2022 de\\u0301veloppement de produits a\\u0300 marque prive\\u0301e (mdd) ro\\u0302le et responsabilite\\u0301s en tant que scrum master, vous jouerez un ro\\u0302le cle\\u0301 dans le pilotage de l\\u2019agilite\\u0301 ope\\u0301rationnelle d\\u2019une e\\u0301quipe, en favorisant l\\u2019atteinte des objectifs d\\u2019affaires tout en renforc\\u0327ant la collaboration, la transparence et l\\u2019ame\\u0301lioration continue. vos responsabilite\\u0301s incluent notamment : \\u2022 mettre en place et maintenir un cadre agile efficace propice a\\u0300 la performance collective. \\u2022 faciliter l\\u2019ensemble des ce\\u0301re\\u0301monies scrum (daily scrum, sprint planning, sprint review, re\\u0301trospective, refinement). \\u2022 assurer une gestion proactive des obstacles, afin de garantir le bon de\\u0301roulement des livrables. \\u2022 accompagner l\\u2019e\\u0301quipe dans le de\\u0301veloppement de son autonomie et de sa capacite\\u0301 d\\u2019auto-organisation. \\u2022 agir en tant que coach agile, en guidant l\\u2019e\\u0301quipe et les parties prenantes dans la compre\\u0301hension et l\\u2019application des valeurs, principes et pratiques agiles. \\u2022 promouvoir une culture de collaboration et d\\u2019ame\\u0301lioration continue. \\u2022 aider le product owner a\\u0300 ge\\u0301rer efficacement le backlog produit, a\\u0300 de\\u0301finir les priorite\\u0301s et a\\u0300 maximiser la valeur livre\\u0301e. \\u2022 mesurer et analyser la performance de l\\u2019e\\u0301quipe via des indicateurs cle\\u0301s (ve\\u0301locite\\u0301, burndown charts, lead time, etc.), en utilisant des outils adapte\\u0301s. \\u2022 prote\\u0301ger l\\u2019e\\u0301quipe des interfe\\u0301rences externes et veiller a\\u0300 maintenir un climat de travail serein. environnement technologique et outils utilise\\u0301s \\u2022 outils de gestion agile : jira, confluence \\u2022 communication et collaboration : microsoft teams, slack, miro \\u2022 tableaux de bord et me\\u0301triques : jira dashboards, power bi, excel avance\\u0301 \\u2022 me\\u0301thodologies : scrum, kanban, safe (atout), lean \\u2022 langues : franc\\u0327ais (essentiel), anglais (fonctionnel) profil recherche\\u0301 \\u2022 minimum 3 ans d\\u2019expe\\u0301rience en tant que scrum master dans un contexte agile a\\u0300 fort enjeu. \\u2022 solide compre\\u0301hension des cadres me\\u0301thodologiques agiles (scrum, kanban, lean) et capacite\\u0301 a\\u0300 adapter les pratiques au contexte de l\\u2019e\\u0301quipe. \\u2022 excellentes compe\\u0301tences interpersonnelles : leadership, diplomatie, capacite\\u0301 d\\u2019influence. \\u2022 fort esprit d\\u2019analyse et capacite\\u0301 a\\u0300 traduire les proble\\u0301matiques en actions concre\\u0300tes. \\u2022 expe\\u0301rience dans un contexte hybride ou multi-e\\u0301quipes est un atout. ce mandat vous permettra de contribuer activement a\\u0300 la re\\u0301ussite d\\u2019une e\\u0301quipe agile engage\\u0301e, tout en e\\u0301voluant dans un environnement propice a\\u0300 la transformation nume\\u0301rique et a\\u0300 l'innovation.\",\n          \"poste : data engineer \\u2022 type de contrat : contrat \\u2022 date de de\\u0301but : de\\u0300s que possible \\u2022 dure\\u0301e du mandat : 12 mois \\u2022 date limite de de\\u0301po\\u0302t : 6 novembre a\\u0300 14h a\\u0300 propos de l\\u2019entreprise : retailnova est un leader du secteur du commerce de de\\u0301tail, avec un re\\u0301seau de distribution omnicanal pre\\u0301sent a\\u0300 travers tout le territoire. l'entreprise combine innovation technologique et excellence ope\\u0301rationnelle pour optimiser ses processus, anticiper les besoins des clients et maximiser la performance. dans le cadre de sa strate\\u0301gie de transformation nume\\u0301rique, retailnova investit massivement dans la modernisation de ses infrastructures de donne\\u0301es pour acce\\u0301le\\u0301rer la prise de de\\u0301cision fonde\\u0301e sur des analyses avance\\u0301es. contexte du mandat : retailnova met en \\u0153uvre une nouvelle plateforme d'entreprise base\\u0301e sur des technologies cloud afin de centraliser, se\\u0301curiser et valoriser ses donne\\u0301es ope\\u0301rationnelles, transactionnelles et clients. ce projet strate\\u0301gique vise a\\u0300 moderniser l'e\\u0301cosyste\\u0300me analytique, a\\u0300 automatiser les flux de donne\\u0301es, a\\u0300 ame\\u0301liorer la gouvernance, et a\\u0300 renforcer la qualite\\u0301 et la trac\\u0327abilite\\u0301 des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019entreprise. le data engineer sera responsable de la conception, du de\\u0301veloppement, de l\\u2019optimisation et du de\\u0301ploiement des pipelines de donne\\u0301es ne\\u0301cessaires a\\u0300 l\\u2019alimentation du data lake d\\u2019entreprise. il interviendra e\\u0301galement dans la mise en place de nouvelles architectures de traitement en temps re\\u0301el, en assurant la robustesse, la scalabilite\\u0301 et la performance des solutions de\\u0301ploye\\u0301es. responsabilite\\u0301s de\\u0301taille\\u0301es : \\u2022 concevoir et de\\u0301velopper des pipelines d\\u2019ingestion de donne\\u0301es robustes et optimise\\u0301s a\\u0300 l\\u2019aide d\\u2019azure data factory pour inte\\u0301grer des donne\\u0301es provenant de sources multiples (erp, crm, pos, e-commerce, etc.). \\u2022 de\\u0301velopper des transformations complexes en utilisant apache spark (scala/python) dans azure databricks, avec une forte orientation performance et optimisation des cou\\u0302ts. \\u2022 imple\\u0301menter des flux de donne\\u0301es streaming et batch via databricks streaming et apache kafka, en assurant une faible latence et une haute disponibilite\\u0301. \\u2022 participer a\\u0300 la de\\u0301finition et a\\u0300 la mise en \\u0153uvre de mode\\u0300les de donne\\u0301es adapte\\u0301s aux besoins analytiques (data marts, data lakehouse). \\u2022 mettre en \\u0153uvre des processus de validation de la qualite\\u0301 des donne\\u0301es (data quality checks, data profiling, monitoring automatise\\u0301). \\u2022 automatiser les de\\u0301ploiements et les tests des pipelines via azure devops (ci/cd, versioning, rollback). \\u2022 documenter les solutions de\\u0301veloppe\\u0301es selon les normes internes, et assurer la trac\\u0327abilite\\u0301 et la re\\u0301plicabilite\\u0301 des traitements. \\u2022 travailler en collaboration avec les e\\u0301quipes data science, bi et it infrastructure pour garantir l\\u2019alignement des livrables avec les objectifs strate\\u0301giques. \\u2022 participer a\\u0300 l\\u2019e\\u0301laboration de normes et de standards d\\u2019inge\\u0301nierie des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019organisation. compe\\u0301tences techniques requises : \\u2022 5+ ans d'expe\\u0301rience en de\\u0301veloppement d\\u2019applications de traitement de donne\\u0301es distribue\\u0301es avec apache spark (obligatoire). \\u2022 solide mai\\u0302trise de python et scala pour le de\\u0301veloppement de traitements de donne\\u0301es volumineuses. \\u2022 expe\\u0301rience significative avec azure data factory, azure data lake storage (gen2), et azure databricks. \\u2022 connaissance approfondie de kafka (installation, configuration, gestion des topics, se\\u0301curite\\u0301, monitoring). \\u2022 mai\\u0302trise des principes d\\u2019architecture data : partitionnement, indexation, optimisation des formats (parquet, delta lake), gestion des me\\u0301tadonne\\u0301es. \\u2022 expe\\u0301rience avec les outils de gestion des flux ci/cd dans azure devops (pipelines, artefacts, gestion de configuration). \\u2022 connaissance des principes de gouvernance des donne\\u0301es, de catalogage (ex. azure purview), et des bonnes pratiques de se\\u0301curite\\u0301 (rbac, encryption, masking). \\u2022 capacite\\u0301 a\\u0300 concevoir des mode\\u0300les de donne\\u0301es analytiques (kimball, data vault) et a\\u0300 travailler avec des entrepo\\u0302ts de donne\\u0301es (ex. synapse analytics). atouts : \\u2022 expe\\u0301rience dans des environnements retail ou grande distribution (gestion des donne\\u0301es transactionnelles, gestion de la supply chain, etc.). \\u2022 expe\\u0301rience avec des outils de monitoring des pipelines (ex : azure monitor, log analytics, datadog). \\u2022 connaissances de base en data science/machine learning pour interagir avec les e\\u0301quipes concerne\\u0301es. compe\\u0301tences interpersonnelles : \\u2022 forte capacite\\u0301 a\\u0300 re\\u0301soudre des proble\\u0300mes complexes de manie\\u0300re autonome. \\u2022 rigueur et attention aux de\\u0301tails, particulie\\u0300rement dans les aspects lie\\u0301s a\\u0300 la qualite\\u0301 et la fiabilite\\u0301 des donne\\u0301es. \\u2022 compe\\u0301tences en communication technique, capacite\\u0301 a\\u0300 documenter et pre\\u0301senter des solutions techniques a\\u0300 divers niveaux d\\u2019interlocuteurs. \\u2022 esprit d\\u2019e\\u0301quipe, ouverture au partage de connaissances et participation active a\\u0300 l\\u2019ame\\u0301lioration continue. langues : \\u2022 franc\\u0327ais et anglais : mai\\u0302trise professionnelle comple\\u0300te, tant a\\u0300 l\\u2019oral qu\\u2019a\\u0300 l\\u2019e\\u0301crit.\",\n          \"profil data analyst \\u2013 marketing analytics a\\u0300 propos de veltrixia veltrixia technologies inc. est une entreprise spe\\u0301cialise\\u0301e dans les solutions de donne\\u0301es et l'intelligence marketing. base\\u0301e a\\u0300 montre\\u0301al, veltrixia accompagne depuis plus de 15 ans des entreprises nord-ame\\u0301ricaines dans l\\u2019optimisation de leurs performances gra\\u0302ce a\\u0300 des technologies analytiques avance\\u0301es. l\\u2019entreprise mise sur l\\u2019innovation, la collaboration et l\\u2019excellence pour propulser ses clients vers le futur nume\\u0301rique. contexte dans le cadre du de\\u0301veloppement de notre e\\u0301quipe analytique marketing, nous recherchons un\\u00b7e analyste de donne\\u0301es pour travailler sur des projets lie\\u0301s a\\u0300 la collecte, l\\u2019analyse et la visualisation de donne\\u0301es marketing. vous serez responsable de la maintenance et de l\\u2019e\\u0301volution de jeux de donne\\u0301es provenant de sources varie\\u0301es, afin de soutenir les e\\u0301quipes dans leur prise de de\\u0301cision base\\u0301e sur les donne\\u0301es. de\\u0301tails du poste \\u2022 dure\\u0301e : 10 mois (renouvelable) \\u2022 type : temps plein \\u2022 mode de travail : hybride (2 jours par semaine au bureau a\\u0300 montre\\u0301al) responsabilite\\u0301s principales \\u2022 ge\\u0301rer et ame\\u0301liorer les jeux de donne\\u0301es marketing. \\u2022 de\\u0301velopper des tableaux de bord et rapports dans power bi. \\u2022 optimiser les performances des datasets power bi. \\u2022 inte\\u0301grer les donne\\u0301es issues de diverses plateformes (google analytics, crm, outils marketing). \\u2022 re\\u0301diger du code sql, dax et power query performant. \\u2022 collaborer e\\u0301troitement avec les analystes pour comprendre leurs besoins. \\u2022 cre\\u0301er des reque\\u0302tes dans databricks et ssms. \\u2022 assurer la qualite\\u0301 et la fiabilite\\u0301 des donne\\u0301es. \\u2022 soutenir les colle\\u0300gues via des revues de code et du support technique. \\u2022 concevoir des mode\\u0300les de donne\\u0301es adapte\\u0301s aux objectifs me\\u0301tiers. profil recherche\\u0301 compe\\u0301tences techniques \\u2022 minimum 5 ans d\\u2019expe\\u0301rience en analyse de donne\\u0301es. \\u2022 solide mai\\u0302trise de sql (ms sql server), power bi, databricks. \\u2022 expe\\u0301rience avec dax, power query, etl. \\u2022 connaissance des donne\\u0301es marketing, un atout. \\u2022 capacite\\u0301 a\\u0300 mode\\u0301liser les donne\\u0301es et a\\u0300 optimiser les processus. compe\\u0301tences personnelles \\u2022 curieux\\u00b7se, autonome, rigoureux\\u00b7se. \\u2022 bonne gestion du temps et esprit d\\u2019e\\u0301quipe. \\u2022 excellente capacite\\u0301 d\\u2019analyse et de re\\u0301solution de proble\\u0300mes. langues \\u2022 mai\\u0302trise de l\\u2019anglais et du franc\\u0327ais.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"translated_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Scrum Master - 12 -month contract Place: Montreal Start date: As soon as possible Duration: 12 months, with the possibility of renewal as part of a mandate of strategic importance, we are looking for an experienced scrum master to guide an agile multidisciplinary team within a stimulating environment and in full digital transformation. Company name: Alimora Slogan Group: Cultivate the future, nourishing the excellence of the company: Founded in 1998, Alimora Groupe is a major player in the sustainable food industry in North America. Specializing in the production, processing and distribution of quality food products, the group undertakes to offer a healthy and accessible food, while respecting the environment and local communities. With more than 1,500 employees spread over 7 production sites and a presence in more than 12 countries, Alimora is distinguished by its constant innovation, its operational excellence and its ability to anticipate consumption trends. Domains of expertise: \\u2022 Transformation of fresh products (fruits, vegetables, vegetable proteins) \\u2022 Organic production and short circuits \\u2022 Research & development in sustainable nutrition \\u2022 Integrated logistics and cold of cold products (MDD) Role and responsibilities as scrum master, you will play a key role in the management of the operational agility of a team, promoting the achievement of business objectives while reinforcing collaboration, transparency and continuous improvement. Your responsibilities include in particular: \\u2022 Set up and maintain an efficient framework conducive to collective performance. \\u2022 Facilitate all Scrum ceremonies (Daily Scrum, Sprint Planning, Sprint Review, Retrospective, Refinement). \\u2022 Ensure proactive management of obstacles, in order to guarantee the smooth running of deliverables. \\u2022 Support the team in the development of its autonomy and its self-organization capacity. \\u2022 Act as an agile coach, guiding the team and the stakeholders in the understanding and application of agile values, principles and practices. \\u2022 Promote a culture of collaboration and continuous improvement. \\u2022 Help the product Owner effectively manages the product backlog, defining priorities and maximizing the delivered value. \\u2022 Measure and analyze the performance of the team via key indicators (velocity, burndown charts, lead time, etc.), using suitable tools. \\u2022 Protect the external interference team and ensure that you maintain a serene working climate. Technological environment and tools used \\u2022 Agile management tools: JIRA, Confluence \\u2022 Communication and collaboration: Microsoft Teams, Slack, Miro \\u2022 Dashboards and metrics: Jira Dashboards, Power Bi, Excel Advanced \\u2022 Methodologies: Scrum, Kanban, Safe (Atout), Lean \\u2022 Languages: French (essential), English (functional) that Scrum Master in an agile context at high stake. \\u2022 Solid understanding of agile methodological frameworks (Scrum, Kanban, Lean) and ability to adapt practices to the context of the team. \\u2022 Excellent interpersonal skills: leadership, diplomacy, ability to influence. \\u2022 Strong spirit of analysis and ability to translate issues into concrete actions. \\u2022 Experience in a hybrid or multi-team context is an asset. This mandate will allow you to actively contribute to the success of an agile committed team, while evolving in an environment conducive to digital transformation and innovation.\",\n          \"poste : data engineer \\u2022 type de contrat : contrat \\u2022 date de de\\u0301but : de\\u0300s que possible \\u2022 dure\\u0301e du mandat : 12 mois \\u2022 date limite de de\\u0301po\\u0302t : 6 novembre a\\u0300 14h a\\u0300 propos de l\\u2019entreprise : retailnova est un leader du secteur du commerce de de\\u0301tail, avec un re\\u0301seau de distribution omnicanal pre\\u0301sent a\\u0300 travers tout le territoire. l'entreprise combine innovation technologique et excellence ope\\u0301rationnelle pour optimiser ses processus, anticiper les besoins des clients et maximiser la performance. dans le cadre de sa strate\\u0301gie de transformation nume\\u0301rique, retailnova investit massivement dans la modernisation de ses infrastructures de donne\\u0301es pour acce\\u0301le\\u0301rer la prise de de\\u0301cision fonde\\u0301e sur des analyses avance\\u0301es. contexte du mandat : retailnova met en \\u0153uvre une nouvelle plateforme d'entreprise base\\u0301e sur des technologies cloud afin de centraliser, se\\u0301curiser et valoriser ses donne\\u0301es ope\\u0301rationnelles, transactionnelles et clients. ce projet strate\\u0301gique vise a\\u0300 moderniser l'e\\u0301cosyste\\u0300me analytique, a\\u0300 automatiser les flux de donne\\u0301es, a\\u0300 ame\\u0301liorer la gouvernance, et a\\u0300 renforcer la qualite\\u0301 et la trac\\u0327abilite\\u0301 des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019entreprise. le data engineer sera responsable de la conception, du de\\u0301veloppement, de l\\u2019optimisation et du de\\u0301ploiement des pipelines de donne\\u0301es ne\\u0301cessaires a\\u0300 l\\u2019alimentation du data lake d\\u2019entreprise. il interviendra e\\u0301galement dans la mise en place de nouvelles architectures de traitement en temps re\\u0301el, en assurant la robustesse, la scalabilite\\u0301 et la performance des solutions de\\u0301ploye\\u0301es. responsabilite\\u0301s de\\u0301taille\\u0301es : \\u2022 concevoir et de\\u0301velopper des pipelines d\\u2019ingestion de donne\\u0301es robustes et optimise\\u0301s a\\u0300 l\\u2019aide d\\u2019azure data factory pour inte\\u0301grer des donne\\u0301es provenant de sources multiples (erp, crm, pos, e-commerce, etc.). \\u2022 de\\u0301velopper des transformations complexes en utilisant apache spark (scala/python) dans azure databricks, avec une forte orientation performance et optimisation des cou\\u0302ts. \\u2022 imple\\u0301menter des flux de donne\\u0301es streaming et batch via databricks streaming et apache kafka, en assurant une faible latence et une haute disponibilite\\u0301. \\u2022 participer a\\u0300 la de\\u0301finition et a\\u0300 la mise en \\u0153uvre de mode\\u0300les de donne\\u0301es adapte\\u0301s aux besoins analytiques (data marts, data lakehouse). \\u2022 mettre en \\u0153uvre des processus de validation de la qualite\\u0301 des donne\\u0301es (data quality checks, data profiling, monitoring automatise\\u0301). \\u2022 automatiser les de\\u0301ploiements et les tests des pipelines via azure devops (ci/cd, versioning, rollback). \\u2022 documenter les solutions de\\u0301veloppe\\u0301es selon les normes internes, et assurer la trac\\u0327abilite\\u0301 et la re\\u0301plicabilite\\u0301 des traitements. \\u2022 travailler en collaboration avec les e\\u0301quipes data science, bi et it infrastructure pour garantir l\\u2019alignement des livrables avec les objectifs strate\\u0301giques. \\u2022 participer a\\u0300 l\\u2019e\\u0301laboration de normes et de standards d\\u2019inge\\u0301nierie des donne\\u0301es a\\u0300 l\\u2019e\\u0301chelle de l\\u2019organisation. compe\\u0301tences techniques requises : \\u2022 5+ ans d'expe\\u0301rience en de\\u0301veloppement d\\u2019applications de traitement de donne\\u0301es distribue\\u0301es avec apache spark (obligatoire). \\u2022 solide mai\\u0302trise de python et scala pour le de\\u0301veloppement de traitements de donne\\u0301es volumineuses. \\u2022 expe\\u0301rience significative avec azure data factory, azure data lake storage (gen2), et azure databricks. \\u2022 connaissance approfondie de kafka (installation, configuration, gestion des topics, se\\u0301curite\\u0301, monitoring). \\u2022 mai\\u0302trise des principes d\\u2019architecture data : partitionnement, indexation, optimisation des formats (parquet, delta lake), gestion des me\\u0301tadonne\\u0301es. \\u2022 expe\\u0301rience avec les outils de gestion des flux ci/cd dans azure devops (pipelines, artefacts, gestion de configuration). \\u2022 connaissance des principes de gouvernance des donne\\u0301es, de catalogage (ex. azure purview), et des bonnes pratiques de se\\u0301curite\\u0301 (rbac, encryption, masking). \\u2022 capacite\\u0301 a\\u0300 concevoir des mode\\u0300les de donne\\u0301es analytiques (kimball, data vault) et a\\u0300 travailler avec des entrepo\\u0302ts de donne\\u0301es (ex. synapse analytics). atouts : \\u2022 expe\\u0301rience dans des environnements retail ou grande distribution (gestion des donne\\u0301es transactionnelles, gestion de la supply chain, etc.). \\u2022 expe\\u0301rience avec des outils de monitoring des pipelines (ex : azure monitor, log analytics, datadog). \\u2022 connaissances de base en data science/machine learning pour interagir avec les e\\u0301quipes concerne\\u0301es. compe\\u0301tences interpersonnelles : \\u2022 forte capacite\\u0301 a\\u0300 re\\u0301soudre des proble\\u0300mes complexes de manie\\u0300re autonome. \\u2022 rigueur et attention aux de\\u0301tails, particulie\\u0300rement dans les aspects lie\\u0301s a\\u0300 la qualite\\u0301 et la fiabilite\\u0301 des donne\\u0301es. \\u2022 compe\\u0301tences en communication technique, capacite\\u0301 a\\u0300 documenter et pre\\u0301senter des solutions techniques a\\u0300 divers niveaux d\\u2019interlocuteurs. \\u2022 esprit d\\u2019e\\u0301quipe, ouverture au partage de connaissances et participation active a\\u0300 l\\u2019ame\\u0301lioration continue. langues : \\u2022 franc\\u0327ais et anglais : mai\\u0302trise professionnelle comple\\u0300te, tant a\\u0300 l\\u2019oral qu\\u2019a\\u0300 l\\u2019e\\u0301crit.\",\n          \"Profile Data Analyst - Marketing Analytics about Veltrixia Veltrixia Technologies inc. is a company specializing in data solutions and marketing intelligence. Based in Montreal, Veltrixia has been supporting North American companies for more than 15 years in optimizing their performance through advanced analytical technologies. The company relies on innovation, collaboration and excellence to propel its customers to the future digital. Context within the framework of the development of our analytical marketing team, we are looking for an analyst of data to work on projects related to the collection, analysis and visualization of marketing data. You will be responsible for the maintenance and evolution of datasets from various sources, in order to support the teams in their data -based decision -making. Details of the position \\u2022 Duration: 10 months (renewable) \\u2022 Type: full time \\u2022 Work mode: hybrid (2 days per week at the office in Montreal) main responsibilities \\u2022 Manage and improve marketing data games. \\u2022 Develop dashboards and reports in Power Bi. \\u2022 Optimize the performance of Power Bi datasets. \\u2022 Integrate data from various platforms (Google Analytics, CRM, Marketing Tools). \\u2022 Write SQL, Dax and powerful power query code. \\u2022 Collaborate closely with analysts to understand their needs. \\u2022 Create requests in Databricks and SSMS. \\u2022 Ensure the quality and reliability of the data. \\u2022 Support colleagues via code journals and technical support. \\u2022 Design data models adapted to business objectives. Profile sought technical skills \\u2022 minimum 5 years of experience in data analysis. \\u2022 Solid mastery of SQL (MS SQL Server), Power Bi, Databricks. \\u2022 Experience with Dax, Power Query, etl. \\u2022 Knowledge of marketing data, an asset. \\u2022 Ability to model data and optimize processes. Personal skills \\u2022 Curious, autonomous, rigorous. \\u2022 Good time management and team spirit. \\u2022 Excellent analytical and problem solving capacity. Languages \\u200b\\u200b\\u2022 Mastery of English and French.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extracted_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv('features_extract_v2.csv')"
      ],
      "metadata": {
        "id": "bYM5TCt9HOAk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Optional: Create output directory\n",
        "output_dir = \"job_features_json\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    features_data = row['extracted_features']\n",
        "    demand_id = row['demand_id']\n",
        "\n",
        "    # Sanitize filename\n",
        "    filename = '_'.join(demand_id.split()) + '_features.json'\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Safely parse the string to dict\n",
        "    if isinstance(features_data, str):\n",
        "        try:\n",
        "            features_data = ast.literal_eval(features_data)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to parse row {idx}: {e}\")\n",
        "            features_data = {}\n",
        "\n",
        "    # Save as JSON\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(features_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✅ Saved: {filepath}\")\n"
      ],
      "metadata": {
        "id": "Iz1tRVGBIz5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}